{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b932690c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81449ecf8c37d8eabb5d8797a84499fc",
     "grade": false,
     "grade_id": "cell-f2471a8af23370fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Week3 Assignments\n",
    "**Please do the assignments using the `mlops_eng` environment.**\n",
    "\n",
    "This week's assignments will give you some hands-on experience with Optuna (and Ray Tune). Similar to the tutorial of the first week, the [red wine dataset](https://archive.ics.uci.edu/dataset/186/wine+quality) will be used in this week's assignments. \n",
    "\n",
    "**Guidelines for submitting assignments**:\n",
    "- For each assignment, a code skeleton is provided. Please put your solutions between the `### START CODE HERE` and `### END CODE HERE` code comments. Please **do not change any code other than those between the `### START CODE HERE` and `### END CODE HERE` comments**. Otherwise your notebook may not pass the tests used in grading.\n",
    "- Some assignments also require you to answer questions (in text) or capture screenshots in order to earn points. Please put your text answers and screenshots in a single PDF file. For each answer and screenshot, please clearly indicate which assignment it corresponds to in your PDF file. Please include the PDF file in your submission.\n",
    "- In Assignments 1 and 2, you'll be asked to save your Optuna Studies in an SQLite database \"optuna.sqlite3\" (the database will be created when you proceed with the assignments). Please also include this database in your submission (**do not change the file name, just keep it as \"optuna.sqlite3\"**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8dfe776",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a694eee709e5728fbd55272102e83b23",
     "grade": false,
     "grade_id": "cell-5251f7930ec308de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import platform\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "from optuna.samplers import TPESampler\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c4b5a4-980b-4bf7-a93c-0841b0ec33cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d30e12dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b218b9c57aee860a174b0096d3817e66",
     "grade": false,
     "grade_id": "cell-907e33c01ce6baa8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current platform: Linux\n"
     ]
    }
   ],
   "source": [
    "# Make sure you've installed the right version of lightgbm and xgboost\n",
    "current_platform = platform.system()\n",
    "print(f\"Current platform: {current_platform}\")\n",
    "if current_platform == \"Darwin\" or current_platform == \"Linux\":\n",
    "    if current_platform == \"Darwin\":  # macOS\n",
    "        assert lgb.__version__.startswith(\"4.5.0\"), f\"Wrong version of lightgbm for platform: {current_platform}\"\n",
    "        assert xgb.__version__.startswith(\"2.0.3\"), f\"Wrong version of xgb for platform: {current_platform}\"\n",
    "    elif current_platform == \"Linux\": # Ubuntu\n",
    "        assert lgb.__version__ == \"4.0.0\", f\"Wrong version of lightgbm for platform: {current_platform}\"\n",
    "        assert xgb.__version__ == \"2.0.3\", f\"Wrong version of xgboost for platform: {current_platform}\"\n",
    "    else:\n",
    "        assert False, f\"Unknown platform: {current_platform}\"\n",
    "else:\n",
    "    assert False, f\"Unexpected platform: {current_platform}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3476fca2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "359816a57fe72335f6bef7a010d6e38b",
     "grade": false,
     "grade_id": "cell-db5b726e21265212",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is just for the grading purpose\n",
    "def is_being_graded():\n",
    "    \"\"\"\n",
    "    Returns True if the notebook is being executed by the auto-grading tool.\n",
    "    \"\"\"\n",
    "    env = os.environ.get(\"NBGRADER_EXECUTION\")\n",
    "    return env == \"autograde\" or env == \"validate\"\n",
    "\n",
    "\n",
    "# Suppress loggings and warnings when grading the notebook\n",
    "if is_being_graded():\n",
    "    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "    for logger in loggers:\n",
    "        logger.setLevel(logging.ERROR)\n",
    "    mlflow.utils.logging_utils.disable_logging()\n",
    "    warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdd5f155",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ad5c08ed5ebc831885975da56fc7304",
     "grade": false,
     "grade_id": "cell-89dcff2b2c701d19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Random seed for making the assignments reproducible\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# MLflow service URI\n",
    "mlflow_tracking_uri = \"http://mlflow-server.local\"\n",
    "\n",
    "# Configure MLflow \n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://mlflow-minio.local\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30cf832a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "265b527d8f4d3510f639118c82937d2a",
     "grade": false,
     "grade_id": "cell-fdd5de47cceb02fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Prepare training and testing data\n",
    "data = pd.read_csv(\"winequality-red.csv\", delimiter=\";\")\n",
    "\n",
    "X = data.drop(\"quality\", axis=1)\n",
    "y = data[\"quality\"]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b80f7a58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6005e4fab5d3aa392d73519e73d85383",
     "grade": false,
     "grade_id": "cell-37420489486547e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An overview of the original dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eae2bbd5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "678b32525cbb26ebe55a665f240f6116",
     "grade": false,
     "grade_id": "cell-02e51dc6b10bf7cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of train_x is (1199, 11)\n",
      "The dimension of train_y is (1199,)\n",
      "The dimension of test_x is (400, 11)\n",
      "The dimension of test_y is (400,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dimension of train_x is {train_x.shape}\")\n",
    "print(f\"The dimension of train_y is {train_y.shape}\")\n",
    "print(f\"The dimension of test_x is {test_x.shape}\")\n",
    "print(f\"The dimension of test_y is {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1805063",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c911f265204417e4c8797d5b75db878e",
     "grade": false,
     "grade_id": "cell-59b310c39429dd2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 1: The basic use of Optuna (2 points)\n",
    "Your task is to use Optuna to find the optimal hyperparameter combination for the [LightGBM regression model](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html) used for predicting red wine quality. **Please use the sklearn API as you did in the first week's assignments.**\n",
    "\n",
    "This assignment has the following requirements:\n",
    "\n",
    "1) Define the objective function (`objective_func`). The target of the optimization is to minimize the MAE (mean absolute error) of the model when evaluating the model against the testing dataset. The hyperparameters to be tuned and their search ranges are shown below. Some of the hyperparameter values are fixed. The hyperparameter values should be sampled in a linear domain if not separately specified. In your objective function, please specify the hyperparameters in the same order as presented in the table.  \n",
    "\n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| n_estimators      | The number of decision trees.                                               | integer | 1000 (fixed value)                                                       |\n",
    "| learning_rate     | The step size of the gradient descent. It controls how quickly the model fits and then overfits the training data.              | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| subsample         | The percentage of training samples to be used to train each tree. `subsample*100%` of the training samples will be randomly selected for training.        | float   | [0.05, 0.5]                                                              |\n",
    "| subsample_freq    | Subsampling frequency. The subsampling will be performed again after `subsample_freq` trees have been trained.                                                     | integer | 1 (fixed value)                                                          |\n",
    "| colsample_bytree  | The percentage of features to use when training each tree.                | float   | [0.05, 0.5]                                                              |\n",
    "| min_child_samples | A leaf node should have `min_child_samples` data points to be further splitted. | integer | [20, 100]                                                                |\n",
    "| num_leaves        | Max number of nodes in a single tree.                                       | integer | [2, 2^10]                                                                |\n",
    "| random_state      | The seed for random number generation for reproducibility.                                   | integer | RANDOM_SEED (fixed value, RANDOM_SEED has been defined as a variable in a previous cell)                                                |\n",
    "\n",
    "2) Define another function named `run_study` that creates and runs a study. Detailed requirements are listed below:\n",
    "    - Use [TPESampler](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html) as the sampler for the hyperparameter sampling and use `RANDOM_SEED` as the seed of the sampler. \n",
    "    - The Optuna study should have a name specified by the `study_name` argument, use the objective function given as the `objective_func` argument and perform `n_trials` trials.\n",
    "    - The study history should be persisted in a relational database specified by the `storage` argument so that the study can be loaded and analyzed later. \n",
    "    - The function should finally return the study.\n",
    "\n",
    "Hints:\n",
    "- [How to sample hyperparameter values in the logarithmic domain?](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_float)\n",
    "- [How to configure a study to use a specific sampler and persist study history in a specific database?](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.create_study.html#optuna-create-study)\n",
    "- You'll probably see LightGBM throw a bunch of warnings of \"No further splits with positive gain, best gain: -inf\". This warning basically means LightGBM can't find a split that would improve the model's performance at a particular node. You can suppress these warning by setting `verbose=-1` when defining your model, e.g., `lgb.LGBMRegressor(verbose=-1, ...)`\n",
    "\n",
    "**Notes**:\n",
    "- When define the search space for the hyperparameters, please use the names given in the \"Hyperparameter\" column in the table above.\n",
    "- Please **do not** use deprecated methods (e.g, suggest_uniform and suggest_loguniform) when define the search space. \n",
    "\n",
    "*More reading material: If you are interested, [the LightGBM documentation](https://lightgbm.readthedocs.io/en/latest/Parameters.html#core-parameters) explains the use of each hyperparameter in more details.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f39b96c3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02ad6d8ca0e4d5a4b8c4b3fe0d0d39c8",
     "grade": false,
     "grade_id": "cell-15d27f8fdf5190ad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective_func(trial):\n",
    "    # TODO:\n",
    "    # Define the hyperparameters to be tuned and their search ranges\n",
    "    # Train a model using the sampled hyperparameters\n",
    "    # Evaluate the model using the test dataset\n",
    "    # Return the evaluation metric\n",
    "    ### START CODE HERE\n",
    "    params = {\n",
    "        \"n_estimators\": 1000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 0.5),\n",
    "        \"subsample_freq\": 1,\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 0.5),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 2**10),\n",
    "        \"random_state\": RANDOM_SEED\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(verbose=-1, **params)\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    pred = model.predict(test_x)\n",
    "\n",
    "    score = mean_absolute_error(test_y, pred)\n",
    "    return score\n",
    "    ### END CODE HERE\n",
    "\n",
    "def run_study(study_name: str, storage: str, objective_func: callable, n_trials: int) -> optuna.study.Study:\n",
    "    \"\"\"\n",
    "    Create and run an Optuna study.\n",
    "    Args:\n",
    "        study_name: The name of the study.\n",
    "        storage: The URI of the storage used to save the study history.\n",
    "        objective_func: The objective function to be optimized.\n",
    "        n_trials: The number of trials the study should perform.\n",
    "    Returns:\n",
    "        A Study object.\n",
    "    \"\"\"\n",
    "    # Delete the study if it already exists\n",
    "    if study_name in optuna.get_all_study_names(storage=storage):\n",
    "        optuna.delete_study(study_name=study_name, storage=storage)\n",
    "\n",
    "    # TODO: Create (and run) the study and record the history in the storage\n",
    "    ### START CODE HERE\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
    "        study_name=study_name,\n",
    "        storage=storage,\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(objective_func, n_trials=n_trials)\n",
    "\n",
    "    return study\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "323727f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26db5eba7a4f29ecab0a4ef903db0746",
     "grade": false,
     "grade_id": "cell-f4795fd66796fd8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 16:22:40,859] A new study created in RDB with name: lgbm-wine-1\n",
      "[I 2025-11-21 16:22:41,139] Trial 0 finished with value: 0.48305757514051956 and parameters: {'learning_rate': 0.005611516415334507, 'subsample': 0.4778214378844623, 'colsample_bytree': 0.3793972738151323, 'min_child_samples': 68, 'num_leaves': 161}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2025-11-21 16:22:41,294] Trial 1 finished with value: 0.6685091743119267 and parameters: {'learning_rate': 0.002051110418843397, 'subsample': 0.07613762547568977, 'colsample_bytree': 0.4397792655987208, 'min_child_samples': 68, 'num_leaves': 726}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2025-11-21 16:22:41,679] Trial 2 finished with value: 0.5330860406894041 and parameters: {'learning_rate': 0.0010994335574766201, 'subsample': 0.48645943347289744, 'colsample_bytree': 0.4245991883601898, 'min_child_samples': 37, 'num_leaves': 188}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2025-11-21 16:22:41,893] Trial 3 finished with value: 0.5338040960963436 and parameters: {'learning_rate': 0.002327067708383781, 'subsample': 0.186909009331792, 'colsample_bytree': 0.28614039423450705, 'min_child_samples': 54, 'num_leaves': 299}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2025-11-21 16:22:42,134] Trial 4 finished with value: 0.5076269460531386 and parameters: {'learning_rate': 0.01673808578875214, 'subsample': 0.11277223729341883, 'colsample_bytree': 0.1814650918408482, 'min_child_samples': 49, 'num_leaves': 468}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2025-11-21 16:22:42,334] Trial 5 finished with value: 0.5076748898380093 and parameters: {'learning_rate': 0.037183641805732096, 'subsample': 0.1398532019712619, 'colsample_bytree': 0.28140549728612524, 'min_child_samples': 67, 'num_leaves': 49}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2025-11-21 16:22:42,476] Trial 6 finished with value: 0.6685091743119267 and parameters: {'learning_rate': 0.016409286730647923, 'subsample': 0.1267358556592812, 'colsample_bytree': 0.0792732168433758, 'min_child_samples': 96, 'num_leaves': 989}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2025-11-21 16:22:42,666] Trial 7 finished with value: 0.5051750796616936 and parameters: {'learning_rate': 0.041380401125610165, 'subsample': 0.1870761961280168, 'colsample_bytree': 0.09395245130287275, 'min_child_samples': 75, 'num_leaves': 452}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2025-11-21 16:22:42,886] Trial 8 finished with value: 0.602185209067021 and parameters: {'learning_rate': 0.0017541893487450805, 'subsample': 0.2728296095500716, 'colsample_bytree': 0.06547483450184828, 'min_child_samples': 93, 'num_leaves': 266}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2025-11-21 16:22:43,079] Trial 9 finished with value: 0.49454634175435186 and parameters: {'learning_rate': 0.02113705944064573, 'subsample': 0.19026998424023495, 'colsample_bytree': 0.28403060953001485, 'min_child_samples': 64, 'num_leaves': 191}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2025-11-21 16:22:43,471] Trial 10 finished with value: 0.46421846512762593 and parameters: {'learning_rate': 0.005609875978923096, 'subsample': 0.46978789997535003, 'colsample_bytree': 0.36325301151124445, 'min_child_samples': 21, 'num_leaves': 17}. Best is trial 10 with value: 0.46421846512762593.\n",
      "[I 2025-11-21 16:22:43,717] Trial 11 finished with value: 0.4839260622308465 and parameters: {'learning_rate': 0.0055881031669443425, 'subsample': 0.4925415013251626, 'colsample_bytree': 0.3706448031058781, 'min_child_samples': 26, 'num_leaves': 5}. Best is trial 10 with value: 0.46421846512762593.\n",
      "[I 2025-11-21 16:22:44,075] Trial 12 finished with value: 0.46846727958527695 and parameters: {'learning_rate': 0.005633225713234431, 'subsample': 0.3990732670799304, 'colsample_bytree': 0.3557251924593569, 'min_child_samples': 24, 'num_leaves': 19}. Best is trial 10 with value: 0.46421846512762593.\n",
      "[I 2025-11-21 16:22:44,530] Trial 13 finished with value: 0.4490225094819387 and parameters: {'learning_rate': 0.006726317178272483, 'subsample': 0.39331551661530145, 'colsample_bytree': 0.4982190144284359, 'min_child_samples': 20, 'num_leaves': 662}. Best is trial 13 with value: 0.4490225094819387.\n",
      "[I 2025-11-21 16:22:44,867] Trial 14 finished with value: 0.4623174947161377 and parameters: {'learning_rate': 0.0091300828477436, 'subsample': 0.3602914705042889, 'colsample_bytree': 0.4984916911753534, 'min_child_samples': 36, 'num_leaves': 687}. Best is trial 13 with value: 0.4490225094819387.\n",
      "[I 2025-11-21 16:22:45,197] Trial 15 finished with value: 0.44139549474694817 and parameters: {'learning_rate': 0.08538648795388477, 'subsample': 0.3625443334866371, 'colsample_bytree': 0.498207439286346, 'min_child_samples': 38, 'num_leaves': 684}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:45,520] Trial 16 finished with value: 0.45298158513185255 and parameters: {'learning_rate': 0.09970682275036434, 'subsample': 0.3436754646846178, 'colsample_bytree': 0.4683173574944437, 'min_child_samples': 39, 'num_leaves': 714}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:45,808] Trial 17 finished with value: 0.495701894159936 and parameters: {'learning_rate': 0.09909565063638189, 'subsample': 0.29572259322277433, 'colsample_bytree': 0.18655083539985107, 'min_child_samples': 47, 'num_leaves': 885}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:46,195] Trial 18 finished with value: 0.4434038694428212 and parameters: {'learning_rate': 0.03671556184809921, 'subsample': 0.4187086614418443, 'colsample_bytree': 0.48538282007418293, 'min_child_samples': 30, 'num_leaves': 570}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:46,535] Trial 19 finished with value: 0.44335870966120255 and parameters: {'learning_rate': 0.053731799997625895, 'subsample': 0.43017554203830133, 'colsample_bytree': 0.4184832504175784, 'min_child_samples': 33, 'num_leaves': 578}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:46,795] Trial 20 finished with value: 0.4625134893789983 and parameters: {'learning_rate': 0.06319576078452216, 'subsample': 0.29178069551740093, 'colsample_bytree': 0.41731449937912957, 'min_child_samples': 44, 'num_leaves': 831}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:47,206] Trial 21 finished with value: 0.44448560452947283 and parameters: {'learning_rate': 0.04524540905246321, 'subsample': 0.4277926712672274, 'colsample_bytree': 0.44279227737290805, 'min_child_samples': 31, 'num_leaves': 558}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:47,623] Trial 22 finished with value: 0.4449876430487491 and parameters: {'learning_rate': 0.027183592230793398, 'subsample': 0.42495777429581916, 'colsample_bytree': 0.4731338608597703, 'min_child_samples': 30, 'num_leaves': 537}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:47,953] Trial 23 finished with value: 0.47278352746529306 and parameters: {'learning_rate': 0.06634213926914975, 'subsample': 0.33888282632930455, 'colsample_bytree': 0.3319155110982273, 'min_child_samples': 55, 'num_leaves': 396}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:48,319] Trial 24 finished with value: 0.45156570004074614 and parameters: {'learning_rate': 0.059317013345016885, 'subsample': 0.4363746618457771, 'colsample_bytree': 0.39533671032494305, 'min_child_samples': 31, 'num_leaves': 602}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:48,688] Trial 25 finished with value: 0.4548851002749927 and parameters: {'learning_rate': 0.028653412461781675, 'subsample': 0.3606533936413034, 'colsample_bytree': 0.46890414257666063, 'min_child_samples': 41, 'num_leaves': 833}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:49,031] Trial 26 finished with value: 0.4647769208431393 and parameters: {'learning_rate': 0.07673007352022662, 'subsample': 0.3852050523585826, 'colsample_bytree': 0.32285913841337865, 'min_child_samples': 33, 'num_leaves': 613}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:49,313] Trial 27 finished with value: 0.48160079751638873 and parameters: {'learning_rate': 0.04793447045409202, 'subsample': 0.3239866908012082, 'colsample_bytree': 0.4073387573921768, 'min_child_samples': 78, 'num_leaves': 359}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:49,551] Trial 28 finished with value: 0.48375889068571815 and parameters: {'learning_rate': 0.030847828892393674, 'subsample': 0.22398310957504441, 'colsample_bytree': 0.44939243060058803, 'min_child_samples': 53, 'num_leaves': 753}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:49,907] Trial 29 finished with value: 0.46912939971036527 and parameters: {'learning_rate': 0.012970437950994895, 'subsample': 0.4566181691646891, 'colsample_bytree': 0.21312225481011907, 'min_child_samples': 27, 'num_leaves': 478}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:50,307] Trial 30 finished with value: 0.4551162002470042 and parameters: {'learning_rate': 0.0794962164313919, 'subsample': 0.406847659791187, 'colsample_bytree': 0.49450564293479493, 'min_child_samples': 44, 'num_leaves': 619}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:50,648] Trial 31 finished with value: 0.4421583837166304 and parameters: {'learning_rate': 0.04759114776546995, 'subsample': 0.4370055967845491, 'colsample_bytree': 0.44538215296236494, 'min_child_samples': 33, 'num_leaves': 542}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:50,998] Trial 32 finished with value: 0.4491740558976645 and parameters: {'learning_rate': 0.05252390675183853, 'subsample': 0.4609139238380871, 'colsample_bytree': 0.4417879397417666, 'min_child_samples': 37, 'num_leaves': 788}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:51,346] Trial 33 finished with value: 0.44595479283243106 and parameters: {'learning_rate': 0.03404578668311577, 'subsample': 0.44255113377054534, 'colsample_bytree': 0.46439197644990027, 'min_child_samples': 34, 'num_leaves': 547}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:51,693] Trial 34 finished with value: 0.453706253490258 and parameters: {'learning_rate': 0.023309644660997556, 'subsample': 0.3778085828117565, 'colsample_bytree': 0.40380754958162035, 'min_child_samples': 26, 'num_leaves': 396}. Best is trial 15 with value: 0.44139549474694817.\n",
      "[I 2025-11-21 16:22:52,070] Trial 35 finished with value: 0.4377161778888277 and parameters: {'learning_rate': 0.07877623013299838, 'subsample': 0.49214080229606594, 'colsample_bytree': 0.41886526176548317, 'min_child_samples': 42, 'num_leaves': 640}. Best is trial 35 with value: 0.4377161778888277.\n",
      "[I 2025-11-21 16:22:52,671] Trial 36 finished with value: 0.4407666942697611 and parameters: {'learning_rate': 0.08097967547298585, 'subsample': 0.49001305105479465, 'colsample_bytree': 0.4276260557279081, 'min_child_samples': 49, 'num_leaves': 654}. Best is trial 35 with value: 0.4377161778888277.\n",
      "[I 2025-11-21 16:22:52,982] Trial 37 finished with value: 0.4696496215679975 and parameters: {'learning_rate': 0.07913576300786627, 'subsample': 0.4908714010652557, 'colsample_bytree': 0.33462124859324527, 'min_child_samples': 59, 'num_leaves': 654}. Best is trial 35 with value: 0.4377161778888277.\n",
      "[I 2025-11-21 16:22:53,197] Trial 38 finished with value: 0.6685091743119267 and parameters: {'learning_rate': 0.09990946357932352, 'subsample': 0.05625390118055745, 'colsample_bytree': 0.23880172327603824, 'min_child_samples': 51, 'num_leaves': 912}. Best is trial 35 with value: 0.4377161778888277.\n",
      "[I 2025-11-21 16:22:53,672] Trial 39 finished with value: 0.45057707385935303 and parameters: {'learning_rate': 0.07692220877479171, 'subsample': 0.4931263223454522, 'colsample_bytree': 0.38450948301304494, 'min_child_samples': 43, 'num_leaves': 761}. Best is trial 35 with value: 0.4377161778888277.\n",
      "[I 2025-11-21 16:22:54,061] Trial 40 finished with value: 0.499769629399527 and parameters: {'learning_rate': 0.003014274727230161, 'subsample': 0.460642860715639, 'colsample_bytree': 0.3067698679263518, 'min_child_samples': 48, 'num_leaves': 501}. Best is trial 35 with value: 0.4377161778888277.\n",
      "[I 2025-11-21 16:22:54,442] Trial 41 finished with value: 0.4350363678628525 and parameters: {'learning_rate': 0.053592095481482166, 'subsample': 0.47526669170357255, 'colsample_bytree': 0.42220385944882993, 'min_child_samples': 39, 'num_leaves': 711}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:54,733] Trial 42 finished with value: 0.4570860935401379 and parameters: {'learning_rate': 0.06273439297188611, 'subsample': 0.4743738247970897, 'colsample_bytree': 0.4313996046290791, 'min_child_samples': 58, 'num_leaves': 705}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:55,035] Trial 43 finished with value: 0.5127803359955099 and parameters: {'learning_rate': 0.04622925368367998, 'subsample': 0.4479455160957899, 'colsample_bytree': 0.12988133261251145, 'min_child_samples': 39, 'num_leaves': 653}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:55,395] Trial 44 finished with value: 0.4520216371501099 and parameters: {'learning_rate': 0.039923542401704225, 'subsample': 0.4735474167684242, 'colsample_bytree': 0.45702245838486066, 'min_child_samples': 47, 'num_leaves': 808}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:55,760] Trial 45 finished with value: 0.4585698306925107 and parameters: {'learning_rate': 0.01862919735728007, 'subsample': 0.49541169144530306, 'colsample_bytree': 0.37714123053854537, 'min_child_samples': 41, 'num_leaves': 996}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:56,021] Trial 46 finished with value: 0.46723986030915937 and parameters: {'learning_rate': 0.08143598912281705, 'subsample': 0.4066576531053914, 'colsample_bytree': 0.4271632147716544, 'min_child_samples': 72, 'num_leaves': 432}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:56,425] Trial 47 finished with value: 0.4744042019838017 and parameters: {'learning_rate': 0.06692279023681458, 'subsample': 0.4696025476030517, 'colsample_bytree': 0.3451959185331103, 'min_child_samples': 88, 'num_leaves': 736}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:56,786] Trial 48 finished with value: 0.4700499967336279 and parameters: {'learning_rate': 0.05211354781900806, 'subsample': 0.44517254939779666, 'colsample_bytree': 0.3922725445735883, 'min_child_samples': 64, 'num_leaves': 870}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:57,227] Trial 49 finished with value: 0.46747174556821686 and parameters: {'learning_rate': 0.013537015144585336, 'subsample': 0.4973120081461907, 'colsample_bytree': 0.2582447508186682, 'min_child_samples': 50, 'num_leaves': 681}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:57,654] Trial 50 finished with value: 0.5351653910637607 and parameters: {'learning_rate': 0.001113717572032855, 'subsample': 0.37272577569360094, 'colsample_bytree': 0.4760627894157579, 'min_child_samples': 37, 'num_leaves': 511}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:58,286] Trial 51 finished with value: 0.44462653361882304 and parameters: {'learning_rate': 0.05386181933986789, 'subsample': 0.4150123887681665, 'colsample_bytree': 0.4227892201080663, 'min_child_samples': 35, 'num_leaves': 589}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:58,714] Trial 52 finished with value: 0.4541022446729782 and parameters: {'learning_rate': 0.039698329877918936, 'subsample': 0.23973845003841482, 'colsample_bytree': 0.4461247811375621, 'min_child_samples': 23, 'num_leaves': 637}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:59,243] Trial 53 finished with value: 0.4536196777856762 and parameters: {'learning_rate': 0.08841022084971306, 'subsample': 0.43499723380940003, 'colsample_bytree': 0.4131866559335381, 'min_child_samples': 44, 'num_leaves': 710}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:22:59,770] Trial 54 finished with value: 0.4559363429239926 and parameters: {'learning_rate': 0.06888172558661687, 'subsample': 0.4739443280425542, 'colsample_bytree': 0.36101012251582254, 'min_child_samples': 39, 'num_leaves': 573}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:23:00,392] Trial 55 finished with value: 0.43635166888938753 and parameters: {'learning_rate': 0.024699173414371754, 'subsample': 0.4532245935509078, 'colsample_bytree': 0.479552754185874, 'min_child_samples': 28, 'num_leaves': 521}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:23:00,966] Trial 56 finished with value: 0.43583269346521675 and parameters: {'learning_rate': 0.02372028072370049, 'subsample': 0.4800941293102379, 'colsample_bytree': 0.4842767841825664, 'min_child_samples': 28, 'num_leaves': 297}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:23:01,449] Trial 57 finished with value: 0.43637220358923573 and parameters: {'learning_rate': 0.02267891948732951, 'subsample': 0.48134848703791305, 'colsample_bytree': 0.485577737257115, 'min_child_samples': 27, 'num_leaves': 126}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:23:01,866] Trial 58 finished with value: 0.4379977624180333 and parameters: {'learning_rate': 0.02365981202805777, 'subsample': 0.48240164138235786, 'colsample_bytree': 0.4876154065023363, 'min_child_samples': 28, 'num_leaves': 178}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:23:02,140] Trial 59 finished with value: 0.49003793198500206 and parameters: {'learning_rate': 0.021341870885220104, 'subsample': 0.09777835082231806, 'colsample_bytree': 0.49101263934987976, 'min_child_samples': 28, 'num_leaves': 111}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:23:02,586] Trial 60 finished with value: 0.4666835604982235 and parameters: {'learning_rate': 0.011373324413229677, 'subsample': 0.16351439780768762, 'colsample_bytree': 0.4798039957160988, 'min_child_samples': 22, 'num_leaves': 234}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:23:03,189] Trial 61 finished with value: 0.43538731094259264 and parameters: {'learning_rate': 0.025136148701684312, 'subsample': 0.48226100995738247, 'colsample_bytree': 0.4609592343485887, 'min_child_samples': 28, 'num_leaves': 130}. Best is trial 41 with value: 0.4350363678628525.\n",
      "[I 2025-11-21 16:23:03,797] Trial 62 finished with value: 0.42014872405310155 and parameters: {'learning_rate': 0.025076400520261857, 'subsample': 0.48136029577521056, 'colsample_bytree': 0.4581941479625377, 'min_child_samples': 20, 'num_leaves': 135}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:04,597] Trial 63 finished with value: 0.42938443310297053 and parameters: {'learning_rate': 0.016426087682754262, 'subsample': 0.4566488347352516, 'colsample_bytree': 0.4556767954225494, 'min_child_samples': 20, 'num_leaves': 109}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:05,252] Trial 64 finished with value: 0.4433318825854681 and parameters: {'learning_rate': 0.008334029082619964, 'subsample': 0.45699862213521586, 'colsample_bytree': 0.47587196033862356, 'min_child_samples': 20, 'num_leaves': 122}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:05,946] Trial 65 finished with value: 0.43775066142426694 and parameters: {'learning_rate': 0.016371900471575485, 'subsample': 0.4583044601986897, 'colsample_bytree': 0.4629397937643391, 'min_child_samples': 24, 'num_leaves': 75}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:06,542] Trial 66 finished with value: 0.4361187221859765 and parameters: {'learning_rate': 0.017043568309508164, 'subsample': 0.47777591969012995, 'colsample_bytree': 0.45580890293737925, 'min_child_samples': 25, 'num_leaves': 278}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:07,197] Trial 67 finished with value: 0.4361945343078869 and parameters: {'learning_rate': 0.017644849560939396, 'subsample': 0.4225445771223221, 'colsample_bytree': 0.4500527643810497, 'min_child_samples': 20, 'num_leaves': 281}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:07,785] Trial 68 finished with value: 0.4303228927744931 and parameters: {'learning_rate': 0.017121397816868614, 'subsample': 0.4286125059322883, 'colsample_bytree': 0.45737939264061844, 'min_child_samples': 20, 'num_leaves': 291}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:08,220] Trial 69 finished with value: 0.44115638698345633 and parameters: {'learning_rate': 0.013332015113264453, 'subsample': 0.4703398233082029, 'colsample_bytree': 0.4598346301458862, 'min_child_samples': 24, 'num_leaves': 330}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:09,135] Trial 70 finished with value: 0.44308644838500955 and parameters: {'learning_rate': 0.014898301077388128, 'subsample': 0.3963988487201868, 'colsample_bytree': 0.4323017834011807, 'min_child_samples': 23, 'num_leaves': 216}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:09,903] Trial 71 finished with value: 0.4287063288415011 and parameters: {'learning_rate': 0.018446524034688293, 'subsample': 0.4197871737945318, 'colsample_bytree': 0.446217420824492, 'min_child_samples': 20, 'num_leaves': 271}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:10,345] Trial 72 finished with value: 0.4445428802407598 and parameters: {'learning_rate': 0.010728026489663202, 'subsample': 0.49998188506312835, 'colsample_bytree': 0.45431402793774645, 'min_child_samples': 25, 'num_leaves': 240}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:10,895] Trial 73 finished with value: 0.45491205909010934 and parameters: {'learning_rate': 0.03332040360600946, 'subsample': 0.41628174326825323, 'colsample_bytree': 0.4039630076307149, 'min_child_samples': 31, 'num_leaves': 314}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:11,455] Trial 74 finished with value: 0.4215266214137319 and parameters: {'learning_rate': 0.028348827409211578, 'subsample': 0.44441853575068074, 'colsample_bytree': 0.4373882131020728, 'min_child_samples': 20, 'num_leaves': 45}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:11,913] Trial 75 finished with value: 0.43198045481428815 and parameters: {'learning_rate': 0.027091712684707964, 'subsample': 0.44589156552932935, 'colsample_bytree': 0.43401242514947463, 'min_child_samples': 21, 'num_leaves': 51}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:12,565] Trial 76 finished with value: 0.42873525042280974 and parameters: {'learning_rate': 0.019859130336736645, 'subsample': 0.4321082716742556, 'colsample_bytree': 0.4363796082598203, 'min_child_samples': 21, 'num_leaves': 39}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:13,098] Trial 77 finished with value: 0.4404805468416916 and parameters: {'learning_rate': 0.028625992963205827, 'subsample': 0.42991112492694045, 'colsample_bytree': 0.4007412853225355, 'min_child_samples': 20, 'num_leaves': 51}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:13,691] Trial 78 finished with value: 0.44670018300473785 and parameters: {'learning_rate': 0.01996673211822791, 'subsample': 0.3920744922999444, 'colsample_bytree': 0.3866982203898095, 'min_child_samples': 22, 'num_leaves': 77}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:14,100] Trial 79 finished with value: 0.43999474963830454 and parameters: {'learning_rate': 0.015176456324180451, 'subsample': 0.4029133584333072, 'colsample_bytree': 0.44075717146489335, 'min_child_samples': 22, 'num_leaves': 27}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:14,534] Trial 80 finished with value: 0.45858981297338525 and parameters: {'learning_rate': 0.00842640260722713, 'subsample': 0.3451828489974794, 'colsample_bytree': 0.3709843257753698, 'min_child_samples': 20, 'num_leaves': 152}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:14,935] Trial 81 finished with value: 0.43617714805439556 and parameters: {'learning_rate': 0.026740288270578217, 'subsample': 0.4368389368521033, 'colsample_bytree': 0.43167765360072924, 'min_child_samples': 25, 'num_leaves': 84}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:15,403] Trial 82 finished with value: 0.4338117329136626 and parameters: {'learning_rate': 0.03172335618898072, 'subsample': 0.44587949391710274, 'colsample_bytree': 0.466668944766839, 'min_child_samples': 22, 'num_leaves': 44}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:16,095] Trial 83 finished with value: 0.43259003445332467 and parameters: {'learning_rate': 0.03118773035968867, 'subsample': 0.4461095103602265, 'colsample_bytree': 0.4378720271594881, 'min_child_samples': 22, 'num_leaves': 44}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:16,722] Trial 84 finished with value: 0.45729210924348224 and parameters: {'learning_rate': 0.0314855519301292, 'subsample': 0.41034324379360004, 'colsample_bytree': 0.4387191122243962, 'min_child_samples': 23, 'num_leaves': 5}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:17,169] Trial 85 finished with value: 0.43395979747969293 and parameters: {'learning_rate': 0.020678683991104212, 'subsample': 0.4456676625406879, 'colsample_bytree': 0.49926769417347716, 'min_child_samples': 22, 'num_leaves': 35}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:17,631] Trial 86 finished with value: 0.4418402805576459 and parameters: {'learning_rate': 0.03760905684274078, 'subsample': 0.4266627124366876, 'colsample_bytree': 0.46803081102718974, 'min_child_samples': 30, 'num_leaves': 62}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:18,132] Trial 87 finished with value: 0.44991832408785853 and parameters: {'learning_rate': 0.01965167847945125, 'subsample': 0.3788864022379647, 'colsample_bytree': 0.4165630981384861, 'min_child_samples': 26, 'num_leaves': 154}. Best is trial 62 with value: 0.42014872405310155.\n",
      "[I 2025-11-21 16:23:18,594] Trial 88 finished with value: 0.418099151103134 and parameters: {'learning_rate': 0.02967900426626908, 'subsample': 0.46364572682407335, 'colsample_bytree': 0.4107961679403244, 'min_child_samples': 20, 'num_leaves': 181}. Best is trial 88 with value: 0.418099151103134.\n",
      "[I 2025-11-21 16:23:19,051] Trial 89 finished with value: 0.42210505320880665 and parameters: {'learning_rate': 0.027979011013985228, 'subsample': 0.42087749342053765, 'colsample_bytree': 0.4095763971310802, 'min_child_samples': 20, 'num_leaves': 193}. Best is trial 88 with value: 0.418099151103134.\n",
      "[I 2025-11-21 16:23:19,506] Trial 90 finished with value: 0.4428766435364821 and parameters: {'learning_rate': 0.012259682419929096, 'subsample': 0.4614787028223032, 'colsample_bytree': 0.4090802273573943, 'min_child_samples': 20, 'num_leaves': 199}. Best is trial 88 with value: 0.418099151103134.\n",
      "[I 2025-11-21 16:23:19,919] Trial 91 finished with value: 0.4349726583633044 and parameters: {'learning_rate': 0.029797373946091915, 'subsample': 0.4215867689535292, 'colsample_bytree': 0.43861694403588025, 'min_child_samples': 25, 'num_leaves': 103}. Best is trial 88 with value: 0.418099151103134.\n",
      "[I 2025-11-21 16:23:20,386] Trial 92 finished with value: 0.42892617342403627 and parameters: {'learning_rate': 0.027078851342445504, 'subsample': 0.43507291247496416, 'colsample_bytree': 0.43350184514903894, 'min_child_samples': 21, 'num_leaves': 169}. Best is trial 88 with value: 0.418099151103134.\n",
      "[I 2025-11-21 16:23:20,897] Trial 93 finished with value: 0.4387265958428051 and parameters: {'learning_rate': 0.025743618214524613, 'subsample': 0.4638011570764096, 'colsample_bytree': 0.3963371092574062, 'min_child_samples': 20, 'num_leaves': 163}. Best is trial 88 with value: 0.418099151103134.\n",
      "[I 2025-11-21 16:23:21,373] Trial 94 finished with value: 0.4301751253834087 and parameters: {'learning_rate': 0.03549373724618361, 'subsample': 0.4369847851580642, 'colsample_bytree': 0.42335878331673354, 'min_child_samples': 24, 'num_leaves': 249}. Best is trial 88 with value: 0.418099151103134.\n",
      "[I 2025-11-21 16:23:22,068] Trial 95 finished with value: 0.4416876158008614 and parameters: {'learning_rate': 0.041851178014467964, 'subsample': 0.43463906929703844, 'colsample_bytree': 0.41513131992359087, 'min_child_samples': 30, 'num_leaves': 199}. Best is trial 88 with value: 0.418099151103134.\n",
      "[I 2025-11-21 16:23:22,655] Trial 96 finished with value: 0.509805118434688 and parameters: {'learning_rate': 0.0153524684847997, 'subsample': 0.3898263184266103, 'colsample_bytree': 0.11479400014345628, 'min_child_samples': 24, 'num_leaves': 260}. Best is trial 88 with value: 0.418099151103134.\n",
      "[I 2025-11-21 16:23:23,076] Trial 97 finished with value: 0.4447261746905309 and parameters: {'learning_rate': 0.03530761895483818, 'subsample': 0.40262589376625196, 'colsample_bytree': 0.4503849846998152, 'min_child_samples': 26, 'num_leaves': 170}. Best is trial 88 with value: 0.418099151103134.\n",
      "[I 2025-11-21 16:23:23,487] Trial 98 finished with value: 0.48350813420006644 and parameters: {'learning_rate': 0.02193815881261892, 'subsample': 0.36948998299484515, 'colsample_bytree': 0.37714126615986726, 'min_child_samples': 85, 'num_leaves': 352}. Best is trial 88 with value: 0.418099151103134.\n",
      "[I 2025-11-21 16:23:24,014] Trial 99 finished with value: 0.4574962027341036 and parameters: {'learning_rate': 0.01764278488119714, 'subsample': 0.4126330794837564, 'colsample_bytree': 0.2957732251789356, 'min_child_samples': 32, 'num_leaves': 240}. Best is trial 88 with value: 0.418099151103134.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE 0.418099151103134\n",
      "Best params: {'learning_rate': 0.02967900426626908, 'subsample': 0.46364572682407335, 'colsample_bytree': 0.4107961679403244, 'min_child_samples': 20, 'num_leaves': 181}\n"
     ]
    }
   ],
   "source": [
    "# Assign \"lgbm-wine-1\" as the study_name to the Optuna study in the first assignment.\n",
    "study_name_1 = \"lgbm-wine-1\"\n",
    "\n",
    "# For this assignment, it is enough to use a simple sqlite3 database for persisting study history\n",
    "storage = \"sqlite:///optuna.sqlite3\"\n",
    "\n",
    "# When grading the notebook, the study will be loaded from the submitted database\n",
    "study_1 = (\n",
    "    run_study(\n",
    "        study_name=study_name_1,\n",
    "        storage=storage,\n",
    "        objective_func=objective_func,\n",
    "        n_trials=100,\n",
    "    )\n",
    "    if not is_being_graded()\n",
    "    else optuna.load_study(study_name=study_name_1, storage=storage)\n",
    ")\n",
    "\n",
    "print(\"Best MAE\", study_1.best_value)\n",
    "print(\"Best params:\", study_1.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af98d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3807826a6ee63298feed0ad35eafa194",
     "grade": false,
     "grade_id": "cell-999478c9b4d31507",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Example output:\n",
    "```text\n",
    "Best MAE 0.418099151103134\n",
    "Best params: {'learning_rate': 0.02967900426626908, 'subsample': 0.46364572682407335, 'colsample_bytree': 0.4107961679403244, 'min_child_samples': 20, 'num_leaves': 181}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "493a7dfd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aad0e00be261349e8b64ea11de40b77b",
     "grade": true,
     "grade_id": "cell-3392ef3c9a8216df",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the optimization results\n",
    "\n",
    "# The required hyperparameters should be optimized\n",
    "for param in [\"learning_rate\", \"subsample\", \"colsample_bytree\", \"min_child_samples\", \"num_leaves\"]:\n",
    "    assert param in study_1.best_trial.params, f\"Missing hyperparameter {param} from the objective function\"\n",
    "\n",
    "# The searching ranges should be correct\n",
    "assert study_1.best_trial.distributions.get(\"learning_rate\").log is True, \"learning_rate should be searched in a log scale\"\n",
    "for param in [\"learning_rate\", \"subsample\", \"colsample_bytree\"]:\n",
    "    assert isinstance(study_1.best_trial.distributions.get(param), optuna.distributions.FloatDistribution), f\"{param} should be a float number\"\n",
    "\n",
    "for param in [\"min_child_samples\", \"num_leaves\"]:\n",
    "    assert isinstance(study_1.best_trial.distributions.get(param), optuna.distributions.IntDistribution), f\"{param} should be an integer\"\n",
    "\n",
    "# The study should perform 100 trials\n",
    "assert len(study_1.trials) == 100, \"Wrong number of trials\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27d95f03",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4c55c2ea5bb1d1ae5894a5d4b034c2d",
     "grade": true,
     "grade_id": "cell-94d2125b6d76a2b7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the Optuna study results\n",
    "assert study_1.best_value < 0.42, \"Too large MAE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff141c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f54a9f42c6cdd8c40a02afe2f80cbf8c",
     "grade": false,
     "grade_id": "cell-a361e0c06af0a9fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 2: Analyzing an Optuna study (2 points)\n",
    "Optuna offers utility functions for visualizing the optimization process (i.e., study history). For example, it can plot the hyperparameter importance and the relationship between a hyperparameter and the objective. In this assignment, you need to analyze the study created in Assignment 1, adjust the search ranges of some hyperparameters to obtain better MAE. Detailed instructions will be provided later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffedc3bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b20258ee0a86bd6d27553b9d85fb2be",
     "grade": false,
     "grade_id": "cell-6143e15920dad03d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "# Configure Jupyter Notebook to render plotly figures drawn by Optuna\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7db4ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccc1b74b10e62c86d1bda2031f145c21",
     "grade": false,
     "grade_id": "cell-00c82c5f09579eb9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2a) Hyperparameter importance\n",
    "Implement a function `show_param_importances` that loads an Optuna study from a storage and return a plot showing importance of the hyperparameters in the study. Similar to the tutorial, use the [FanovaImportanceEvaluator](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.FanovaImportanceEvaluator.html#optuna.importance.FanovaImportanceEvaluator) as the importance evaluator and set `RANDOM_STATE` as the seed for the evaluator for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88de9c8f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2caa9c80bd17e2cb75b6fd7ab1dc1330",
     "grade": false,
     "grade_id": "cell-56495b0b9ae677e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_30.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the hyperparameter importance\n",
    "def show_param_importances(study_name: str, storage: str) -> plotly.graph_objs.Figure:\n",
    "    \"\"\"\n",
    "    Plot the hyperparameter importance of a study.\n",
    "    Args:\n",
    "        study_name: The name of the study.\n",
    "        storage: The URI of the storage used to save the study history.\n",
    "    Returns:\n",
    "        A plotly Figure object.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "\n",
    "    # The notebook-setting was displaying a blank page, so I changed it to iframe\n",
    "    pio.renderers.default = \"iframe\"\n",
    "    \n",
    "    loaded_study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "    fig = optuna.visualization.plot_param_importances(loaded_study, evaluator=optuna.importance.FanovaImportanceEvaluator(seed=RANDOM_SEED))\n",
    "        \n",
    "    return fig\n",
    "    ### END CODE HERE\n",
    "\n",
    "param_importance_fig_wine = show_param_importances(study_name=study_name_1, storage=storage)\n",
    "param_importance_fig_wine.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f056c23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7287095cde0bbee523b24599ad476da0",
     "grade": false,
     "grade_id": "cell-d181f40585b27273",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, put the names of the three most important hyperparameters in to a list named `important_hyperparams`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a96cad0a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57bab6be38c1392a94fa47119a5c4db4",
     "grade": false,
     "grade_id": "cell-1fec64bc9fcdc304",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: important_hyperparams = ...\n",
    "### START CODE HERE\n",
    "important_hyperparams = [\n",
    "    \"subsample\",\n",
    "    \"colsample_bytree\",\n",
    "    \"min_child_samples\"\n",
    "]\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a5ef1b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f180c87336301806e432157c4c14ddb3",
     "grade": true,
     "grade_id": "cell-0ac20cc5651d27ca",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(param_importance_fig_wine, plotly.graph_objs.Figure), \"Incorrect importance plot\"\n",
    "assert len(important_hyperparams) == 3, \"Incorrect number of important hyperparameters\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a8161",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f2fc21a2710944e798c2e8a65e66726",
     "grade": false,
     "grade_id": "cell-78714ff483c9bad2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2b) The impact of hyperparameters\n",
    "Complete the `plot_relationship_between_hyperparams_and_obj` function that loads an Optuna study from a storage and return a plot showing the relationships of the most three important hyperparameters and the objective in a slice plot. \n",
    "\n",
    "Hint: [How to plot the relationship between a hyperparameter and the objective?](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_slice.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f13dd0bf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "615724b6a8d3a40ea54f8ad55bbbad11",
     "grade": false,
     "grade_id": "cell-cd0ab0fecac2d570",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_31.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_relationship_between_hyperparams_and_obj(study_name: str, storage: str, important_hyperparams: List[str]) -> plotly.graph_objs.Figure:\n",
    "    \"\"\"\n",
    "    Plot the relationship between the hyperparameters and the objective function.\n",
    "    Args:\n",
    "        study_name: The name of the study.\n",
    "        storage: The URI of the storage used to save the study history.\n",
    "        important_hyperparams: A list of hyperparameters that are considered important.\n",
    "    Returns:\n",
    "        A plotly Figure object.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    loaded_study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "\n",
    "    fig = optuna.visualization.plot_slice(loaded_study, params=important_hyperparams)\n",
    "\n",
    "    return fig\n",
    "    ### END CODE HERE\n",
    "\n",
    "slice_fig_wine = show_relationship_between_hyperparams_and_obj(study_name=study_name_1, storage=storage, important_hyperparams=important_hyperparams)\n",
    "slice_fig_wine.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af58d9a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7cd8e85416d3c701bd43b26b62a0156",
     "grade": true,
     "grade_id": "cell-47c46025a5acbdea",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(slice_fig_wine, plotly.graph_objs.Figure), \"Incorrect slice plot\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e84985c-4c3c-4ade-b9b8-2f9366ca7955",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "108a203e92ad19ed3a79030341f9e083",
     "grade": false,
     "grade_id": "cell-6eea914c144ea5bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question for Assignment 2b\n",
    "Now you know the most three important hyperparameters that affect the objective value. Looking at the positions of the points in the slice plot, which area the points that resulted in better MAE are concentrated on? How would you adjust the search ranges of these three hyperparameters in the next study? Please put your answer in your PDF file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7b6a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7be6c85098b5a7bc31d5b4009c4cd746",
     "grade": false,
     "grade_id": "cell-13a5f28b8234856a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2c) An improved Optuna study\n",
    "In this assignment, you have two tasks. First, complete another objective function `objective_func_2`, where you need to adjust the search ranges of the most three important hyperparameters to improve MAE (keep values/search ranges of other hyperparameters as the same as in Assignment 1).\n",
    "\n",
    "Then complete the `run_improved_study` that starts and runs another Optuna study. Detailed requirements are listed below:\n",
    "- Use [TPESampler](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html) as the sampler for the hyperparameter sampling and use `RANDOM_SEED` as the seed of the sampler. \n",
    "- The Optuna study should have a name specified by the `new_study_name` argument, use the objective function given as the `objective_func` argument and perform `n_trials` trials.\n",
    "- The study history should be persisted in a relational database specified by the `storage` argument so that the study can be loaded and analyzed later. \n",
    "- You should start the study using the best hyperparameter combination of a previous study. The previous study should be loaded from the given storage using the study name given as the `prev_study_name` argument.\n",
    "- The trials should be tracked in an MLflow experiment. \n",
    "- The function should finally return the study.\n",
    "\n",
    "**Hints**: \n",
    "- Now you know the most three important hyperparameters that affect the objective value. Looking at the positions of the points in the slice plot, which areas the points that resulted in better MAE are concentrated on? \n",
    "- [How to log Optuna trials to MLflow?](https://optuna-integration.readthedocs.io/en/stable/reference/generated/optuna_integration.MLflowCallback.html)\n",
    "- To start the new study using the best hyperparameter combination of the previous study, you can 1) load the previous study whose name is given as the `prev_study_name` argument, 2) retrieve the optimal hyperparameters combination of the previous study, 3) create a new study, 4) insert a trial with the best hyperparameter values of the previous study into the new study, and then start the optimization of the new study.\n",
    "\n",
    "\n",
    "You may also find the following links helpful:\n",
    "- [optuna.load_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.load_study.html)\n",
    "- [study.best_params](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.best_params)\n",
    "- [enqueue_trial](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.enqueue_trial). \n",
    "\n",
    "**Note**: If you want to delete all the trials in MLflow, don't delete the experiment, as deleting the experiment using the UI will not really delete the experiment in the PostgreSQL database used by MLflow, which will cause problems when recording trials under an experiment with the same name as the deleted experiment. Instead, you can keep the experiment and delete the trials, as shown in the image below. (If you feel like permanently deleting MLflow experiments from the PostgreSQL database, please check the `delete_from_mlflow.md` file in Week1 tutorials.\n",
    "\n",
    "![](./images/mlflow-delete-trials.jpg)\n",
    "\n",
    "<details>\n",
    "    <summary>If callback and decorator are new to you...</summary>\n",
    "    <p>Briefly speaking, both callbacks and decorators are used to modify or enhance the behavior of another function without modifying its original source code. A callback is a function that is typically provided as an argument to another function. A decorator typically takes another function as an argument. Feel free to google more about them. </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63ee89bc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c367b4cdebe2f19fc1361949d0dc020",
     "grade": false,
     "grade_id": "cell-a0685c1f324066ae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def improved_objective_func(trial):\n",
    "    ### START CODE HERE\n",
    "    params = {\n",
    "        \"n_estimators\": 1000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.375, 0.5),\n",
    "        \"subsample_freq\": 1,\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 0.5),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 30),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 2**10),\n",
    "        \"random_state\": RANDOM_SEED\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(verbose=-1, **params)\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    pred = model.predict(test_x)\n",
    "\n",
    "    score = mean_absolute_error(test_y, pred)\n",
    "    return score\n",
    "    ### END CODE HERE\n",
    "\n",
    "def run_improved_study(prev_study_name: str, new_study_name: str, storage: str, objective_func: callable, n_trials: int) -> optuna.study.Study:\n",
    "    \"\"\"\n",
    "    Create and run an Optuna study based on a previous study.\n",
    "    Args:\n",
    "        prev_study_name: The name of the previous study.\n",
    "        new_study_name: The name of the new study.\n",
    "        storage: The URI of the storage used to save the study history.\n",
    "        objective_func: The objective function to be optimized.\n",
    "        n_trials: The number of trials the study should perform.\n",
    "    Returns:   \n",
    "        An Optuna Study object.\n",
    "    \"\"\"\n",
    "    # Some cleanup before starting the new study\n",
    "    # Delete the study if it already exists\n",
    "    if new_study_name in optuna.get_all_study_names(storage=storage):\n",
    "        optuna.delete_study(study_name=new_study_name, storage=storage)\n",
    "\n",
    "    mlflow_exp = mlflow.get_experiment_by_name(new_study_name)\n",
    "    if mlflow_exp is not None:\n",
    "        # Delete all old trials in the experiment\n",
    "        for run in mlflow.search_runs(mlflow_exp.experiment_id, output_format=\"list\"):\n",
    "            mlflow.delete_run(run.info.run_id)\n",
    "        \n",
    "    ### START CODE HERE\n",
    "    prev_study = optuna.load_study(study_name=prev_study_name, storage=storage)\n",
    "    best_params = prev_study.best_trial.params\n",
    "\n",
    "    mlflow_cb = MLflowCallback(\n",
    "        tracking_uri=mlflow_tracking_uri,\n",
    "        metric_name=\"mean_absolute_error\",\n",
    "        create_experiment=True,\n",
    "        mlflow_kwargs=None,\n",
    "        tag_study_user_attrs=False,\n",
    "        tag_trial_user_attrs=True\n",
    "    )\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
    "        study_name=new_study_name,\n",
    "        storage=storage,\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    study.enqueue_trial(best_params)\n",
    "\n",
    "    study.optimize(objective_func, n_trials=n_trials, callbacks=[mlflow_cb])\n",
    "\n",
    "    return study\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1cad27b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76e4c2be557ca64c561b5554d17c5ebf",
     "grade": false,
     "grade_id": "cell-d5ab13033714df18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7091/1801856260.py:51: ExperimentalWarning:\n",
      "\n",
      "MLflowCallback is experimental (supported from v1.4.0). The interface can change in the future.\n",
      "\n",
      "[I 2025-11-21 17:50:51,711] A new study created in RDB with name: lgbm-wine-2\n",
      "[I 2025-11-21 17:50:52,742] Trial 0 finished with value: 0.418099151103134 and parameters: {'learning_rate': 0.02967900426626908, 'subsample': 0.46364572682407335, 'colsample_bytree': 0.4107961679403244, 'min_child_samples': 20, 'num_leaves': 181}. Best is trial 0 with value: 0.418099151103134.\n",
      "2025/11/21 17:50:52 INFO mlflow.tracking.fluent: Experiment with name 'lgbm-wine-2' does not exist. Creating a new experiment.\n",
      "[I 2025-11-21 17:50:53,955] Trial 1 finished with value: 0.4532147378133376 and parameters: {'learning_rate': 0.005611516415334507, 'subsample': 0.4938392883012395, 'colsample_bytree': 0.47319939418114054, 'min_child_samples': 24, 'num_leaves': 161}. Best is trial 0 with value: 0.418099151103134.\n",
      "[I 2025-11-21 17:50:55,085] Trial 2 finished with value: 0.493698689183998 and parameters: {'learning_rate': 0.002051110418843397, 'subsample': 0.38226045152102495, 'colsample_bytree': 0.48661761457749353, 'min_child_samples': 24, 'num_leaves': 726}. Best is trial 0 with value: 0.418099151103134.\n",
      "[I 2025-11-21 17:50:56,223] Trial 3 finished with value: 0.5225941719504733 and parameters: {'learning_rate': 0.0010994335574766201, 'subsample': 0.4962387315202493, 'colsample_bytree': 0.4832442640800422, 'min_child_samples': 18, 'num_leaves': 188}. Best is trial 0 with value: 0.418099151103134.\n",
      "[I 2025-11-21 17:50:57,264] Trial 4 finished with value: 0.4830692207184903 and parameters: {'learning_rate': 0.002327067708383781, 'subsample': 0.4130302803699422, 'colsample_bytree': 0.4524756431632238, 'min_child_samples': 21, 'num_leaves': 299}. Best is trial 0 with value: 0.418099151103134.\n",
      "[I 2025-11-21 17:50:58,154] Trial 5 finished with value: 0.4404643197400949 and parameters: {'learning_rate': 0.01673808578875214, 'subsample': 0.39243673258150524, 'colsample_bytree': 0.4292144648535218, 'min_child_samples': 20, 'num_leaves': 468}. Best is trial 0 with value: 0.418099151103134.\n",
      "[I 2025-11-21 17:50:58,945] Trial 6 finished with value: 0.43093511444677207 and parameters: {'learning_rate': 0.037183641805732096, 'subsample': 0.39995922276979495, 'colsample_bytree': 0.4514234438413612, 'min_child_samples': 24, 'num_leaves': 49}. Best is trial 0 with value: 0.418099151103134.\n",
      "[I 2025-11-21 17:50:59,733] Trial 7 finished with value: 0.45921870458729286 and parameters: {'learning_rate': 0.016409286730647923, 'subsample': 0.3963155154609114, 'colsample_bytree': 0.406505159298528, 'min_child_samples': 30, 'num_leaves': 989}. Best is trial 0 with value: 0.418099151103134.\n",
      "[I 2025-11-21 17:51:01,112] Trial 8 finished with value: 0.437713138919372 and parameters: {'learning_rate': 0.041380401125610165, 'subsample': 0.4130767211466713, 'colsample_bytree': 0.40976721140063843, 'min_child_samples': 25, 'num_leaves': 452}. Best is trial 0 with value: 0.418099151103134.\n",
      "[I 2025-11-21 17:51:01,902] Trial 9 finished with value: 0.5124836327760078 and parameters: {'learning_rate': 0.0017541893487450805, 'subsample': 0.4368971137639088, 'colsample_bytree': 0.40343885211152186, 'min_child_samples': 29, 'num_leaves': 266}. Best is trial 0 with value: 0.418099151103134.\n",
      "[I 2025-11-21 17:51:03,019] Trial 10 finished with value: 0.4249154093139822 and parameters: {'learning_rate': 0.08737127108050576, 'subsample': 0.46589600852135227, 'colsample_bytree': 0.43076942114502625, 'min_child_samples': 15, 'num_leaves': 668}. Best is trial 0 with value: 0.418099151103134.\n",
      "[I 2025-11-21 17:51:04,482] Trial 11 finished with value: 0.42625404273874823 and parameters: {'learning_rate': 0.0739621343495483, 'subsample': 0.4681126633335087, 'colsample_bytree': 0.4277577971119243, 'min_child_samples': 15, 'num_leaves': 757}. Best is trial 0 with value: 0.418099151103134.\n",
      "[I 2025-11-21 17:51:06,574] Trial 12 finished with value: 0.41564107968727415 and parameters: {'learning_rate': 0.09800121104392304, 'subsample': 0.4621444872925118, 'colsample_bytree': 0.4285854308445099, 'min_child_samples': 15, 'num_leaves': 691}. Best is trial 12 with value: 0.41564107968727415.\n",
      "[I 2025-11-21 17:51:08,101] Trial 13 finished with value: 0.43012005699706213 and parameters: {'learning_rate': 0.03641686745841245, 'subsample': 0.45822498575285425, 'colsample_bytree': 0.420277152601501, 'min_child_samples': 18, 'num_leaves': 932}. Best is trial 12 with value: 0.41564107968727415.\n",
      "[I 2025-11-21 17:51:09,379] Trial 14 finished with value: 0.44203007382147147 and parameters: {'learning_rate': 0.007189221919431304, 'subsample': 0.4469005326847222, 'colsample_bytree': 0.43862676602380063, 'min_child_samples': 18, 'num_leaves': 554}. Best is trial 12 with value: 0.41564107968727415.\n",
      "[I 2025-11-21 17:51:09,934] Trial 15 finished with value: 0.46244892701763474 and parameters: {'learning_rate': 0.01798521023135952, 'subsample': 0.4740802637384785, 'colsample_bytree': 0.41727707702097655, 'min_child_samples': 16, 'num_leaves': 5}. Best is trial 12 with value: 0.41564107968727415.\n",
      "[I 2025-11-21 17:51:10,663] Trial 16 finished with value: 0.42209197457207426 and parameters: {'learning_rate': 0.09970682275036434, 'subsample': 0.4342767690549798, 'colsample_bytree': 0.463669592790689, 'min_child_samples': 21, 'num_leaves': 865}. Best is trial 12 with value: 0.41564107968727415.\n",
      "[I 2025-11-21 17:51:11,412] Trial 17 finished with value: 0.4313970938627883 and parameters: {'learning_rate': 0.056947727925534604, 'subsample': 0.4838860872362892, 'colsample_bytree': 0.4418277676312869, 'min_child_samples': 27, 'num_leaves': 597}. Best is trial 12 with value: 0.41564107968727415.\n",
      "[I 2025-11-21 17:51:12,388] Trial 18 finished with value: 0.42551746833153586 and parameters: {'learning_rate': 0.02466131466334866, 'subsample': 0.45262261587620967, 'colsample_bytree': 0.4197035750858817, 'min_child_samples': 17, 'num_leaves': 405}. Best is trial 12 with value: 0.41564107968727415.\n",
      "[I 2025-11-21 17:51:13,322] Trial 19 finished with value: 0.4436805635505568 and parameters: {'learning_rate': 0.008707666927007677, 'subsample': 0.4228503738211882, 'colsample_bytree': 0.4969269852048718, 'min_child_samples': 20, 'num_leaves': 835}. Best is trial 12 with value: 0.41564107968727415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE 0.41564107968727415\n",
      "Best params: {'learning_rate': 0.09800121104392304, 'subsample': 0.4621444872925118, 'colsample_bytree': 0.4285854308445099, 'min_child_samples': 15, 'num_leaves': 691}\n"
     ]
    }
   ],
   "source": [
    "# Name of the new study\n",
    "new_study_name = \"lgbm-wine-2\"\n",
    "\n",
    "study_2 = (\n",
    "    run_improved_study(\n",
    "        prev_study_name=study_name_1,\n",
    "        new_study_name=new_study_name,\n",
    "        storage=storage,\n",
    "        objective_func=improved_objective_func,\n",
    "        n_trials=20,\n",
    "    )\n",
    "    if not is_being_graded()\n",
    "    else optuna.load_study(study_name=new_study_name, storage=storage)\n",
    ")\n",
    "print(\"Best MAE\", study_2.best_value)\n",
    "print(\"Best params:\", study_2.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7014c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba50d3ccb2c5ebc45d94186f81e6bdfa",
     "grade": false,
     "grade_id": "cell-f4730c7f565c1394",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Example output:\n",
    "```text\n",
    "Best MAE 0.386448234091213\n",
    "Best params: {'learning_rate': 0.020120960308154467, 'subsample': 0.9215993238868946, 'colsample_bytree': 0.825439643398257, 'min_child_samples': 10, 'num_leaves': 855}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eac0d1d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cc8c03f6b53737bff7db85b98164399",
     "grade": true,
     "grade_id": "cell-37fd5bed7f79740e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# New MAE should be smaller than the previous one\n",
    "assert study_2.best_value < study_1.best_value, \"The new MAE should be smaller than the previous one\"\n",
    "\n",
    "# The new study should have 20 trials\n",
    "assert len(study_2.trials) == 20, \"Wrong number of trials\"\n",
    "\n",
    "# The first trial should be the same as the best trial of the previous study\n",
    "assert study_2.trials[0].values[0] == study_1.best_value\n",
    "\n",
    "# The ranges of at least one of the three most important hyperparameters should be changed\n",
    "param_changed = False\n",
    "for param in important_hyperparams:\n",
    "    new_distribution = study_2.best_trial.distributions.get(param)\n",
    "    old_distribution = study_1.best_trial.distributions.get(param)\n",
    "    if new_distribution.low != old_distribution.low or new_distribution.high != old_distribution.high:\n",
    "        param_changed = True\n",
    "        break\n",
    "assert param_changed, \"The ranges of at least one of the three most important hyperparameters should be changed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a249bbab-e23e-4c8c-a411-82f80b5630cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bb2724a1e489a74aea93c570c6234ca",
     "grade": false,
     "grade_id": "cell-e817787ef6626f78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Screenshot for Assignment 2c\n",
    "Capture a screenshot of the trails of the \"lgbm-wine-2\" study in MLflow UI. Please include the metrics and parameters of each trial in your screenshot.\n",
    "\n",
    "<details>\n",
    "    <summary>Example:</summary>\n",
    "    <img src=\"./images/trials-mlflow.png\" width=1000/>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a66a70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0890502f27353e504ec7bb2e8131257",
     "grade": false,
     "grade_id": "cell-3b864625da24f09c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 3: More about MLflow (2 points)\n",
    "### 3a) Find the MLflow run with the best hyperparameter combination\n",
    "In assignment 2c), you found the best hyperparameter combination for your model. Now, your task is to complete the `find_best_run_id` function. The function receives an MLflow Experiment name as the argument and returns the best MLflow Run ID that resulted in the best hyperparameter combination. In other words, this function should find the MLflow Run with the smallest MAE. \n",
    "\n",
    "You may find the following MLflow docs useful:\n",
    "- [How to retrieve an MLflow Experiment given an Experiment name?](https://mlflow.org/docs/2.3.2/python_api/mlflow.html?highlight=get_experiment_by_name#mlflow.get_experiment_by_name)\n",
    "- [How to search MLflow Runs inside an MLflow Experiment?](https://mlflow.org/docs/2.3.2/python_api/mlflow.html?highlight=search_runs#mlflow.search_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98b8e2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f770718234ce98bb7d9e903b9fc45d0a",
     "grade": false,
     "grade_id": "cell-49f6069d6e076b5c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_best_run_id(mlflow_experiment_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Find the ID of the MLflow run with the smallest MAE\n",
    "    Args:\n",
    "        mlflow_experiment_name: The name of the MLflow experiment where the run should be found\n",
    "    Return:\n",
    "        An MLflow run ID\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b92b09",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78cb191d3641479f7afbc9241829a38d",
     "grade": true,
     "grade_id": "cell-5aa7e5ba272a4100",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# From your MLflow UI you can check whether the printed MLflow Run ID really produced the smallest MAE\n",
    "if not is_being_graded():\n",
    "    best_run_id = find_best_run_id(mlflow_experiment_name=new_study_name)\n",
    "    print(f\"The best MLflow Run ID is: {best_run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee8d8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7defc2d30672332f15cbac88c3eecae9",
     "grade": false,
     "grade_id": "cell-ac5afe4ff1ac28e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3b) Train a model using the best hyperparameter combination\n",
    "Your task is to train a model using the best hyperparameter combination found in Assignment 2c). You also need to upload and register the model to MLflow, the model needs to be associated with the MLflow Run where the optimal hyperparameter combination was found.\n",
    "\n",
    "For example, suppose running the \"lgbm-wine-2\" study of Assignment 2 created an MLflow Run 14 where the best hyperparameter combination was found, the model created in this assignment needs to be associated with MLflow Run 14, as shown below:\n",
    "\n",
    "<img src=\"./images/mlflow-best-run.png\" width=1200/>\n",
    "\n",
    "<img src=\"./images/mlflow-best-model.png\" width=1200/>\n",
    "\n",
    "Hints:\n",
    "- You may find the following function helpful: [mlflow.start_run](https://mlflow.org/docs/2.3.2/python_api/mlflow.html#mlflow.start_run) (Pay attention to the use of the `run_id` parameter).\n",
    "- It would probably be more convenient to retrieve the best hyperparameter combination using `optuna.load_study` rather than from the Mlflow Run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700a4a5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7281b465add4ff602f88c1888ee1016",
     "grade": false,
     "grade_id": "cell-5167cdef32da16d4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_optimized_model(study_name: str, storage: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Train a model with the best hyperparameters found by Optuna.\n",
    "    Args:\n",
    "        study_name: The name of the Optuna study where the best hyperparameters have been found. This is also the name of the MLflow experiment.\n",
    "        storage: The URI of the storage used to save the study history.\n",
    "        model_name: The name of the registered MLflow model.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4b942",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffaf93d09aa2681fb984217c4c598a6a",
     "grade": true,
     "grade_id": "cell-42706fe4f4834bc6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# From your MLflow UI you can make sure that the registered model is associated with the \n",
    "if not is_being_graded():\n",
    "    train_optimized_model(study_name=new_study_name, storage=storage, model_name=\"optuna-lgbm-wine\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4611d2-fa0e-4d9f-8a58-dea5260deb6f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4fb7c94825d95ac820b6c6de65c61dc",
     "grade": false,
     "grade_id": "cell-b170d027be4a30de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Screenshots for Assignment 3b)\n",
    "Submit the screenshots of the registered model version info and the corresponding MLflow run. You can take the images in the assignment instructions above as an example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1dfc29",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bab90346c90b6e12cabf680887036ae",
     "grade": false,
     "grade_id": "cell-1a0420c24806c592",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 4: Conditional search space with Optuna (2 points)\n",
    "Complete the objective function `objective_func_multimodel`. The target is to optimize the hyperparameters of a LightGBM and an [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor) regression model for the red wine quality prediction use case and see which model type is better in the use case. Similar to the previous assignments, the objective is also a smaller MAE. The hyperparameters to be tuned and their search ranges are given below. Use TPESampler with `RANDOM_SEED` as the random seed.\n",
    "\n",
    "**LightGBM**: \n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| n_estimators      | The number of decision trees.                                               | integer | 1000 (fixed value)                                                       |\n",
    "| learning_rate     | The step size of the gradient descent. It controls how quickly the model fits and then overfits the training data.              | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| subsample         | The percentage of training samples to be used to train each tree. `subsample*100%` of the training samples will be randomly selected for training.        | float   | [0.05, 1.0]                                                              |\n",
    "| subsample_freq    | Subsampling frequency. The subsampling will be performed again after `subsample_freq` trees have been trained.                                                     | integer | 1 (fixed value)                                                          |\n",
    "| colsample_bytree  | The percentage of features to use when training each tree.                | float   | [0.05, 1.0]                                                              |\n",
    "| min_child_samples | A leaf node should have at least `min_child_samples` data points to be further splitted. | integer | [1, 100]                                                                |\n",
    "| num_leaves        | Max number of nodes in a single tree.                                       | integer | [2, 2^10]                                                                |\n",
    "| random_state      | The seed for random number generation for reproducibility.                                   | integer | RANDOM_SEED (fixed value) \n",
    "\n",
    "**XGBoost**\n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| n_estimators      | Same as LightGBM.                                               | integer | 1000 (fixed value)                                                       |\n",
    "| learning_rate     | Same as LightGBM.             | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| subsample         | Same as LightGBM.            | float   | [0.05, 1.0]                                                              |                                         \n",
    "| colsample_bytree  | Same as LightGBM.                | float   | [0.05, 1.0]                                                              |\n",
    "| min_child_weight | Minimum sum of instance weight needed for a leaf mode to be further splitted (similar to min_child_samples in LightGBN). | integer | [1, 100]                                                                |\n",
    "| max_depth        | Max depth of a single tree                                       | integer | [1, 10]                                                                |\n",
    "| random_state      | Same as LightGBM.                                   | integer | RANDOM_SEED (fixed value)    \n",
    "\n",
    "**Notes**: \n",
    "- When specifying the search ranges for the model type, please use **\"model_type\"** as the parameter name that indicates the model type and `[\"lgbm\", \"xgb\"]` as the value candidates.\n",
    "- Some of the hyperparameters to be optimized used by the LightGBM and XGBoost models share the same names. In the assignment, these hyperparameters are `learning_rate, subsample, colsample_bytree`. You need to give these hyperparameters a unique name when defining their search ranges using `trial.suggest_*` so that Optuna can properly optimize the correct hyperparameters for each different model. Please rename the hyperparameters as `<hyperparameter-name>_lgbm` for the LightGBM model and `<hyperparameter-name>_xgb` for the XGBoost model, such as `learning_rate_lgbm` and `learning_rate_xgb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9bf03",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef66c88d48c448b4804abb08c9a06a91",
     "grade": false,
     "grade_id": "cell-c778c2cd2bd9ad2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def objective_func_multimodel(trial):\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65a160",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83bfc316012ed588749cd53891ffa95e",
     "grade": true,
     "grade_id": "cell-dd641c2f99f55316",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "multimodel_study_name = \"multimodel-wine\"\n",
    "multimodel_study = run_study(\n",
    "    study_name=multimodel_study_name,\n",
    "    storage=None,\n",
    "    objective_func=objective_func_multimodel,\n",
    "    n_trials=10,\n",
    ")\n",
    "\n",
    "print(\"Best MAE\", multimodel_study.best_value)\n",
    "print(\"Best params:\", multimodel_study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded721db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57e686afb3690d53a9044624a06b0640",
     "grade": false,
     "grade_id": "cell-8bb4c463904c3328",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Example output:\n",
    "```text\n",
    "Best MAE 0.42899067751612513\n",
    "Best params: {'model_type': 'lgbm', 'learning_rate_lgbm': 0.012172847081122434, 'subsample_lgbm': 0.18387801372602453, 'colsample_bytree_lgbm': 0.8120871317163377, 'min_child_samples': 8, 'num_leaves': 1011}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44eabfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec95bd9ad5e47f0f2e0483332eb63af1",
     "grade": false,
     "grade_id": "cell-02c2bb77613a18bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 5: Ensemble averaging (2 points)\n",
    "Complete the objective function `objective_func_ensemble`. The purpose is to Optuna to find the optimal weight combination to combine the predictions of an Sklearn's RandomForest, a XGBoost, and a LightGBM model to obtain better predictions for the red wine quality us case. Detailed instructions are given below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdb0a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0bd13b7b97dda633005e175675c076e",
     "grade": false,
     "grade_id": "cell-2c406b700d88e7d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before going to the assignment, first train three models for the red wine quality use case using Sklearn's RandomForest, XGBoost, and LightGBM and evaluate MAE of each model. For simplicity, the default configurations of the hyperparameters are used except for \"random_state\" which is set as `RANDOM_SEED` as in the previous assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca98e6b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "736c9b6f5fc140438fa6051d605dcfba",
     "grade": false,
     "grade_id": "cell-6b13961c39f1ce32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "xgb_model = xgb.XGBRegressor(random_state=RANDOM_SEED)\n",
    "lgbm_model = lgb.LGBMRegressor(random_state=RANDOM_SEED, verbose=-1)\n",
    "\n",
    "model_names = [\"rf_model\", \"xgb_model\", \"lgbm_model\"]\n",
    "\n",
    "for name in model_names:\n",
    "    model = eval(name)\n",
    "    model.fit(train_x, train_y)\n",
    "    predictions = model.predict(test_x)\n",
    "    print(f\"{name}: {mean_absolute_error(test_y, predictions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e656f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17180485444f4b234e890e48e4f1074f",
     "grade": false,
     "grade_id": "cell-af7b23190fa28641",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can see the best performing model is the random forest one, with an MAE of 0.4228.\n",
    "\n",
    "Now, it is time to combine the predictions of these models to improve the final prediction. In this assignment, your task is to complete the objective function `objective_func_ensemble`. The target is to find the best weight combination for the three models that have been trained in the cell above to obtain smaller MAE. Use 3 as the [step of discretization](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_int) when searching weight for each model. The search range of the weight should be [1, 100]. Use TPESampler (with `RANDOM_SEED` as the seed) in the study.\n",
    "\n",
    "**Note**: Please use \"rf_model\", \"xgb_model\", and \"lgbm_model\" as the names of the parameters that specify the weights of the random forest, XGBoost, and LightGBM models, respectively. E.g., \n",
    "```python\n",
    "study = run_study(study_name=\"ensemble-wine\", storage=None, objective_func=objective_func_ensemble, n_trials=10)\n",
    "print(study.best_trial.params)\n",
    "# Example_output:\n",
    "# {'rf_model': 46, 'xgb_model': 79, 'lgbm_model': 19}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ab33a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b52e1953f25572083c6e29a99a36e2b",
     "grade": false,
     "grade_id": "cell-c58cfc6b2eb0a37f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "all_predictions = {name: (eval(name)).predict(test_x) for name in model_names}\n",
    "\n",
    "# Define the objective function\n",
    "def objective_func_ensemble(trial):\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65360ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9aac4b653c3c2aee81a58e077d81e92f",
     "grade": false,
     "grade_id": "cell-49f6332e4ca5b339",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "study_ensemble = run_study(study_name=\"ensemble-wine\", storage=None, objective_func=objective_func_ensemble, n_trials=10)\n",
    "\n",
    "print(\"Best MAE\", study_ensemble.best_value)\n",
    "print(\"Best params:\", study_ensemble.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad76c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f5e44de11c35f8e0fd5fc75a9dbcb92",
     "grade": false,
     "grade_id": "cell-28abe759ddfbdf3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Example output:\n",
    "```text\n",
    "Best MAE 0.41523636202081865\n",
    "Best params: {'rf_model': 46, 'xgb_model': 79, 'lgbm_model': 19}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0ad36",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d308dd8b5232627a44e454d5c759ab9",
     "grade": true,
     "grade_id": "cell-f6df020e710b6302",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert (\n",
    "    study_ensemble.best_value < 0.4228\n",
    "), \"The MAE should be smaller than 0.4228 (the MAE of the best single model)\"\n",
    "\n",
    "assert set(study_ensemble.best_trial.params.keys()) == set([\"rf_model\", \"xgb_model\", \"lgbm_model\"]), \"Incorret parameter names\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70090913",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb8641167064af2d5fe631b4ca9f194d",
     "grade": false,
     "grade_id": "cell-2489dd0853cd1559",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 6: Scale up Optuna using Ray Tune\n",
    "In this assignment, you need to parallelize the Optuna hyperparameter optimization process using Ray Tune. Specifically, you will do this in three steps:\n",
    "1. Complete the `trainable` function to define a Trainable. You need to specify the model, the model training process, and the metric to be optimized. Similar to Assignment 1, the model is an LightGBM regressor, and the metric to be optimized is MAE. \n",
    "1. Complete the `create_search_algo` function to define search spaces and the search algorithm. Use [OptunaSearch](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.search.optuna.OptunaSearch.html) here. The search spaces should be the same as Assignment 1. Use the TPESampler as you did in Assignment 1.\n",
    "1. Complete the `create_ray_tuner` function to define a Tuner that launches hyperparameter optimization trials. This function receives a Boolean argument named `parallel`. If `parallel` is set to False, the Tuner should perform the hyperparameter optimization trials one by one. Otherwise, the Tuner should use all available CPUs on your machine to run the trials concurrently. You'll find the details of the arguments passed to the function in the function docs. \n",
    "\n",
    "**Notes**:\n",
    "- Remember to specify the random seed (`RANDOM_SEED`) for your model and sampler.\n",
    "- When define the Trainable, please use **\"mae\"** as the metric name when report the metric to the Tuner. \n",
    "- The actual number of the concurrent jobs doesn't matter (as long as it's larger than 1) as it depends on the number of CPUs available on your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8c5dd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "096aa3f4d3d268b85fd8bec072b03423",
     "grade": false,
     "grade_id": "cell-49483c1930dddf4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def trainable(config: Dict):\n",
    "    \"\"\"\n",
    "    Defines a model using the given configuration, trains it, and report the metric.\n",
    "    Args:\n",
    "        config: A dictionary containing the hyperparameters to be tuned\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n",
    "\n",
    "def create_search_algo() -> OptunaSearch:\n",
    "    \"\"\"\n",
    "    Create an Optuna search algorithm.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n",
    "\n",
    "def create_ray_tuner(\n",
    "    trainable: callable, algo: OptunaSearch, n_trials: int, parallel: bool = False\n",
    ") -> tune.Tuner:\n",
    "    \"\"\"\n",
    "    Create a Ray Tune Tuner\n",
    "    Args:\n",
    "        trainable: The Trainable that specifies the objective.\n",
    "        algo: The search algorithm.\n",
    "        n_trials: The number of trials.\n",
    "        parallel: Whether to run the trials in parallel.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f49de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80fe383040be89e63ca18982866d63db",
     "grade": true,
     "grade_id": "cell-97d1c69ad8af53c6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# When the Tuner is running trials sequentially, the best hyperparameters should be the same as the ones found by the Optuna study in the first assignment\n",
    "tuner = create_ray_tuner(trainable=trainable, algo=create_search_algo(), n_trials=10, parallel=False)\n",
    "results = tuner.fit()\n",
    "original_optuna_study = run_study(study_name=\"original\", storage=None, objective_func=objective_func, n_trials=10)\n",
    "assert results.get_best_result(metric=\"mae\", mode=\"min\").metrics.get(\"mae\") == original_optuna_study.best_value, \"Incorrect best hyperparameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32426049",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b199f3ae2274ea7df3e95247d4118bd6",
     "grade": true,
     "grade_id": "cell-7269c75a99605c04",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Mock a slow training process\n",
    "def slow_trainable(config):\n",
    "    time.sleep(10)\n",
    "    return ({\"mae\": random.random()})\n",
    "\n",
    "tuner_parallelized = create_ray_tuner(trainable=slow_trainable, algo=create_search_algo(), n_trials=5, parallel=True)\n",
    "start_time = time.time()\n",
    "results = tuner_parallelized.fit()\n",
    "end_time = time.time()\n",
    "\n",
    "assert end_time - start_time < 50, \"It should take less than 50s to complete the optimization in the parallel mode\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50105b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "068a598f580995f00c138679043df4f4",
     "grade": false,
     "grade_id": "cell-10b4b9d0c0640ad0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Wrap-up\n",
    "Please include the following files in your submission:\n",
    "- This Jupyter notebook (`week3_assignments.ipynb`)\n",
    "- The \"optuna.sqlite3\" database file\n",
    "- The PDF file containing your answers for Assignment 2b and screenshots for Assignments 2c and 3b\n",
    "\n",
    "**N.B.** Before making your submission, please check that your notebook and database files are named **exactly** as specified here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
