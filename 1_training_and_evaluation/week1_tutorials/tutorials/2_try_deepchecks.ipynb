{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepchecks tutorial\n",
    "[Deepchecks](https://docs.deepchecks.com/stable/getting-started/welcome.html) is a Python library for validating ML data and models. In the example below, you'll see how to use Deepchecks to evaluate a trained model before deploying the model to production (offline model evaluation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepchecks example\n",
    "Similar to the MLflow tutorial, we'll also train a ElasticNet model for red wine quality prediction in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from deepchecks.tabular.checks import TrainTestPerformance, DatasetsSizeComparison\n",
    "from deepchecks.tabular import Dataset, Suite\n",
    "from deepchecks import SuiteResult, CheckResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ElasticNet(alpha=0.5, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ElasticNet</label><div class=\"sk-toggleable__content\"><pre>ElasticNet(alpha=0.5, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ElasticNet(alpha=0.5, random_state=42)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a red wine model\n",
    "\n",
    "csv_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "    \n",
    "data = pd.read_csv(csv_url, sep=\";\")\n",
    "\n",
    "# Split the data into training and test sets. (0.6, 0.4) split.\n",
    "train, test = train_test_split(data, test_size=0.4, random_state=42)\n",
    "\n",
    "# The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "train_x = train.drop([\"quality\"], axis=1)\n",
    "test_x = test.drop([\"quality\"], axis=1)\n",
    "train_y = train[[\"quality\"]]\n",
    "test_y = test[[\"quality\"]]\n",
    "\n",
    "# Just use hard-coded parameters\n",
    "alpha = 0.5\n",
    "l1_ratio = 0.5\n",
    "\n",
    "lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "lr.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use Deepchecks to evaluate how the trained model performs on both training and testing dataset. Specifically, we'll evaluate the model's mean absolute error (MAE) and root mean square error (RMSE) on the training and testing dataset. \n",
    "\n",
    "As per the Deepchecks documentation, Deepchecks follows the convention that greater metric value represent better performance. Therefore, it is recommended to only use metrics that follow this convention, for example, negative MAE instead of MAE.\n",
    "\n",
    "In the next code cell, the type of variable `result` is [CheckResult](https://docs.deepchecks.com/stable/api/generated/deepchecks.core.CheckResult.html), which is class containing the result of a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f08fb7decf4fa3b77d9e15dda9dda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Train Test Performance</b></h4>'), HTML(value='<p>Summarize given model perf…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the training and testing dataset into a format that Deepchecks can handle\n",
    "train_dataset = Dataset(df=train_x, label=train_y, cat_features=[])\n",
    "test_dataset = Dataset(df=test_x, label=test_y, cat_features=[])\n",
    "\n",
    "# Define a test, the \"scorers\" parameter specifies the metrics (negative MAE and negative RMSE) to be used in the test\n",
    "check = TrainTestPerformance(scorers=[\"neg_mae\", \"neg_rmse\"])\n",
    "result: CheckResult = check.run(train_dataset, test_dataset, lr) \n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "<img src=\"../images/deepchecks-single-test.png\" width=800>\n",
    "\n",
    "Suppose we have a baseline model that produces MAE and RMSE of 0.9 on the same testing dataset, we may draw two conclusions from the evaluation result:\n",
    "1) This model seems to be able to make better prediction than the baseline model.\n",
    "2) The MAE and RMSE don't seem to drop significantly on testing dataset, which can be seen as a good signal: the model isn't overfitting the training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides a single test, Deepchecks provides a convenient way of grouping multiple tests into a collection, namely a Suite. In the example below, we specify a Suite containing two tests: 1) the familiar MAE and RMSE tests on both training and testing dataset, 2) comparing the size of testing dataset against the training dataset.\n",
    "\n",
    "The type of variable `result` is [SuiteResult](https://docs.deepchecks.com/stable/api/generated/deepchecks.core.SuiteResult.html), which is a class containing the results of a suite run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3eb3188c20a43618f2db583de256cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_31O8LI6JR416I3475C1AS5TF5\">Example test suite…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify a suite\n",
    "suite = Suite(\"Example test suite\", \n",
    "    TrainTestPerformance(scorers=[\"neg_mae\", \"neg_rmse\"]),\n",
    "    DatasetsSizeComparison()\n",
    "    # More tests can be added here\n",
    ")\n",
    "result: SuiteResult = suite.run(model=lr, train_dataset=train_dataset, test_dataset=test_dataset)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "<img src=\"../images/deepchecks-suite.png\" width=800>\n",
    "\n",
    "If you click the \"Other\" part in the output result above, you can first see the same result of MAE and RMSE you got when running the second-to-last code cell. Moreover, you can also see the results of dataset size comparison between the training and testing dataset: \n",
    "\n",
    "<img src=\"../images/deepchecks-suite-dataset-size.png\" width=500>\n",
    "\n",
    "The \"Didn't pass\" and \"Passed\" parts are empty because we haven't specify [conditions](https://docs.deepchecks.com/stable/general/usage/customizations/auto_examples/plot_configure_check_conditions.html) that determine whether a test succeeds or not. You'll explore more about the test conditions in this week's assignments. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
