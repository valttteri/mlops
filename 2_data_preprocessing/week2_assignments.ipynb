{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6802fe72",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dae986581c9a90b03c11e65cf5a54420",
     "grade": false,
     "grade_id": "cell-dc28f1b673110a45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Week 2 assignments \n",
    "**Please do the assignments using the `mlops_eng` environment.**\n",
    "\n",
    "In this week's assignments, you will practice data cleaning and validation, feature extraction, and feature engineering. You will use Pandas to write scripts to automate the data wrangling parts and [Great Expectations](https://greatexpectations.io/) for data validation.\n",
    "\n",
    "**Guidelines for submitting assignments**:\n",
    "- For each assignment, a code skeleton is provided. Please put your solutions in between the `### START CODE HERE` and `### END CODE HERE` code comments. Please **do not change any code other than those between the `### START CODE HERE` and `### END CODE HERE` comments**. Otherwise your notebook may not pass the tests used in grading.\n",
    "- Some assignments also require you to capture screenshots in order to earn points. Please put all your answers and the required screenshots into a single PDF file. For each screenshot, please clearly indicate which assignment it corresponds to in your PDF file. \n",
    "- When preparing your submission, be sure to include this assignment notebook. In the last assignment, you will need to modify a Python script named `etl.py`. Besides this assignment notebook, please also include `etl.py` in your submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226046f6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c97e904d7bef9214650bbd9a63f62016",
     "grade": false,
     "grade_id": "cell-c579576634cc0e95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Import all the packages needed in the assignments and set global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8dfe776",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a1d9a3bcdf834ebb35ad55e1c2e2ff8",
     "grade": false,
     "grade_id": "cell-a846ce59fe102c18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Optional\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import great_expectations as gx\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "from thefuzz import fuzz, process\n",
    "\n",
    "from great_expectations.data_context import EphemeralDataContext, FileDataContext\n",
    "from great_expectations.datasource.fluent import BatchRequest\n",
    "from great_expectations.exceptions import DataContextError\n",
    "from great_expectations.expectations.expectation import Expectation\n",
    "from great_expectations.checkpoint.types.checkpoint_result import CheckpointResult\n",
    "\n",
    "from test_helper import series_approximately_same\n",
    "\n",
    "# Random seed for making the assignments reproducible\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Path of current working directory\n",
    "WORKING_DIR = Path.cwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20af261d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca2ae492f87e0f8c46675d39a682c8e7",
     "grade": false,
     "grade_id": "cell-634f86b36f0d553a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This function is for the grading purpose only\n",
    "def is_being_graded():\n",
    "    \"\"\"\n",
    "    Returns True if the notebook is being executed by the auto-grading tool.\n",
    "    \"\"\"\n",
    "    env = os.environ.get(\"NBGRADER_EXECUTION\")\n",
    "    return env == \"autograde\" or env == \"validate\"\n",
    "\n",
    "\n",
    "# Suppress loggings and warnings when grading the notebook\n",
    "if is_being_graded():\n",
    "    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "    for logger in loggers:\n",
    "        logger.setLevel(logging.ERROR)\n",
    "    warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55b4eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ca6801f056b7c437eab8643c6ad3b3c",
     "grade": false,
     "grade_id": "cell-c0fdcc5ab656205e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 1: Data extraction (2 points)\n",
    "We will be working with a synthetic housing price dataset (based on [this](https://www.kaggle.com/datasets/htagholdings/property-sales) and [this](https://www.kaggle.com/datasets/harlfoxem/housesalesprediction) dataset), which lists house prices at the Canberra area in Australia from 2009 to 2016. The data description is provided in the file [Data_description.csv](./Data_description.csv). We will use this dataset to train a regression model to predict prices of houses given some info about the houses. \n",
    "\n",
    "### 1a) Load the data\n",
    "Let's first load the training dataset, which consists of two files. The file `deals.csv` holds the price information for the sold houses along with some minor information about the houses. The file `house_info.json` has more detailed information about the houses. Complete the function `file_reader` that loads both files into Pandas DataFrames. \n",
    "\n",
    "Hint: The `.json` file is saved in a \"list-like\" format, so we need to use parameter \"`orient='records'`\" when use Pandas to read the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30cf832a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4b7b773096f3934960bcf1d5849fd19",
     "grade": false,
     "grade_id": "cell-57e8048a8c13a8c9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def file_reader(path: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Read files of deals and house information into DataFrames\n",
    "    Args:\n",
    "        path (Path): Path to the folder where the files exist.\n",
    "    Returns:\n",
    "        a tuple consisting of a Pandas DataFrame of deals and a Pandas DataFrame of house information\n",
    "    \"\"\"\n",
    "    \n",
    "    price_file_path = path / \"deals.csv\"\n",
    "    house_file_path = path / \"house_info.json\" \n",
    "\n",
    "    ### START CODE HERE\n",
    "    deals = pd.read_csv(price_file_path)\n",
    "    houses = pd.read_json(house_file_path, orient=\"records\")\n",
    "\n",
    "    return deals, houses\n",
    "    ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d020c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c58bd33625c3099dba872d0fedb4ec51",
     "grade": false,
     "grade_id": "cell-b757e0c3faa0f505",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, run the cell below to call the function you just created. Let's take a quick look at the DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed167458",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a80338686b002d5ece1d22ec899b9a20",
     "grade": true,
     "grade_id": "cell-e4ddaf5de362b74d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_path = WORKING_DIR / \"data\" / \"reference\" / \"train\"\n",
    "prices, house_data = file_reader(train_path)\n",
    "\n",
    "assert prices.shape == (11046, 6), \"The shape of the price DataFrame is incorrect.\"\n",
    "assert house_data.shape == (\n",
    "    11046,\n",
    "    20,\n",
    "), \"The shape of the house_data DataFrame is incorrect.\"\n",
    "\n",
    "assert sorted(prices.columns) == sorted(\n",
    "    [\"datesold\", \"price\", \"building_year\", \"bedrooms\", \"postcode\", \"area\"]\n",
    "), \"The columns of the price DataFrame are incorrect.\"\n",
    "\n",
    "assert sorted(house_data.columns) == sorted(\n",
    "    [\n",
    "        \"date\",\n",
    "        \"postcode\",\n",
    "        \"area\",\n",
    "        \"bedrooms\",\n",
    "        \"bathrooms\",\n",
    "        \"yr_built\",\n",
    "        \"yr_renovated\",\n",
    "        \"condition\",\n",
    "        \"grade\",\n",
    "        \"floors\",\n",
    "        \"sqft_living\",\n",
    "        \"sqft_lot\",\n",
    "        \"sqft_above\",\n",
    "        \"sqft_basement\",\n",
    "        \"sqft_living15\",\n",
    "        \"sqft_lot15\",\n",
    "        \"waterfront\",\n",
    "        \"view\",\n",
    "        \"distance\",\n",
    "        \"prev_owner\",\n",
    "    ]\n",
    "), \"The columns of the house_data DataFrame are incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772e7e2d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ffe5592882829769c5a1dbbe17ac9bb",
     "grade": false,
     "grade_id": "cell-5825900baa0db344",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datesold</th>\n",
       "      <th>price</th>\n",
       "      <th>building_year</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>postcode</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-April-2009</td>\n",
       "      <td>437100</td>\n",
       "      <td>1977</td>\n",
       "      <td>4</td>\n",
       "      <td>2906</td>\n",
       "      <td>Conder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-April-2009</td>\n",
       "      <td>461300</td>\n",
       "      <td>1969</td>\n",
       "      <td>4</td>\n",
       "      <td>2615</td>\n",
       "      <td>Kippax Centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-April-2009</td>\n",
       "      <td>280900</td>\n",
       "      <td>1966</td>\n",
       "      <td>3</td>\n",
       "      <td>2615</td>\n",
       "      <td>Higgins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-April-2009</td>\n",
       "      <td>351600</td>\n",
       "      <td>1978</td>\n",
       "      <td>3</td>\n",
       "      <td>2620</td>\n",
       "      <td>Tinderry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-April-2009</td>\n",
       "      <td>431300</td>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>2914</td>\n",
       "      <td>Bonner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        datesold   price  building_year  bedrooms  postcode           area\n",
       "0  01-April-2009  437100           1977         4      2906         Conder\n",
       "1  01-April-2009  461300           1969         4      2615  Kippax Centre\n",
       "2  01-April-2009  280900           1966         3      2615        Higgins\n",
       "3  01-April-2009  351600           1978         3      2620       Tinderry\n",
       "4  01-April-2009  431300           2005         3      2914         Bonner"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>postcode</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>floors</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>distance</th>\n",
       "      <th>prev_owner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-10-24</td>\n",
       "      <td>2906</td>\n",
       "      <td>Gordon</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2875</td>\n",
       "      <td>13413</td>\n",
       "      <td>2875</td>\n",
       "      <td>0</td>\n",
       "      <td>2765.0</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.10</td>\n",
       "      <td>Gregory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-10-02</td>\n",
       "      <td>2605</td>\n",
       "      <td>Curtin</td>\n",
       "      <td>4</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Good</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2450</td>\n",
       "      <td>5346</td>\n",
       "      <td>1940</td>\n",
       "      <td>510</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>5535.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.62</td>\n",
       "      <td>Barrett</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-09-25</td>\n",
       "      <td>2615</td>\n",
       "      <td>Macgregor</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1735</td>\n",
       "      <td>8048</td>\n",
       "      <td>1235</td>\n",
       "      <td>500</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>7649.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.04</td>\n",
       "      <td>Bowman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-11-03</td>\n",
       "      <td>2906</td>\n",
       "      <td>Banks</td>\n",
       "      <td>4</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2555</td>\n",
       "      <td>12448</td>\n",
       "      <td>1585</td>\n",
       "      <td>970</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>10026.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.40</td>\n",
       "      <td>Wilson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-12-16</td>\n",
       "      <td>2914</td>\n",
       "      <td>Bonner</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1590</td>\n",
       "      <td>1750</td>\n",
       "      <td>1410</td>\n",
       "      <td>180</td>\n",
       "      <td>1588.0</td>\n",
       "      <td>2120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.01</td>\n",
       "      <td>Allen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  postcode       area  bedrooms  bathrooms  yr_built  \\\n",
       "0 2011-10-24      2906     Gordon         4       3.00      1997   \n",
       "1 2010-10-02      2605     Curtin         4       2.00      1927   \n",
       "2 2015-09-25      2615  Macgregor         3       1.75      1976   \n",
       "3 2011-11-03      2906      Banks         4       2.00      1977   \n",
       "4 2014-12-16      2914     Bonner         3       2.50      2004   \n",
       "\n",
       "   yr_renovated     condition  grade  floors  sqft_living  sqft_lot  \\\n",
       "0           NaN  Satisfactory      9     2.0         2875     13413   \n",
       "1           NaN          Good      8     2.0         2450      5346   \n",
       "2           NaN  Satisfactory      7     1.0         1735      8048   \n",
       "3           NaN  Satisfactory      8     1.0         2555     12448   \n",
       "4           NaN  Satisfactory      8     2.5         1590      1750   \n",
       "\n",
       "   sqft_above  sqft_basement  sqft_living15  sqft_lot15  waterfront  view  \\\n",
       "0        2875              0         2765.0     13500.0           0     0   \n",
       "1        1940            510         2200.0      5535.0           0     0   \n",
       "2        1235            500         1734.0      7649.0           0     0   \n",
       "3        1585            970         2341.0     10026.0           0     0   \n",
       "4        1410            180         1588.0      2120.0           0     0   \n",
       "\n",
       "   distance prev_owner  \n",
       "0     20.10    Gregory  \n",
       "1      7.62    Barrett  \n",
       "2     13.04     Bowman  \n",
       "3     21.40     Wilson  \n",
       "4     14.01      Allen  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(prices.head())\n",
    "display(house_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932e443",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a5b991ce0fdd8c989a57f0ca8e41f92",
     "grade": false,
     "grade_id": "cell-e4dc5254b36f296d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It seems that both DataFrames share a date column as well as four other columns (building year, number of bedrooms, postcode and area), so we can use these columns to merge the datasets into one. To be able to do so, we need to make sure the column names and types are same in both DataFrames.\n",
    "\n",
    "### 1b) Check column types\n",
    "Let's first check the data type of the shared columns. Complete the `show_col_dtype` function that returns the dtype of a given column of a given DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc30a19e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83395ccd4dc156e19979ba97ecbbf641",
     "grade": false,
     "grade_id": "cell-34249e760fc55514",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def show_col_dtype(df: pd.DataFrame, col_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Print the dtype of a column of a DataFrame\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame whose column needs to be checked\n",
    "        col_name(str): the name of the column to be checked\n",
    "    Returns:\n",
    "        Column type name as a string\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    type = df.dtypes[col_name]\n",
    "    \n",
    "    return type\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa10066",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58b7e871d3949ecc87736ac4ed5f9d34",
     "grade": true,
     "grade_id": "cell-0e37b4be25ac15e6",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert show_col_dtype(prices, \"datesold\") == \"object\", \"The dtype of the datesold column in the prices DataFrame is not correctly returned.\"\n",
    "assert show_col_dtype(house_data, \"date\") == \"datetime64[ns]\", \"The dtype of the date column in the house_data DataFrame is not correctly returned.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c97fa2f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbd6c7b28eb84569e19e5d865df9d825",
     "grade": false,
     "grade_id": "cell-0dccd54a35377533",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dtype of the datesold column is object\n",
      "The dtype of the date column is datetime64[ns]\n",
      "The dtype of the building_year column is int64\n",
      "The dtype of the yr_built column is int64\n",
      "The dtype of the bedrooms column is int64\n",
      "The dtype of the bedrooms column is int64\n",
      "The dtype of the postcode column is int64\n",
      "The dtype of the postcode column is int64\n",
      "The dtype of the area column is object\n",
      "The dtype of the area column is object\n"
     ]
    }
   ],
   "source": [
    "def print_dtype_helper(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"\n",
    "    Print the dtype of a column of a DataFrame\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame whose column needs to be checked\n",
    "        col_name(str): the name of the column to be checked\n",
    "    \"\"\"\n",
    "    print(f\"The dtype of the {col_name} column is {show_col_dtype(df, col_name)}\")\n",
    "\n",
    "# date column\n",
    "print_dtype_helper(prices, \"datesold\")\n",
    "print_dtype_helper(house_data, \"date\")\n",
    "\n",
    "# building year column\n",
    "print_dtype_helper(prices, \"building_year\")\n",
    "print_dtype_helper(house_data, \"yr_built\")\n",
    "\n",
    "# bedroom number column\n",
    "print_dtype_helper(prices, \"bedrooms\")\n",
    "print_dtype_helper(house_data, \"bedrooms\")\n",
    "\n",
    "# postcode column\n",
    "print_dtype_helper(prices, \"postcode\")\n",
    "print_dtype_helper(house_data, \"postcode\")\n",
    "\n",
    "# area column\n",
    "print_dtype_helper(prices, \"area\")\n",
    "print_dtype_helper(house_data, \"area\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f2081",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b3391264fe4d9f6ed2913e9961c1cfa",
     "grade": false,
     "grade_id": "cell-7f578a3a4d196323",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "From the output, you should see that only the date columns have different dtypes:\n",
    "\n",
    "The dtype of column datesold is: object \n",
    "\n",
    "The dtype of column date is: datetime64[ns] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330080d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d234d40da58ba3e2b2fe1c29b52aa0e",
     "grade": false,
     "grade_id": "cell-2ab4705dc1f71303",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1c) Merge price and house_data DataFrames\n",
    "Now, it seems that only the column `datesold` needs to be converted to datetime format. Additionally, one of the building year columns needs to be renamed to the same as the other one. Do the following in the `dataframe_merge` function:\n",
    "1. Convert the `datesold` column in the price DataFrame into `panda.datetime` format and store it in a new column named 'date' in the same DataFrame. (Do not delete the `datesold` column for now.)\n",
    "1. Rename the column `building_year` to `yr_built` in the price DataFrame.\n",
    "1. Merge the DataFrames using columns `date`, `postcode`, `bedrooms`, `area` and `yr_built` as keys.\n",
    "\n",
    "The following functions may be helpful:\n",
    "- [pandas.to_datetime](https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.to_datetime.html)\n",
    "- [pandas.DataFrame.rename](https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.DataFrame.rename.html)\n",
    "- [pandas.merge](https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.DataFrame.merge.html)\n",
    "\n",
    "NOTE: _In reality, we could not rely on these 5 columns to form a unique indicator for the merge to succeed. However, in the scope of this exercise we know a priori that this property holds for our data._  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc10b52d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b21dfbef66525cb1884b18f579542ecc",
     "grade": false,
     "grade_id": "cell-2adaa197674e195a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dataframe_merger(prices: pd.DataFrame, house_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge the two DataFrames given as inputs\n",
    "    Args:\n",
    "        prices (DataFrame): A pandas DataFrame holding the price information\n",
    "        house_data (DataFrame): A pandas DataFrame holding the detailed information about the sold houses\n",
    "    Returns:\n",
    "        The merged pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "    prices[\"date\"] = pd.to_datetime(prices[\"datesold\"])\n",
    "\n",
    "    prices = prices.rename(columns={\"building_year\": \"yr_built\"})\n",
    "\n",
    "    prices = prices.merge(house_data, on=[\"date\", \"postcode\", \"bedrooms\", \"area\", \"yr_built\"])\n",
    "\n",
    "    return prices\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d224819f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e7a11cfbc8a2262a58e369b1fd209a6",
     "grade": true,
     "grade_id": "cell-a1645813b2d85a95",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_df = dataframe_merger(prices, house_data)\n",
    "assert train_df.shape == (11046, 22), \"The shape of the merged DataFrame is incorrect.\"\n",
    "assert sorted(train_df.columns) == sorted(\n",
    "    [\n",
    "        \"datesold\",\n",
    "        \"price\",\n",
    "        \"yr_built\",\n",
    "        \"bedrooms\",\n",
    "        \"postcode\",\n",
    "        \"area\",\n",
    "        \"date\",\n",
    "        \"bathrooms\",\n",
    "        \"yr_renovated\",\n",
    "        \"condition\",\n",
    "        \"grade\",\n",
    "        \"floors\",\n",
    "        \"sqft_living\",\n",
    "        \"sqft_lot\",\n",
    "        \"sqft_above\",\n",
    "        \"sqft_basement\",\n",
    "        \"sqft_living15\",\n",
    "        \"sqft_lot15\",\n",
    "        \"waterfront\",\n",
    "        \"view\",\n",
    "        \"distance\",\n",
    "        \"prev_owner\",\n",
    "    ]\n",
    "), \"The columns of the merged DataFrame are incorrect.\"\n",
    "assert (\n",
    "    show_col_dtype(train_df, \"date\") == \"datetime64[ns]\"\n",
    "), \"The dtype of the date column in the merged DataFrame is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "567aa088",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddb08251466257720c95d6cb9a627905",
     "grade": false,
     "grade_id": "cell-648e40b0df3f6e08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datesold</th>\n",
       "      <th>price</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>postcode</th>\n",
       "      <th>area</th>\n",
       "      <th>date</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>condition</th>\n",
       "      <th>...</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>distance</th>\n",
       "      <th>prev_owner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-April-2009</td>\n",
       "      <td>437100</td>\n",
       "      <td>1977</td>\n",
       "      <td>4</td>\n",
       "      <td>2906</td>\n",
       "      <td>Conder</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>2.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>satisfactory</td>\n",
       "      <td>...</td>\n",
       "      <td>2280</td>\n",
       "      <td>8694</td>\n",
       "      <td>1380</td>\n",
       "      <td>900</td>\n",
       "      <td>1877.0</td>\n",
       "      <td>8052.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.14</td>\n",
       "      <td>Bailey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-April-2009</td>\n",
       "      <td>461300</td>\n",
       "      <td>1969</td>\n",
       "      <td>4</td>\n",
       "      <td>2615</td>\n",
       "      <td>Kippax Centre</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>2.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>...</td>\n",
       "      <td>2220</td>\n",
       "      <td>14833</td>\n",
       "      <td>1610</td>\n",
       "      <td>610</td>\n",
       "      <td>2202.0</td>\n",
       "      <td>12620.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.74</td>\n",
       "      <td>Buckley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-April-2009</td>\n",
       "      <td>280900</td>\n",
       "      <td>1966</td>\n",
       "      <td>3</td>\n",
       "      <td>2615</td>\n",
       "      <td>Higgins</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>1.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>...</td>\n",
       "      <td>1130</td>\n",
       "      <td>8867</td>\n",
       "      <td>1130</td>\n",
       "      <td>0</td>\n",
       "      <td>1466.0</td>\n",
       "      <td>8594.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.90</td>\n",
       "      <td>Morgan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-April-2009</td>\n",
       "      <td>351600</td>\n",
       "      <td>1978</td>\n",
       "      <td>3</td>\n",
       "      <td>2620</td>\n",
       "      <td>Tinderry</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>...</td>\n",
       "      <td>1800</td>\n",
       "      <td>11431</td>\n",
       "      <td>1420</td>\n",
       "      <td>380</td>\n",
       "      <td>2032.0</td>\n",
       "      <td>11044.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.48</td>\n",
       "      <td>Cumming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-April-2009</td>\n",
       "      <td>431300</td>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>2914</td>\n",
       "      <td>Bonner</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>...</td>\n",
       "      <td>1795</td>\n",
       "      <td>2248</td>\n",
       "      <td>1635</td>\n",
       "      <td>160</td>\n",
       "      <td>1784.0</td>\n",
       "      <td>2714.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.02</td>\n",
       "      <td>Roberts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        datesold   price  yr_built  bedrooms  postcode           area  \\\n",
       "0  01-April-2009  437100      1977         4      2906         Conder   \n",
       "1  01-April-2009  461300      1969         4      2615  Kippax Centre   \n",
       "2  01-April-2009  280900      1966         3      2615        Higgins   \n",
       "3  01-April-2009  351600      1978         3      2620       Tinderry   \n",
       "4  01-April-2009  431300      2005         3      2914         Bonner   \n",
       "\n",
       "        date  bathrooms  yr_renovated     condition  ...  sqft_living  \\\n",
       "0 2009-04-01       2.50           NaN  satisfactory  ...         2280   \n",
       "1 2009-04-01       2.25           NaN  Satisfactory  ...         2220   \n",
       "2 2009-04-01       1.25           NaN  Satisfactory  ...         1130   \n",
       "3 2009-04-01       2.00           NaN  Satisfactory  ...         1800   \n",
       "4 2009-04-01       2.00           NaN  Satisfactory  ...         1795   \n",
       "\n",
       "   sqft_lot  sqft_above  sqft_basement  sqft_living15  sqft_lot15  waterfront  \\\n",
       "0      8694        1380            900         1877.0      8052.0           0   \n",
       "1     14833        1610            610         2202.0     12620.0           0   \n",
       "2      8867        1130              0         1466.0      8594.0           0   \n",
       "3     11431        1420            380         2032.0     11044.0           0   \n",
       "4      2248        1635            160         1784.0      2714.0           0   \n",
       "\n",
       "   view  distance  prev_owner  \n",
       "0     0     20.14      Bailey  \n",
       "1     0     11.74     Buckley  \n",
       "2     0     10.90      Morgan  \n",
       "3     0      9.48     Cumming  \n",
       "4     0     13.02     Roberts  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a look at the merged DataFrame\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1805063",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "714c81ef3109021270bde16c25ea2510",
     "grade": false,
     "grade_id": "cell-7ab59c65689579c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 2: Data cleaning (2 points)\n",
    "### 2a) Missing values\n",
    "We have successfully extracted our data, we need to do some cleaning. By now we've noticed there are missing values in some columns. Let's start by checking which columns contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f39b96c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97052276fac03169d9607b585268c27c",
     "grade": false,
     "grade_id": "cell-9e2bca6fa5405e87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datesold             0\n",
      "price                0\n",
      "yr_built             0\n",
      "bedrooms             0\n",
      "postcode             0\n",
      "area                 0\n",
      "date                 0\n",
      "bathrooms            0\n",
      "yr_renovated     11045\n",
      "condition            0\n",
      "grade                0\n",
      "floors               0\n",
      "sqft_living          0\n",
      "sqft_lot             0\n",
      "sqft_above           0\n",
      "sqft_basement        0\n",
      "sqft_living15      135\n",
      "sqft_lot15        1079\n",
      "waterfront           0\n",
      "view                 0\n",
      "distance             0\n",
      "prev_owner           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f4e31",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88bef44f74764e3092879640f307b375",
     "grade": false,
     "grade_id": "cell-a778d8ca8288a1ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The column `yr_renovated` contains $\\frac{11045}{11046}$ missing values. With such a high fraction, it is not likely that any imputation scheme would result in a usable feature. So let's just drop that column as useless. While we're at it, we might also drop some other columns as well: \n",
    "- The name of the previous owner will not help in our prediction task, so the column `prev_owner` can be dropped as well. \n",
    "- The column `datesold` contains redundant duplicate information (the same info is included in the column `date`), so we won't need that either. \n",
    "- The values in the `sqft_living` column represent the summation of the values in the `sqft_above` and `sqft_basement` columns, so any one of the `sqft_above` and `sqft_basement` columns is individually redundant.\n",
    "\n",
    "The columns `sqft_living15` and `sqft_lot15` have roughly only 1.5% and 10% of values missing respectively so we'll be able to use them. We will deal with the missing values in these columns later in the feature engineering part.\n",
    "\n",
    "Remove the four columns `yr_renovated`, `prev_owner`, `datesold`, and `sqft_above` from our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "671d684f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b2f6fb5c5a838941c080893199e6796",
     "grade": false,
     "grade_id": "cell-c994d69c9fa20827",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def drop_futile_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes unneeded columns from the argument DataFrame\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding all of the housing data\n",
    "    Returns:\n",
    "        The pandas DataFrame with the columns listed in the instruction removed\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "    cols = [\"yr_renovated\", \"prev_owner\", \"datesold\", \"sqft_above\"]\n",
    "    df = df.drop(columns=cols)\n",
    "\n",
    "    return df\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03176b26",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9310ee7d00aaf42c46a74af8945c6d6d",
     "grade": false,
     "grade_id": "cell-df5ce081d306053b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to see if our function works and the columns are indeed dropped. At this point, we should have 18 columns remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1090449",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5625fdc858fa60ecd50b19ae9bd6de2",
     "grade": true,
     "grade_id": "cell-745815085745c91b",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_with_columns_dropped = drop_futile_columns(train_df)\n",
    "\n",
    "assert df_with_columns_dropped.shape == (\n",
    "    11046,\n",
    "    18,\n",
    "), \"The shape of the DataFrame is incorrect after dropping columns.\"\n",
    "assert sorted(df_with_columns_dropped.columns) == sorted(\n",
    "    [\n",
    "        \"price\",\n",
    "        \"yr_built\",\n",
    "        \"bedrooms\",\n",
    "        \"postcode\",\n",
    "        \"area\",\n",
    "        \"date\",\n",
    "        \"bathrooms\",\n",
    "        \"condition\",\n",
    "        \"grade\",\n",
    "        \"floors\",\n",
    "        \"sqft_living\",\n",
    "        \"sqft_lot\",\n",
    "        \"sqft_basement\",\n",
    "        \"sqft_living15\",\n",
    "        \"sqft_lot15\",\n",
    "        \"waterfront\",\n",
    "        \"view\",\n",
    "        \"distance\",\n",
    "    ]\n",
    "), \"The columns of the DataFrame are incorrect after dropping columns.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa28c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5649788a9e94ef660517b96a276a31a8",
     "grade": false,
     "grade_id": "cell-6a994a5b5aad7627",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2b) Inconsistent entries\n",
    "\n",
    "It's very common for the extracted data to contain inconsistencies. Let's take a closer look at our columns to see if we can spot some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86c10d84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "529f00da30cfd20758dfbfbeead4ed69",
     "grade": false,
     "grade_id": "cell-03d29276284c460e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>postcode</th>\n",
       "      <th>date</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>grade</th>\n",
       "      <th>floors</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.104600e+04</td>\n",
       "      <td>11046.000000</td>\n",
       "      <td>11046.000000</td>\n",
       "      <td>11046.000000</td>\n",
       "      <td>11046</td>\n",
       "      <td>11046.000000</td>\n",
       "      <td>11046.000000</td>\n",
       "      <td>11046.000000</td>\n",
       "      <td>11046.000000</td>\n",
       "      <td>11046.000000</td>\n",
       "      <td>11046.000000</td>\n",
       "      <td>10911.000000</td>\n",
       "      <td>9967.000000</td>\n",
       "      <td>11046.000000</td>\n",
       "      <td>11046.000000</td>\n",
       "      <td>11046.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.925457e+05</td>\n",
       "      <td>1972.844197</td>\n",
       "      <td>3.526073</td>\n",
       "      <td>2747.149285</td>\n",
       "      <td>2012-12-03 18:05:48.071700224</td>\n",
       "      <td>2.213064</td>\n",
       "      <td>8.171103</td>\n",
       "      <td>1.553639</td>\n",
       "      <td>2348.855694</td>\n",
       "      <td>12675.191744</td>\n",
       "      <td>270.469853</td>\n",
       "      <td>2299.625516</td>\n",
       "      <td>11184.507776</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.014938</td>\n",
       "      <td>147.499479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.090000e+04</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2600.000000</td>\n",
       "      <td>2007-07-02 00:00:00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1404.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.458000e+05</td>\n",
       "      <td>1961.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2611.000000</td>\n",
       "      <td>2011-02-18 00:00:00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1795.000000</td>\n",
       "      <td>5837.250000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1828.000000</td>\n",
       "      <td>5656.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.366000e+05</td>\n",
       "      <td>1977.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2617.000000</td>\n",
       "      <td>2013-05-03 12:00:00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2220.000000</td>\n",
       "      <td>9357.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>2176.000000</td>\n",
       "      <td>8905.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.613000e+05</td>\n",
       "      <td>1992.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2905.000000</td>\n",
       "      <td>2014-11-18 00:00:00</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2780.000000</td>\n",
       "      <td>14005.750000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>2683.000000</td>\n",
       "      <td>12701.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.989300e+06</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2914.000000</td>\n",
       "      <td>2015-12-24 00:00:00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8640.000000</td>\n",
       "      <td>482350.000000</td>\n",
       "      <td>2280.000000</td>\n",
       "      <td>4612.000000</td>\n",
       "      <td>182404.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>22210.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.554195e+05</td>\n",
       "      <td>27.100996</td>\n",
       "      <td>0.695975</td>\n",
       "      <td>148.665660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.489251</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.450539</td>\n",
       "      <td>706.753347</td>\n",
       "      <td>15545.277547</td>\n",
       "      <td>306.938218</td>\n",
       "      <td>593.615570</td>\n",
       "      <td>10507.371020</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>1340.088375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price      yr_built      bedrooms      postcode  \\\n",
       "count  1.104600e+04  11046.000000  11046.000000  11046.000000   \n",
       "mean   5.925457e+05   1972.844197      3.526073   2747.149285   \n",
       "min    5.090000e+04   1900.000000      0.000000   2600.000000   \n",
       "25%    4.458000e+05   1961.000000      3.000000   2611.000000   \n",
       "50%    5.366000e+05   1977.000000      3.000000   2617.000000   \n",
       "75%    6.613000e+05   1992.000000      4.000000   2905.000000   \n",
       "max    7.989300e+06   2015.000000      5.000000   2914.000000   \n",
       "std    2.554195e+05     27.100996      0.695975    148.665660   \n",
       "\n",
       "                                date     bathrooms         grade  \\\n",
       "count                          11046  11046.000000  11046.000000   \n",
       "mean   2012-12-03 18:05:48.071700224      2.213064      8.171103   \n",
       "min              2007-07-02 00:00:00      1.000000      6.000000   \n",
       "25%              2011-02-18 00:00:00      2.000000      7.000000   \n",
       "50%              2013-05-03 12:00:00      2.000000      8.000000   \n",
       "75%              2014-11-18 00:00:00      2.500000      9.000000   \n",
       "max              2015-12-24 00:00:00      5.000000     12.000000   \n",
       "std                              NaN      0.489251      1.066406   \n",
       "\n",
       "             floors   sqft_living       sqft_lot  sqft_basement  \\\n",
       "count  11046.000000  11046.000000   11046.000000   11046.000000   \n",
       "mean       1.553639   2348.855694   12675.191744     270.469853   \n",
       "min        1.000000    720.000000    1167.000000       0.000000   \n",
       "25%        1.000000   1795.000000    5837.250000      10.000000   \n",
       "50%        1.500000   2220.000000    9357.000000     130.000000   \n",
       "75%        2.000000   2780.000000   14005.750000     470.000000   \n",
       "max        3.000000   8640.000000  482350.000000    2280.000000   \n",
       "std        0.450539    706.753347   15545.277547     306.938218   \n",
       "\n",
       "       sqft_living15     sqft_lot15    waterfront          view      distance  \n",
       "count   10911.000000    9967.000000  11046.000000  11046.000000  11046.000000  \n",
       "mean     2299.625516   11184.507776      0.000272      0.014938    147.499479  \n",
       "min      1208.000000    1404.000000      0.000000      0.000000      0.000000  \n",
       "25%      1828.000000    5656.500000      0.000000      0.000000      8.080000  \n",
       "50%      2176.000000    8905.000000      0.000000      0.000000     11.225000  \n",
       "75%      2683.000000   12701.000000      0.000000      0.000000     13.990000  \n",
       "max      4612.000000  182404.000000      1.000000      4.000000  22210.000000  \n",
       "std       593.615570   10507.371020      0.016479      0.211600   1340.088375  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_with_columns_dropped.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed572116",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3810f742512befc17304acf1139673da",
     "grade": false,
     "grade_id": "cell-86076a285a6c6c30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This gives us an overview of all numerical data in the form of some statistics. In general, everything else seems to be in order, but the column `distance` seems to have some extremely high values. The data description says that this column tells the distance from the city center in km, so the max value of 22210.0 is clearly wrong. Let's take a further look at this column. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed0b439e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7606de142c83eca9e05a52075988fb63",
     "grade": false,
     "grade_id": "cell-ff7673666edda231",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_low (10912 rows):count    10912.000000\n",
      "mean        11.088641\n",
      "std          4.907516\n",
      "min          0.000000\n",
      "25%          7.967500\n",
      "50%         11.160000\n",
      "75%         13.880000\n",
      "max         45.680000\n",
      "Name: distance, dtype: float64\n",
      "\n",
      "df_high (134 rows):count      134.000000\n",
      "mean     11255.820896\n",
      "std       4825.372992\n",
      "min        860.000000\n",
      "25%       8377.500000\n",
      "50%      10970.000000\n",
      "75%      13757.500000\n",
      "max      22210.000000\n",
      "Name: distance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_low = df_with_columns_dropped[df_with_columns_dropped[\"distance\"] < 100]\n",
    "df_high = df_with_columns_dropped[df_with_columns_dropped[\"distance\"] >= 100]\n",
    "print(f\"df_low ({len(df_low.index)} rows):{df_low['distance'].describe()}\")\n",
    "print(f\"\\ndf_high ({len(df_high.index)} rows):{df_high['distance'].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebecb14",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fe1d347c96b1a5df762403ae70f5057",
     "grade": false,
     "grade_id": "cell-2ddbf9ed81e2483c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It seems there are 134 rows where the distance has been stated in metres instead of kilometers. Complete the `correct_distance_unit` function that changes those values into kilometers. \n",
    "\n",
    "**Note**: In this and some other upcoming assignments, you'll need to create functions that manipulate a DataFrame, such as changing the type, values of a column or adding some new columns. As you'll see, a DataFrame will be passed as an argument to the functions. To help you debug your code more easily, please apply your modifications to the copy of the DataFrame rather than directly changing the DataFrame given as an argument. Below is an example:\n",
    "```python\n",
    "def manipulate_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # First make a copy of the original df\n",
    "    df_copy = df.copy()\n",
    "    # Then change the copy\n",
    "    df_copy[\"column\"] = ...\n",
    "    # Finally return the copy\n",
    "    return df_copy\n",
    "\n",
    "# The function can be used like\n",
    "new_df = manipulate_df(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3a57b93",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40eeafd50d89aeed72a374cfc33ed483",
     "grade": false,
     "grade_id": "cell-d1033fd93ffca044",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def correct_distance_unit(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Correct the falsely input values in column 'distance'\n",
    "    Args:\n",
    "        df (DataFrame): A Pandas DataFrame holding all of the housing data\n",
    "    Returns:\n",
    "        The Pandas DataFrame with the values in column 'distance' corrected\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    ### START CODE HERE\n",
    "    df_copy[\"distance\"] = df_copy[\"distance\"].apply(lambda x: x/1000 if x >= 100 else x)\n",
    "\n",
    "    return df_copy\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd89b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "847414f06cb3cff6efd7d280cee4f7b1",
     "grade": false,
     "grade_id": "cell-ee0849bab625dfb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's run the cell below to see if our function works. The maximum distance should now be 45.68 km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95b6dff5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab2dae7cd1056c21e9bc2928de35cb79",
     "grade": true,
     "grade_id": "cell-3bc1cb8180de7bbb",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_with_corrected_distance_unit = correct_distance_unit(df_with_columns_dropped)\n",
    "\n",
    "distance_column_description = df_with_corrected_distance_unit[\"distance\"].describe()[[\"count\", \"mean\", \"std\"]]\n",
    "expected_distance_column_description = pd.Series(\n",
    "    {\n",
    "        \"count\": 11046,\n",
    "        \"mean\": 11.090669,\n",
    "        \"std\": 4.906347,\n",
    "    }\n",
    ")\n",
    "\n",
    "assert series_approximately_same(\n",
    "    distance_column_description, expected_distance_column_description, 1e-6\n",
    "), \"The distance column is not correctly corrected.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1365a5c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcc7ddb8f87cff6a8abe38ac96043c64",
     "grade": false,
     "grade_id": "cell-007e5270437ec117",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2c) Fixing typos\n",
    "\n",
    "Now we should have our numerical columns in order. Let's next take a look at the string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbfc9589",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3307d649ee51e85587ccbd909ccedae7",
     "grade": false,
     "grade_id": "cell-2b0ba5fa68ca2ba0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'condition' has 16 unique values:\n",
      "['satisfactory' 'Satisfactory' 'Good' 'Excellent' 'good' 'Satisfactoryy'\n",
      " 'Goood' 'Satisfactry' 'satisfatory' 'Satisfattory' 'God' 'Good '\n",
      " 'excellent' 'Satisfatory' 'god' 'satisfattory']\n",
      "\n",
      "Column 'area' has 131 unique values:\n",
      "['Conder' 'Kippax Centre' 'Higgins' 'Tinderry' 'Bonner' 'Fraser' 'Kambah'\n",
      " 'Duffy' 'Page' 'Kaleen' 'Kingston' 'Gowrie' 'Administrative' 'Gordon'\n",
      " 'Kippax' 'Ginninderra Village' 'Bonython' 'Curtin' 'Uriarra' 'Forde'\n",
      " 'Monash' 'Waramanga' 'Scullin' 'Pearce' 'Fairbairn Raaf' 'Amaroo'\n",
      " 'Nicholls' 'Macarthur' 'Dickson' 'Theodore' 'Lyneham' \"O'connor\"\n",
      " 'Hackett' 'Downer' 'Ainslie' 'Acton' 'Stirling' 'Belconnen Dc' 'Watson'\n",
      " 'Belconnen' 'Melba' 'Causeway' 'Isabella Plains' 'Holder' 'Russell'\n",
      " 'Spence' 'Red Hill' 'Holt' 'Wanniassa' 'Hughes' 'Lawson' 'Cook' 'Torrens'\n",
      " 'Chapman' 'Harman' 'Dunlop' 'Farrer' 'Jamison Centre' 'Hawker'\n",
      " 'Charnwood' 'Weston Creek' 'Mawson' 'Turner' 'Latham' 'Oxley' 'Ngunnawal'\n",
      " 'Palmerston' 'Florey' 'Pierces Creek' 'Isaacs' 'Yarralumla' 'Calwell'\n",
      " 'Gilmore' 'Harrison' 'Giralang' 'Bruce' 'Franklin' 'Macquarie'\n",
      " 'Narrabundah' 'Banks' 'Erindale Centre' 'Richardson' 'Hmas Harman'\n",
      " 'Fadden' 'Chisholm' 'Evatt' 'Garran' 'Crace' 'Flynn' 'Brindabella'\n",
      " 'Mckellar' 'Capital Hill' 'Griffith' 'Canberra' 'Woden' 'Phillip'\n",
      " 'Tuggeranong' 'Macgregor' 'Reid' 'Manuka' 'Duntroon' 'Forrest' 'Aranda'\n",
      " 'Mitchell' 'Fisher' 'Swinger Hill' 'Braddon' 'Chifley' 'Russell Hill'\n",
      " 'Weetangera' 'Mount Stromlo' 'Uriarra Forest' 'Deakin' 'Weston'\n",
      " \"O'malley\" 'Rivett' 'Hall' 'Campbell' 'Parliament House' 'Lyons'\n",
      " 'Greenway' 'Clear Range' 'Black Mountain' 'Parkes' 'Fyshwick' 'Barton'\n",
      " 'Yarrow' 'Pialligo' 'Wallaroo' 'City' 'Gundaroo']\n"
     ]
    }
   ],
   "source": [
    "condition_unique = train_df[\"condition\"].unique()\n",
    "print(\n",
    "    f\"Column 'condition' has {len(condition_unique)} unique values:\\n{condition_unique}\"\n",
    ")\n",
    "area_unique = train_df[\"area\"].unique()\n",
    "print(f\"\\nColumn 'area' has {len(area_unique)} unique values:\\n{area_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20938f6d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40cfd3ab78c18b096aca1afa2034de96",
     "grade": false,
     "grade_id": "cell-a6bf060ede9b0b3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The column `area` seems to be ok, but the column `condition` contains several typos. \n",
    "\n",
    "Start by lowercasing the conditions and removing any unnecessary trailing white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3219193",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49c36629840ed39bb5038c0c9a029cf3",
     "grade": false,
     "grade_id": "cell-eac5717176f44de7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def string_transformer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lowercases all values in the column 'condition' and removes trailing white space.\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding all of the housing data\n",
    "    Returns:\n",
    "        The pandas DataFrame with the 'condition' column values being lower-cased and stripped of trailing white space\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    ### START CODE HERE\n",
    "    df_copy[\"condition\"] = df_copy[\"condition\"].apply(lambda x: x.strip().lower())\n",
    "    \n",
    "    return df_copy\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c9856d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a1ad3d13dccd062fdf5d201c8ee7325",
     "grade": false,
     "grade_id": "cell-94de6c9678ad5be8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's apply this function to our training data to see what changed. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4dc1313a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12d18ceed9d1a517604ccd8b921fb4b2",
     "grade": true,
     "grade_id": "cell-3207fc853001b994",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_with_condition_column_transformed = string_transformer(\n",
    "    df_with_corrected_distance_unit\n",
    ")\n",
    "assert sorted(\n",
    "    df_with_condition_column_transformed[\"condition\"].unique().tolist()\n",
    ") == sorted(\n",
    "    [\n",
    "        \"satisfactory\",\n",
    "        \"good\",\n",
    "        \"excellent\",\n",
    "        \"satisfactoryy\",\n",
    "        \"goood\",\n",
    "        \"satisfactry\",\n",
    "        \"satisfatory\",\n",
    "        \"satisfattory\",\n",
    "        \"god\",\n",
    "    ]\n",
    "), \"The values in the condition column are incorrect after the transformation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13fd78d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3161b8fc104653e5c332902a4d8f91d",
     "grade": false,
     "grade_id": "cell-8bd648c7d43d28d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We see that the data contains three conditions, namely: 'excellent', 'good' and 'satisfactory'. Other values are clearly typos. Let's use [TheFuzz](https://github.com/seatgeek/thefuzz) library (already imported above) to automate the process of correcting typos. \n",
    "\n",
    "Implement the following to complete the `typo_fixer` function:\n",
    "1. Loop over the condition column and find the best approximate match from a list of correct conditions for each entry in the condition column. You can use [process.extractOne](https://github.com/seatgeek/thefuzz#process) to find the single best match for an entry and use `fuzz.ratio` as the scorer inside the `extractOne` function.\n",
    "2. Update the column `condition` by replacing each entry with its best approximate match if the similarity score is greater than the specified threshold. Otherwise, keep the entry unchanged. In this way, we fix only those strings that feel confident enough to be typos of the correct conditions. Using a threshold is a good practice so that we are not too aggressive in correcting the typos. \n",
    "3. Create a new column `similarity_scores`, where you store the similarity score of each (original entry)-(approximate match) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0da1ebb1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eba01de9a107b6a3ce46f200b9b2a7f6",
     "grade": false,
     "grade_id": "cell-caf4965dd55bcaf7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def typo_fixer(\n",
    "    df: pd.DataFrame, threshold: float, correct_condition_values: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Uses fuzzy string matching to fix typos in the column 'condition'. It loops through each entry in the column and\n",
    "    replaces them with suggested corrections if the similarity score is high enough.\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding all of the housing data\n",
    "        threshold (int): A number between 0-100. Only the entries with score above this number are replaced.\n",
    "        correct_condition_values (List): A list of correct strings that we hope the condition column to include. For example, correct_ones=['excellent', 'good', 'satisfactory'] in the case of the training dataset.\n",
    "    Returns:\n",
    "        The pandas DataFrame with the 'condition' column values corrected\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    ### START CODE HERE\n",
    "    similarity_scores = []\n",
    "    new_conditions = []\n",
    "\n",
    "    for cnd in df_copy[\"condition\"]:\n",
    "        best_match, score = process.extractOne(cnd, correct_condition_values, scorer=fuzz.ratio)\n",
    "\n",
    "        if score > threshold:\n",
    "            new_conditions.append(best_match)\n",
    "        else:\n",
    "            new_conditions.append(cnd)\n",
    "\n",
    "        similarity_scores.append(score)\n",
    "\n",
    "    df_copy[\"condition\"] = new_conditions\n",
    "    df_copy[\"similarity_scores\"] = similarity_scores\n",
    "\n",
    "    return df_copy    \n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6528114",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db48e3f9c99263dd12283dc705f25d08",
     "grade": true,
     "grade_id": "cell-abbe844980a5e731",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "threshold = 80\n",
    "df_typo_fixed = typo_fixer(\n",
    "    df_with_condition_column_transformed,\n",
    "    threshold,\n",
    "    [\"excellent\", \"good\", \"satisfactory\"],\n",
    ")\n",
    "\n",
    "assert sorted(df_typo_fixed[\"condition\"].unique()) == sorted(\n",
    "    [\"satisfactory\", \"good\", \"excellent\"]\n",
    "), \"The correct condition values are not correct after the typo fixing.\"\n",
    "\n",
    "assert (\n",
    "    df_typo_fixed[\"similarity_scores\"].min() == 86\n",
    "), \"The lowest similarity score is incorrect after the typo fixing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e5cdd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "327bd248b91137eafee976dfbf9c793d",
     "grade": false,
     "grade_id": "cell-a61124eb73ae0f37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have our data extraction and cleaning operations in place, let's conclude the first two assignments by wrapping everything up to this point inside a single function `data_extraction`. It loads the price and house info datasets, merges them into a single DataFrame and polishes the DataFrame.\n",
    "\n",
    "Run the cell below to define the data extraction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8eda8aae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d73ed6a77dc5457b568bb022c9fb94d7",
     "grade": false,
     "grade_id": "cell-3316f85a7e407f3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def data_extraction(path: Path, correct_condition_values: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The entire data extraction/cleaning pipeline wrapped inside a single function.\n",
    "    Args:\n",
    "        path (Path): Path to the folder where the files exist.\n",
    "        correct_condition_values (List): A list of correct strings that we hope the condition column to include.\n",
    "    Returns:\n",
    "        df (DataFrame): A pandas DataFrame holding all of the (cleaned) housing data.\n",
    "    \"\"\"\n",
    "    threshold = 80\n",
    "    prices, house_data = file_reader(path)\n",
    "    df_merged = dataframe_merger(prices, house_data)\n",
    "    df_with_columns_dropped = drop_futile_columns(df_merged)\n",
    "    df_with_corrected_distance_unit = correct_distance_unit(df_with_columns_dropped)\n",
    "    df_with_condition_column_transformed = string_transformer(df_with_corrected_distance_unit)\n",
    "    df_typo_fixed = typo_fixer(df_with_condition_column_transformed, threshold, correct_condition_values)\n",
    "    return df_typo_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab736373",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0de692ffd7659f58a35f6e5654a953d4",
     "grade": false,
     "grade_id": "cell-948c174918a3934a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>postcode</th>\n",
       "      <th>area</th>\n",
       "      <th>date</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>floors</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>distance</th>\n",
       "      <th>similarity_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>437100</td>\n",
       "      <td>1977</td>\n",
       "      <td>4</td>\n",
       "      <td>2906</td>\n",
       "      <td>Conder</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>2.50</td>\n",
       "      <td>satisfactory</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2280</td>\n",
       "      <td>8694</td>\n",
       "      <td>900</td>\n",
       "      <td>1877.0</td>\n",
       "      <td>8052.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.14</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>461300</td>\n",
       "      <td>1969</td>\n",
       "      <td>4</td>\n",
       "      <td>2615</td>\n",
       "      <td>Kippax Centre</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>2.25</td>\n",
       "      <td>satisfactory</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2220</td>\n",
       "      <td>14833</td>\n",
       "      <td>610</td>\n",
       "      <td>2202.0</td>\n",
       "      <td>12620.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.74</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>280900</td>\n",
       "      <td>1966</td>\n",
       "      <td>3</td>\n",
       "      <td>2615</td>\n",
       "      <td>Higgins</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>1.25</td>\n",
       "      <td>satisfactory</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1130</td>\n",
       "      <td>8867</td>\n",
       "      <td>0</td>\n",
       "      <td>1466.0</td>\n",
       "      <td>8594.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.90</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>351600</td>\n",
       "      <td>1978</td>\n",
       "      <td>3</td>\n",
       "      <td>2620</td>\n",
       "      <td>Tinderry</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>satisfactory</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1800</td>\n",
       "      <td>11431</td>\n",
       "      <td>380</td>\n",
       "      <td>2032.0</td>\n",
       "      <td>11044.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.48</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>431300</td>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>2914</td>\n",
       "      <td>Bonner</td>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>satisfactory</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1795</td>\n",
       "      <td>2248</td>\n",
       "      <td>160</td>\n",
       "      <td>1784.0</td>\n",
       "      <td>2714.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.02</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  yr_built  bedrooms  postcode           area       date  bathrooms  \\\n",
       "0  437100      1977         4      2906         Conder 2009-04-01       2.50   \n",
       "1  461300      1969         4      2615  Kippax Centre 2009-04-01       2.25   \n",
       "2  280900      1966         3      2615        Higgins 2009-04-01       1.25   \n",
       "3  351600      1978         3      2620       Tinderry 2009-04-01       2.00   \n",
       "4  431300      2005         3      2914         Bonner 2009-04-01       2.00   \n",
       "\n",
       "      condition  grade  floors  sqft_living  sqft_lot  sqft_basement  \\\n",
       "0  satisfactory      7     1.0         2280      8694            900   \n",
       "1  satisfactory      8     1.0         2220     14833            610   \n",
       "2  satisfactory      7     1.0         1130      8867              0   \n",
       "3  satisfactory      8     1.0         1800     11431            380   \n",
       "4  satisfactory      8     2.0         1795      2248            160   \n",
       "\n",
       "   sqft_living15  sqft_lot15  waterfront  view  distance  similarity_scores  \n",
       "0         1877.0      8052.0           0     0     20.14                100  \n",
       "1         2202.0     12620.0           0     0     11.74                100  \n",
       "2         1466.0      8594.0           0     0     10.90                100  \n",
       "3         2032.0     11044.0           0     0      9.48                100  \n",
       "4         1784.0      2714.0           0     0     13.02                100  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a training DataFrame that goes through the data extraction and cleaning processes. This DataFrame will be used in the following assignments\n",
    "train_path = WORKING_DIR / \"data\" / \"reference\" / \"train\"\n",
    "df_after_data_extraction = data_extraction(\n",
    "    train_path, [\"excellent\", \"good\", \"satisfactory\"]\n",
    ")\n",
    "\n",
    "assert df_after_data_extraction.shape == (\n",
    "    11046,\n",
    "    19,\n",
    "), \"The shape of the DataFrame is incorrect after the data extraction.\"\n",
    "assert sorted(df_after_data_extraction.columns) == sorted(\n",
    "    [\n",
    "        \"price\",\n",
    "        \"yr_built\",\n",
    "        \"bedrooms\",\n",
    "        \"postcode\",\n",
    "        \"area\",\n",
    "        \"date\",\n",
    "        \"bathrooms\",\n",
    "        \"condition\",\n",
    "        \"grade\",\n",
    "        \"floors\",\n",
    "        \"sqft_living\",\n",
    "        \"sqft_lot\",\n",
    "        \"sqft_basement\",\n",
    "        \"sqft_living15\",\n",
    "        \"sqft_lot15\",\n",
    "        \"waterfront\",\n",
    "        \"view\",\n",
    "        \"distance\",\n",
    "        \"similarity_scores\",\n",
    "    ]\n",
    "), \"The columns of the DataFrame are incorrect after the data extraction.\"\n",
    "\n",
    "df_after_data_extraction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9f947",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b52f128ae99d7d002620c4deee53c3a3",
     "grade": false,
     "grade_id": "cell-7e0a6d565f4ecafe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "<img src=\"./images/extracted_train_df.png\" width=1300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0ad46",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e696962f054e91e31f1962789c5709f",
     "grade": false,
     "grade_id": "cell-dc826358857213dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 3: Data validation with Great Expectations (2 points)\n",
    "\n",
    "In this assignment, we practice the use of Great Expectations as a tool for data validation. We provide you with some code to help you instantiate and get started with Great Expectations. Running the code cell below creates a folder `gx` in your working directory with all things related to working with Great Expectations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877ca4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdcf3004e5f4993325ff76a03aa9b3c6",
     "grade": false,
     "grade_id": "cell-77b92bf7a2ffb8fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate the DataFrame that has gone through the data extraction and cleaning processes\n",
    "train_path = WORKING_DIR / \"data\" / \"reference\" / \"train\"\n",
    "train_df_clean = data_extraction(train_path, [\"excellent\", \"good\", \"satisfactory\"])\n",
    "\n",
    "housing_datasource_name = \"housing_datasource\"\n",
    "housing_expectation_suite_name = \"housing_expectation_suite\"\n",
    "\n",
    "# Instantiate a Data Context and save it in filesystem.\n",
    "context = gx.get_context(project_root_dir=str(WORKING_DIR))\n",
    "\n",
    "# Connect to data in our housing DataFrame.\n",
    "try:\n",
    "    # Create a new Data Source in the Data Context\n",
    "    datasource = context.sources.add_pandas(name=housing_datasource_name)\n",
    "except DataContextError:\n",
    "    # The Data Source already exists in the Data Context\n",
    "    datasource = context.get_datasource(housing_datasource_name)\n",
    "\n",
    "try:\n",
    "    # Create a new DataFrame Data Asset\n",
    "    training_data_asset = datasource.add_dataframe_asset(name=\"training_data\")\n",
    "except ValueError:\n",
    "    # The Data Asset already exists\n",
    "    training_data_asset = datasource.get_asset(\"training_data\")\n",
    "\n",
    "# Request all data in the DataFrame as a single batch\n",
    "training_batch_request = training_data_asset.build_batch_request(\n",
    "    dataframe=train_df_clean\n",
    ")\n",
    "\n",
    "# Create an Expectation Suite\n",
    "context.add_or_update_expectation_suite(housing_expectation_suite_name)\n",
    "\n",
    "# Create a Validator\n",
    "housing_validator = context.get_validator(\n",
    "    batch_request=training_batch_request,\n",
    "    expectation_suite_name=housing_expectation_suite_name,\n",
    ")\n",
    "display(housing_validator.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a447615",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "804fab3ee8b8a40eef77aefa5252a11e",
     "grade": false,
     "grade_id": "cell-c8a1c893a0b35a6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3a) Create Expectations\n",
    "Your task is to create the following Expectations using the `expect_*` methods provided by the Validator. Additionally, recall that the `expect_*` methods will also validate the data loaded into the Validator, so please also store the validation results in a list named `test_results`:\n",
    "1. DataFrame `df` should contain all the columns in the same order as `train_df` (the training DataFrame produced by the `data_extraction` function).\n",
    "1. Column `price` can not contain NaN values.\n",
    "1. Column `yr_built` shouldn't have values smaller than 1917.\n",
    "1. Column `bedrooms` should be an integer.\n",
    "1. Column `condition` should contain only values from the set {'satisfactory', 'good', 'excellent'}.\n",
    "\n",
    "Hints: The following validation functions may be helpful. You can find more information of their usage [here](https://greatexpectations.io/expectations/). \n",
    "- expect_table_columns_to_match_ordered_list\n",
    "- expect_column_values_to_not_be_null\n",
    "- expect_column_values_to_be_between\n",
    "- expect_column_values_to_be_of_type\n",
    "- expect_column_values_to_be_in_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da1db23",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb36cccffb5dfeafa04b5472b600372e",
     "grade": false,
     "grade_id": "cell-3731785336751adf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_results = []\n",
    "\n",
    "# TODO: test_results.append(...)\n",
    "\n",
    "### START CODE HERE\n",
    "raise NotImplementedError()\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5952e18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "081187106d072e981350b7a00fa4b3dc",
     "grade": true,
     "grade_id": "cell-c05cfedbe8f48d75",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check if the list of the test results is defined correctly\n",
    "assert len(test_results) == 5, \"The number of the expectations is incorrect.\"\n",
    "\n",
    "failed_expectation_list = [\n",
    "    result for result in test_results if not result[\"success\"]\n",
    "]\n",
    "\n",
    "assert (\n",
    "    len(failed_expectation_list) == 1\n",
    "), \"The number of the failed expectations is incorrect.\"\n",
    "\n",
    "failed_expectation = failed_expectation_list[0]\n",
    "\n",
    "assert (\n",
    "    failed_expectation[\"expectation_config\"][\"expectation_type\"]\n",
    "    == \"expect_column_values_to_be_between\"\n",
    "), \"The failed expectation type is incorrect.\"\n",
    "\n",
    "assert (\n",
    "    failed_expectation[\"result\"][\"unexpected_count\"] == 437\n",
    "), \"The number of the unexpected values in the failed expectation is incorrect.\"\n",
    "\n",
    "succeeded_expectation_type_list = [\n",
    "    result[\"expectation_config\"][\"expectation_type\"]\n",
    "    for result in test_results\n",
    "    if result[\"success\"]\n",
    "]\n",
    "expected_succeeded_expectation_types = [\n",
    "    \"expect_table_columns_to_match_ordered_list\",\n",
    "    \"expect_column_values_to_not_be_null\",\n",
    "    \"expect_column_values_to_be_of_type\",\n",
    "    \"expect_column_values_to_be_in_set\",\n",
    "]\n",
    "assert sorted(succeeded_expectation_type_list) == sorted(\n",
    "    expected_succeeded_expectation_types\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcaa0f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a3bde6f3e134524ada321d47159f2ba",
     "grade": false,
     "grade_id": "cell-5cfb9e5adc0b747c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the validation results of the batch loaded into the Validator\n",
    "for i, result in enumerate(test_results):\n",
    "    if result['success'] is True:      \n",
    "        print(f\"Test {i} succeeded ({result['expectation_config']['expectation_type']})\")\n",
    "    else:\n",
    "        print(f\"Test {i} failed ({result['expectation_config']['expectation_type']})\")\n",
    "        for item in result['result'].items():\n",
    "            print(\"\\t\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c8c89",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2709e45da8098dc748823d42551a2f34",
     "grade": false,
     "grade_id": "cell-ccea20b1b0277ea3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We see that the Expectation regarding the building year is not met. There are $\\frac{437}{11046}=3.96\\%$ of entries not conforming to our Expectation. There are (at least) three ways in which we can deal with this: \n",
    "1. We can manually check the min value of the column `yr_built` (in fact, we already have the info in Assignment 2b) and use that as the lower bound. \n",
    "1. We can use the keyword argument `mostly` to specify a fraction of the data that must conform with our expectations. This allows for some values to be outside of the defined interval. \n",
    "1. We can try to define the boundaries automatically (see below). \n",
    "\n",
    "Which ever approach we take, we can just redefine the Expectation and that will overwrite the old one.\n",
    "\n",
    "Some (but not all) Expectations allow for automatic inferring of boundaries for values using the batch loaded into the Validator. This is called auto-initializing. You can check if an Expectation can be auto-initialized as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf2859",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7e03b6697b8cb1e764e7034adc198fe",
     "grade": false,
     "grade_id": "cell-7e5c817fe5c2b3e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "boolean_1 = Expectation.is_expectation_auto_initializing(name=\"expect_table_columns_to_match_ordered_list\")\n",
    "boolean_2 = Expectation.is_expectation_auto_initializing(name=\"expect_column_values_to_be_between\")\n",
    "print(f\"\\nboolean_1 = {boolean_1} and boolean_2 = {boolean_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406124c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a56d66033cd25fa56c6a2f064fd361a5",
     "grade": false,
     "grade_id": "cell-e677298a665b8ae5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Using auto-initializing is convenient especially when working with a large number of covariates. However, it requires that we have access to some data, which has already been validated (luckily, we do). Also, auto-initialization might produce some rules that are too strict or too loose, so always be careful when using them and check the results.\n",
    "\n",
    "Update our `housing_validator` with the following Expectations: \n",
    "- The values of columns `yr_built`, `bedrooms`, and `date` should be between a minimum value and a maximum value. These values are automatically inferred by Great Expectations.\n",
    "- The values of columns `area` should be in a set. The set is also automatically inferred by Great Expectations.\n",
    "\n",
    "Similar to what you did previously, also append the resulted ExpectationValidationResults into a list. \n",
    "\n",
    "Hint: Check [here](https://docs.greatexpectations.io/docs/guides/expectations/how_to_use_auto_initializing_expectations/) on how to enable auto-initialization of an Expectation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cce19d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee7ec2cfb9ea942ca2b12b1b164c875e",
     "grade": false,
     "grade_id": "cell-df9bc5dee93b1eb0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_results = []\n",
    "\n",
    "# TODO: test_results.append(...)\n",
    "### START CODE HERE\n",
    "raise NotImplementedError()\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bdd18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89e2961889a101aad6f49a4ea81558c0",
     "grade": true,
     "grade_id": "cell-b2365218489bde15",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the test_results list again\n",
    "assert len(test_results) == 4, \"There should be 4 expectation validation results\"\n",
    "\n",
    "# Sort test_results by column name in expectation config\n",
    "test_results_sorted = sorted(\n",
    "    test_results, key=lambda k: k[\"expectation_config\"][\"kwargs\"][\"column\"]\n",
    ")\n",
    "\n",
    "expected_expectation_types = [\"expect_column_values_to_be_in_set\"] + [\n",
    "    \"expect_column_values_to_be_between\"\n",
    "] * 3\n",
    "expected_validated_column_names = [\"area\", \"bedrooms\", \"date\", \"yr_built\"]\n",
    "\n",
    "for i, result in enumerate(test_results_sorted):\n",
    "    assert (\n",
    "        result[\"expectation_config\"][\"expectation_type\"]\n",
    "        == expected_expectation_types[i]\n",
    "    ), \"Incorrect expectation type\"\n",
    "    assert (\n",
    "        result[\"expectation_config\"][\"kwargs\"].get(\"auto\") is True\n",
    "    ), \"Auto-initializing is not enabled\"\n",
    "    assert (\n",
    "        result[\"expectation_config\"][\"kwargs\"].get(\"column\")\n",
    "        == expected_validated_column_names[i]\n",
    "    ), \"Incorrect column name to be validated in the expectation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e5b59",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad60893056cf7f9de00106de1eea3a90",
     "grade": false,
     "grade_id": "cell-79d2b58ca0d990a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the Expectation configuration that Great Expectations automatically inferred\n",
    "for i, result in enumerate(test_results):\n",
    "    print(f\"Test {i} ({result['expectation_config']['expectation_type']}) resulted in config:\")\n",
    "    for item in result['expectation_config']['kwargs'].items():\n",
    "        print(\"\\t\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de3885",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eae065e2f43d05a2c67f4693e7bbd199",
     "grade": false,
     "grade_id": "cell-7444a8e9187bf8ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "Test 0 (expect_column_values_to_be_between) resulted in config:\n",
    "\t ('column', 'yr_built')\n",
    "\t ('min_value', 1900)\n",
    "\t ('max_value', 2015)\n",
    "\t ('mostly', 1.0)\n",
    "\t ('strict_min', False)\n",
    "\t ('strict_max', False)\n",
    "\t ('auto', True)\n",
    "\t ('batch_id', 'housing_datasource-training_data')\n",
    "Test 1 (expect_column_values_to_be_between) resulted in config:\n",
    "\t ('max_value', 5)\n",
    "\t ('column', 'bedrooms')\n",
    "\t ('min_value', 0)\n",
    "\t ('strict_max', False)\n",
    "\t ('mostly', 1.0)\n",
    "\t ('strict_min', False)\n",
    "\t ('auto', True)\n",
    "\t ('batch_id', 'housing_datasource-training_data')\n",
    "Test 2 (expect_column_values_to_be_between) resulted in config:\n",
    "\t ('max_value', '2015-12-24T00:00:00')\n",
    "\t ('column', 'date')\n",
    "\t ('min_value', '2007-07-02T00:00:00')\n",
    "\t ('strict_max', False)\n",
    "\t ('mostly', 1.0)\n",
    "\t ('strict_min', False)\n",
    "\t ('auto', True)\n",
    "\t ('batch_id', 'housing_datasource-training_data')\n",
    "Test 3 (expect_column_values_to_be_in_set) resulted in config:\n",
    "\t ('column', 'area')\n",
    "\t ('value_set', ['Jamison Centre', 'Gundaroo', 'Wanniassa', 'Kaleen', ...])\n",
    "\t ('mostly', 1.0)\n",
    "\t ('auto', True)\n",
    "\t ('batch_id', 'housing_datasource-training_data')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0296a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccd2da01ea8d6cb6f9061b75f13c5516",
     "grade": false,
     "grade_id": "cell-17e10abccf8857d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We could specify more Expectations, but the Expectations we've defined suffice for now. Recall that the Expectation Suite we have configured this far exists only in memory and has to be persisted for future use. Let's save the Expectation Suite into our Data Context.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8e861",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0949a501938dcf2ee9c4487551187b53",
     "grade": false,
     "grade_id": "cell-fd96ea4d16853405",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Persist the Expectation Suite\n",
    "housing_validator.save_expectation_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a3ee8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d323522bc2bdea60fac1eb1daa2aed11",
     "grade": false,
     "grade_id": "cell-92d15196cf71511d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3b) Validate new data against the defined Expectations\n",
    "\n",
    "Let's run our newly defined Expectation Suite against some test data. This is done by creating and configuring a [Checkpoint](https://docs.greatexpectations.io/docs/terms/checkpoint). Before we do that, we'll have to create a Batch Request for the new data. We have some unprocessed test data residing in a test data folder. \n",
    "\n",
    "Create the following functions:\n",
    "1. `batch_creator`, which \n",
    "    1. creates a new DataFrame DataAsset named \"test_data\" to a Data Source or gets the asset from the Data Source if it's already existing, then \n",
    "    1. creates a new Batch Request for the given DataFrame and returns the new Batch Request.\n",
    "1. `create_checkpoint`, which \n",
    "    1. creates a new Checkpoint with the following configurations:\n",
    "        - The name of the Checkpoint should be the given `checkpoint_name`,\n",
    "        - It should validate the data of the given `batch_request` using an Expectation Suite whose name is the given `expectation_suite_name`\n",
    "    1. uses the created Checkpoint to run a validation, the validation running name should be the given `run_name`,\n",
    "    1. finally, returns the validation results. \n",
    "\n",
    "Hints: \n",
    "- The following functions may be useful for implementing the `batch_creator` function: \n",
    "    - [add_dataframe_asset and build_batch_request](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/in_memory/connect_in_memory_data/#add-a-data-asset-to-the-data-source)\n",
    "    - [get_asset](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/batch_requests/how_to_request_data_from_a_data_asset#retrieve-your-data-asset)\n",
    "\n",
    "- The following functions may be useful for implementing the `create_checkpoint` function: \n",
    "    - [add_or_update_checkpoint](https://docs.greatexpectations.io/docs/reference/api/data_context/AbstractDataContext_class#great_expectations.data_context.AbstractDataContext.add_or_update_checkpoint)\n",
    "    - [(Checkpoint).run](https://docs.greatexpectations.io/docs/reference/api/checkpoint/Checkpoint_class#great_expectations.checkpoint.Checkpoint.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba25b77d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a1a0e9018634d61da259ad594306694",
     "grade": false,
     "grade_id": "cell-35742ca2a5bef3a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def batch_creator(df: pd.DataFrame, context: FileDataContext, data_source_name: str) -> BatchRequest:\n",
    "    \"\"\"\n",
    "    Creates a new Batch Request using the given DataFrame\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding the cleaned housing data. \n",
    "        context (GX FileDataContext): The current active GX Data Context \n",
    "        data_source_name (str): Name of the GX Data Source, to which the DataFrame is added\n",
    "    Returns:\n",
    "        new_batch_request (GX BatchRequest): The GX batch request created using df\n",
    "    \"\"\"\n",
    "    datasource = context.get_datasource(data_source_name)\n",
    "    test_asset_name = \"test_data\"\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n",
    "\n",
    "def create_checkpoint(context: FileDataContext, batch_request: BatchRequest, checkpoint_name: str, expectation_suite_name: str, run_name: str) -> CheckpointResult:\n",
    "    \"\"\"\n",
    "    Creates a new GX Checkpoint from the argument batch_request\n",
    "    Args:\n",
    "        context (GX FileDataContext): The current active context \n",
    "        batch_request (GX BatchRequest): A GX batch request used to create the Checkpoint\n",
    "        checkpoint_name (str): Name of the Checkpoint\n",
    "        expectation_suite_name (str): Name of the Expectation Suite used to validate the data\n",
    "        run_name (str): Name of the validation running\n",
    "    Returns:\n",
    "        checkpoint_result (GX CheckpointResult): \n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7412ee29",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0afb448ff1de2862c2c1289e767df4b8",
     "grade": false,
     "grade_id": "cell-5b495e9cdd5a8e7a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to call both functions. This should create the Checkpoint and returns a Checkpoint result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad14e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cd54c646c351552457ba5e3241d3162",
     "grade": false,
     "grade_id": "cell-6a516296b0f92793",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test data location\n",
    "test_path = WORKING_DIR / \"data\" / \"reference\" / \"test\"\n",
    "\n",
    "# Create the batch request and checkpoint\n",
    "context = gx.get_context()\n",
    "test_df_cleaned = data_extraction(test_path, [\"satisfactory\", \"good\", \"excellent\"])\n",
    "test_batch_request = batch_creator(\n",
    "    df=test_df_cleaned, context=context, data_source_name=housing_datasource_name\n",
    ")\n",
    "\n",
    "test_data_checkpoint_result = create_checkpoint(\n",
    "    context=context,\n",
    "    batch_request=test_batch_request,\n",
    "    checkpoint_name=\"test_data_checkpoint\",\n",
    "    expectation_suite_name=housing_expectation_suite_name,\n",
    "    run_name=\"test_run\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a4292",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ea5c838ce04adb0d2631fe96a1fc8ba",
     "grade": true,
     "grade_id": "cell-6e3aa26f9cec963d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check if the results of the Checkpoint run are correct\n",
    "validation_result = test_data_checkpoint_result.list_validation_results()[0]\n",
    "assert validation_result[\"statistics\"] == {\n",
    "    \"evaluated_expectations\": 8,\n",
    "    \"successful_expectations\": 7,\n",
    "    \"unsuccessful_expectations\": 1,\n",
    "    \"success_percent\": 87.5,\n",
    "}, \"The statistics of the validation result are incorrect.\"\n",
    "\n",
    "\n",
    "failed_validation_result = validation_result.get_failed_validation_results()[\"results\"][0]\n",
    "assert failed_validation_result[\"expectation_config\"][\"expectation_type\"] == \"expect_column_values_to_be_in_set\", \"The failed expectation type is incorrect.\"\n",
    "assert failed_validation_result[\"expectation_config\"][\"kwargs\"][\"column\"] == \"condition\", \"The column that doesn't pass the validation is incorrect.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea17531",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "630bcc676416bd003c3521f36bc50698",
     "grade": false,
     "grade_id": "cell-d8e95f44bceb9f80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Don't open the browser if the notebook is being graded. The if statement is needed for the grading purpose\n",
    "# You should see the browser open\n",
    "if not is_being_graded():\n",
    "    context.view_validation_result(test_data_checkpoint_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910ef15",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75093559da55ac4f16e28158dd0f94bb",
     "grade": false,
     "grade_id": "cell-04939a8e51b09336",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If everything went smoothly, a page showing the result should be opened in your browser. We see that one out of eight expectations has failed. By scrolling down, we see that the failed expectation is the one which specifies that the values of column `cond_typos_fixed` has to be one of {'satisfactory', 'good', 'excellent'}. The test data seems to have new values 'tolerable' and 'poor'. You can find the result in `gx/uncommitted/data_docs/local_site/index.html`\n",
    "\n",
    "NOTE: _If we hadn't specified a threshold when we did fuzzy string matching, the algorithm would have changed the values 'tolerable' and 'poor' to their nearest matches from {'satisfactory', 'good', 'excellent'}, which would have been very undesirable._\n",
    "\n",
    "Do the following:\n",
    "1. Update the Expectation for column `condition` to include values 'tolerable' and 'poor' as well.\n",
    "1. Save the updated Expectation Suite into our Data Context.\n",
    "1. Use the `create_checkpoint` function you just made to create a new Checkpoint with arguments (`context`, `test_batch_request`, `\"second_test_checkpoint\"`, `\"housing_expectation_suite\"`, `\"second_test_run\"`). Store the results as `second_test_checkpoint_result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb417abd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d577c822f95e599ed4f222e7ca153af",
     "grade": false,
     "grade_id": "cell-98ab1ba2e66e8924",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: second_test_data_checkpoint_result = ...\n",
    "### START CODE HERE\n",
    "raise NotImplementedError()\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd455f6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc6abbb3c332a76ae8eaa5d11e8da720",
     "grade": true,
     "grade_id": "cell-48242efaf9bbfb4b",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the results of the second Checkpoint Run\n",
    "assert (\n",
    "    second_test_data_checkpoint_result.success\n",
    "), \"The second test checkpoint is not successful.\"\n",
    "\n",
    "validation_result = second_test_data_checkpoint_result.list_validation_results()[0]\n",
    "assert (\n",
    "    validation_result[\"statistics\"][\"evaluated_expectations\"] == 8\n",
    "), \"The number of the evaluated expectations is incorrect.\"\n",
    "\n",
    "condition_expectation = list(\n",
    "    filter(\n",
    "        lambda r: r[\"expectation_config\"][\"expectation_type\"]\n",
    "        == \"expect_column_values_to_be_in_set\"\n",
    "        and r[\"expectation_config\"][\"kwargs\"][\"column\"] == \"condition\",\n",
    "        validation_result.results,\n",
    "    )\n",
    ")[0]\n",
    "assert sorted(\n",
    "    condition_expectation[\"expectation_config\"][\"kwargs\"][\"value_set\"]\n",
    ") == sorted([\"poor\", \"tolerable\", \"satisfactory\", \"good\", \"excellent\"]), \"The Expectation for the condition column is not correctly configured.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d5004",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "389caa5227106dc5b8f18111fbb63a2c",
     "grade": false,
     "grade_id": "cell-6e95ec8ec72b90cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not is_being_graded():\n",
    "    context.view_validation_result(second_test_data_checkpoint_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f110da",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba6c55aadf35769fa0ff46ad0268beaa",
     "grade": false,
     "grade_id": "cell-a12ffee74a09fc17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we should see that all Expectations have been met.\n",
    "\n",
    "NOTE: _Creating Checkpoints and validating data against a set of Expectations doesn't do anything in itself (other than create the Data Docs). We can of course manually check the Data Docs after validating each new batch of data, but it would be nice if could automate at least some part of this process. This is what the [Great Expectations Actions](https://docs.greatexpectations.io/docs/guides/validation/validation_actions/actions_lp) are for. However, we won't utilize this functionality in this exercise._\n",
    "\n",
    "### Screenshots to be submitted for Assignment 3\n",
    "1. A screenshot of the failed Checkpoint result (i.e., the Checkpoint run named \"test-run\").\n",
    "1. A screenshot of the succeeded Checkpoint result (i.e., the Checkpoint run named \"second-test-run\"). \n",
    "\n",
    "**Note**: It's enough to show the overview and the failed/corrected Validation. \n",
    "\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <figure>\n",
    "        <img src=\"./images/test-run-overview.png\" width=600/>\n",
    "        <img src=\"./images/test-run-condition.png\" width=800/>\n",
    "        <figcaption>test_run</figcaption>\n",
    "    </figure>\n",
    "    <br />\n",
    "    <figure>\n",
    "        <img src=\"./images/second-test-run-overview.png\" width=700/>\n",
    "        <img src=\"./images/second-test-run-condition.png\" width=800/>\n",
    "        <figcaption>second_test_run</figcaption>\n",
    "    </figure>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a6efe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a627d3f78bc24f3e00b634370e7b0a08",
     "grade": false,
     "grade_id": "cell-f1fc01df865427fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 4: Feature engineering (2 points)\n",
    "\n",
    "In this part we will touch on the subject of feature engineering. Since this subject is dealt with in more detail in the course '[Introduction to Machine Learning](https://studies.helsinki.fi/courses/course-unit/hy-CU-118207827-2021-08-01)', we try to keep it light whilst also (perhaps) covering some corners not talked about in that course. Some of the methods presented in this exercise require us to divide our data into features and targets, so let's begin by building a function to achieve that (the function is ready so you don't need to implement anything). Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a0f05",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57d74ea50310f415b940910ba0bc7807",
     "grade": false,
     "grade_id": "cell-c01cc77908fddd60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def separate_X_and_y(df: pd.DataFrame, target: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Separates the features and targets.\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding the cleaned housing data\n",
    "        target (str): Name of the target column\n",
    "    Returns:\n",
    "        X (DataFrame): A Pandas DataFrame holding the cleaned housing data without the target column\n",
    "        y (Series): A pandas Series with the target values\n",
    "    \"\"\"\n",
    "    y = df[target].astype(\"float64\")\n",
    "    X = df.loc[:, df.columns != target]\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5382b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "947965456ece9ec40eaa79f82e244eb3",
     "grade": false,
     "grade_id": "cell-8bdf3e74dfc672ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Then, we'll use this function to separate the target column from the rest for both the training and testing datasets. Let's create the entire datasets from the beginning. Again, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43abd85c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2367addb675122ee304893acdf636caf",
     "grade": false,
     "grade_id": "cell-349abacd59933286",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_path = WORKING_DIR / \"data\" / \"reference\" / \"train\"\n",
    "test_path = WORKING_DIR / \"data\" / \"reference\" / \"test\"\n",
    "\n",
    "train_df = data_extraction(train_path, [\"satisfactory\", \"good\", \"excellent\"])\n",
    "test_df = data_extraction(\n",
    "    test_path, [\"poor\", \"tolerable\", \"satisfactory\", \"good\", \"excellent\"]\n",
    ")\n",
    "\n",
    "X_train, y_train = separate_X_and_y(train_df, target=\"price\")\n",
    "X_test, y_test = separate_X_and_y(test_df, target=\"price\")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "display(X_train.head())\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfcc50a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad9ef03ed6783e6ed5a5384c7d80638c",
     "grade": false,
     "grade_id": "cell-48e0679192dc7b68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Expected output:\n",
    "```text\n",
    "X_train shape: (11046, 18)\n",
    "y_train shape: (11046,)\n",
    "X_test shape: (2762, 18)\n",
    "y_test shape: (2762,)\n",
    "```\n",
    "<img src=\"./images/data-split-output.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff82507",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b199231600c7143b6473442cb48a34df",
     "grade": false,
     "grade_id": "cell-cf33cddea7064098",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4a) Imputing missing values\n",
    "\n",
    "As we saw in Assignment 2a), the columns `sqft_living15` and `sqft_lot15` have roughly 1.5% and 10% of values missing respectively in the training data so we could try to impute something in place of the missing values. We could use a simple imputation scheme, such as imputing either zero, mean, median or mode to both columns in place of missing values. However, we could also use a more sophisticated scheme, such as [MICE](https://www.machinelearningplus.com/machine-learning/mice-imputation/), where we use a regressor model to infer the missing values by comparing other features iteratively. We will use the [scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html) in this exercise. \n",
    "\n",
    "Your task is to complete the `impute_missing` function that uses `IterativeImputer` to impute missing values of Dataframes `X_train` and `X_test`. We use parameter `add_indicator=True` to include two new columns that indicate which values were imputed. These indicator variables will have a value of 1 where the original data was missing and 0 where it was not. This information might be valuable in model training (and inference). \n",
    "\n",
    "You can use the `fit_transform` method to fit the imputer on a DataFrame X and return the transformed X. Note that the fit_transform method will return a Numpy array instead of a DataFrame, and the shape of the returned Numpy array will not match the shape of the DataFrame given as an argument, so you'll have to figure a way around this.\n",
    "\n",
    "Hint: You can use the method [get_feature_names_out](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer.get_feature_names_out) to list the column names of the `IterativeImputer` object once it has been fitted.\n",
    "\n",
    "NOTE: _The computational complexity increases very rapidly with regards to the number of features in the dataset._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd34fc8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "724ce13b001b9c38d3182a152f6a978f",
     "grade": false,
     "grade_id": "cell-98af70442792c361",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def impute_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Imputes missing numerical values using MICE.\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding the features\n",
    "    Returns:\n",
    "        The pandas DataFrame with the missing values imputed\n",
    "    \"\"\"\n",
    "    imp = IterativeImputer(random_state=RANDOM_SEED, add_indicator=True)\n",
    "\n",
    "    # A mask indicating which columns need to be imputed.\n",
    "    # We'll not include categorical or datetime columns in the calculations\n",
    "    included_columns_mask = [\n",
    "        True if x not in [\"postcode\", \"area\", \"date\", \"condition\"] else False\n",
    "        for x in df.columns\n",
    "    ]\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974bbfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17d4706988228c96ea535089c1abefc6",
     "grade": false,
     "grade_id": "cell-be8831eb79d44530",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's again test the function we just created. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1a232",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8955666cbc6dab2b9e4e5e8144c8cf30",
     "grade": true,
     "grade_id": "cell-f64f1a83007b2493",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_missing_value_imputed = impute_missing(X_train)\n",
    "X_test_missing_value_imputed = impute_missing(X_test)\n",
    "nan_train = X_train_missing_value_imputed.isnull().sum().sum()\n",
    "nan_test = X_test_missing_value_imputed.isnull().sum().sum()\n",
    "\n",
    "assert nan_train == 0, \"There are still missing values in the training set.\"\n",
    "assert nan_test == 0, \"There are still missing values in the test set.\"\n",
    "assert sorted(X_train_missing_value_imputed.columns) == sorted(\n",
    "    [\n",
    "        \"yr_built\",\n",
    "        \"bedrooms\",\n",
    "        \"postcode\",\n",
    "        \"area\",\n",
    "        \"date\",\n",
    "        \"bathrooms\",\n",
    "        \"condition\",\n",
    "        \"grade\",\n",
    "        \"floors\",\n",
    "        \"sqft_living\",\n",
    "        \"sqft_lot\",\n",
    "        \"sqft_basement\",\n",
    "        \"sqft_living15\",\n",
    "        \"sqft_lot15\",\n",
    "        \"waterfront\",\n",
    "        \"view\",\n",
    "        \"distance\",\n",
    "        \"similarity_scores\",\n",
    "        \"missingindicator_sqft_living15\",\n",
    "        \"missingindicator_sqft_lot15\",\n",
    "    ]\n",
    "), \"The columns of the DataFrame are incorrect after the missing value imputation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02d325",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9c70525b71219d6dd9624d64f32dddd",
     "grade": false,
     "grade_id": "cell-01abe12f22ce5c32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4b) Decomposing datetime and categorical variables\n",
    "\n",
    "The column `date` is actually hoarding a bunch of potential features. Since we converted this column to `DateTime` format, we can use the attributes of this format to decompose the column into several features, which might benefit model training. \n",
    "Create a function, which takes a DataFrame and a column name for a DateTime column as arguments and\n",
    "1. uses `pandas.Series.dt.*` attributes to decompose the DateTime column into three new columns: `year`, `quarter`, and `weekday`. The column `quarter` should have values 1-4 and the column `weekday` should have values 0-6, where 0 is Monday, 1 is Tuesday, and so on. (You might take a look at [this blog](https://datagy.io/pandas-datetime/)).\n",
    "1. removes the original DateTime column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06129ac7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2538fdc061fe546eb45a22a8220047d",
     "grade": false,
     "grade_id": "cell-7ffc479ffa2f7b8e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def datetime_decomposer(df: pd.DataFrame, dt_column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Decomposes datetime values into year, quarter, and weekday.\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding the features\n",
    "        dt_column_name(str): The name of the datetime column\n",
    "    Returns:\n",
    "        The pandas DataFrame with the datetime column decomposed\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a87a74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5973393a24116cacc27117a63d436dcd",
     "grade": false,
     "grade_id": "cell-c4e8954b69ae1c05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Apply this function for both `X_train` and `X_test` by running the cell below. There should be 22 columns at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e55cac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c12b1e3e71351300e9c31227f28cf00",
     "grade": true,
     "grade_id": "cell-de616c8a3ce3f68a",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_datetime_decomposed = datetime_decomposer(X_train_missing_value_imputed, \"date\")\n",
    "X_test_datetime_decomposed = datetime_decomposer(X_test_missing_value_imputed, \"date\")\n",
    "\n",
    "assert sorted(X_train_datetime_decomposed.columns) == sorted(\n",
    "    [\n",
    "        \"yr_built\",\n",
    "        \"bedrooms\",\n",
    "        \"postcode\",\n",
    "        \"area\",\n",
    "        \"bathrooms\",\n",
    "        \"condition\",\n",
    "        \"grade\",\n",
    "        \"floors\",\n",
    "        \"sqft_living\",\n",
    "        \"sqft_lot\",\n",
    "        \"sqft_basement\",\n",
    "        \"sqft_living15\",\n",
    "        \"sqft_lot15\",\n",
    "        \"waterfront\",\n",
    "        \"view\",\n",
    "        \"distance\",\n",
    "        \"similarity_scores\",\n",
    "        \"missingindicator_sqft_living15\",\n",
    "        \"missingindicator_sqft_lot15\",\n",
    "        \"year\",\n",
    "        \"quarter\",\n",
    "        \"weekday\",\n",
    "    ]\n",
    "), \"The columns of the DataFrame are incorrect after the datetime decomposition.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b716033",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e36d416e0b4b1adbc747eeef0ba3b45",
     "grade": false,
     "grade_id": "cell-960ad93b07634224",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Some ML algorithms can be used with categorical variables, but others require the features to be in numerical format. It might make sense to convert them anyway, if for example, we plan to compare different algorithms against each other. One simple option is to just assign a random integer for each value. This is called label encoding or ordinal encoding. However, this approach induces an arbitrary ordering for the values, which might confuse some algorithms (although 1 < 2, it doesn't make sense to state that 'cat' < 'dog'). If a variable is ordinal (like the variable `condition` in our case) and we __control the order__, this might be OK. \n",
    "\n",
    "Complete the `condition_encoder` function that uses label encoding to convert the values in column `condition` in a given DataFrame to numerical ones. Use scaling poor=1, tolerable=2, satisfactory=3, good=4, and excellent=5. \n",
    "\n",
    "Hint: The [pandas.Series.map](https://pandas.pydata.org/docs/dev/reference/api/pandas.Series.map.html#pandas.Series.map) function may be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af2687",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed5b432da0e611f2dede6a00d3925e99",
     "grade": false,
     "grade_id": "cell-0594d33c0b71e803",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def condition_encoder(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encodes conditions to numerical range 1-5\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding the features\n",
    "    Returns:\n",
    "        The pandas DataFrame with the condition column encoded\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663dcd27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d48e613da1e5247594a1f55387b615db",
     "grade": false,
     "grade_id": "cell-c14726b00830ce5e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to apply the encoder for the column `condition` in both `X_train` and `X_test`. You should see the following statistics:\n",
    "\n",
    "| | X_train | X_test |\n",
    "| :-: | :-: | :-: |\n",
    "| mean | 3.362665 | 3.296886 |\n",
    "| std | 0.526798 | 0.622107 |\n",
    "| min | 3 | 1 |\n",
    "| max | 5 | 5| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b37c3c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d2c07cbff739ecd5fd4501ab3f3a640",
     "grade": true,
     "grade_id": "cell-909b338ea9eef878",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_condition_encoded = condition_encoder(X_train_datetime_decomposed)\n",
    "X_test_condition_encoded = condition_encoder(X_test_datetime_decomposed)\n",
    "print(X_train_condition_encoded[\"condition\"].describe())\n",
    "print()\n",
    "print(X_test_condition_encoded[\"condition\"].describe())\n",
    "\n",
    "# Check if the condition column is correctly encoded\n",
    "X_train_condition_column_description = X_train_condition_encoded[\n",
    "    \"condition\"\n",
    "].describe()[[\"mean\", \"std\", \"min\", \"max\"]]\n",
    "expected_X_train_condition_column_description = pd.Series(\n",
    "    {\n",
    "        \"mean\": 3.362665,\n",
    "        \"std\": 0.526798,\n",
    "        \"min\": 3,\n",
    "        \"max\": 5,\n",
    "    }\n",
    ")\n",
    "assert series_approximately_same(\n",
    "    X_train_condition_column_description,\n",
    "    expected_X_train_condition_column_description,\n",
    "    1e-6,\n",
    "), \"The condition column of the training set is not correctly encoded.\"\n",
    "\n",
    "X_test_condition_column_description = X_test_condition_encoded[\"condition\"].describe()[\n",
    "    [\"mean\", \"std\", \"min\", \"max\"]\n",
    "]\n",
    "expected_X_test_condition_column_description = pd.Series(\n",
    "    {\n",
    "        \"mean\": 3.296886,\n",
    "        \"std\": 0.622107,\n",
    "        \"min\": 1,\n",
    "        \"max\": 5,\n",
    "    }\n",
    ")\n",
    "assert series_approximately_same(\n",
    "    X_test_condition_column_description,\n",
    "    expected_X_test_condition_column_description,\n",
    "    1e-6,\n",
    "), \"The condition column of the test set is not correctly encoded.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb380a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02f3f04489422b9ae0dbf24a0557e057",
     "grade": false,
     "grade_id": "cell-c0e7132c85554f6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In many cases, using label encoding is not advisable, because it implies that the values are ordered. For instance, take a look at the variable `postcode`, which although in numerical format, is actually a categorical variable. If we used them as is, a linear regressor model would have to assume that either a bigger postcode implies a bigger price or a smaller price. If (and when) this is not the case, the (assumably) valuable information in this column would be rendered pretty much useless.\n",
    "\n",
    "Another option for converting categorical variables into numeric format is to use one-hot-encoding, where a binary feature is created for each unique value of the variable. This often leads to sparse feature matrices, when the cardinality of the categorical variable is high. For example, the `area` column has 131 unique values, so using one-hot-encoding would require us to present this variable using 130 additional features.  \n",
    "\n",
    "One popular option is to use [target encoding](https://scikit-learn.org/stable/modules/preprocessing.html#target-encoder). The simplest way to use target encoding in a regression setting is to calculate the mean of target values and use that to encode categorical features. This might make sense, since for example, it's highly likely that the price of a house correlates with the area on which it is built. To reduce overfitting, the calculated means of categories can be (Laplace) smoothed with the overall mean of the target. Thus, the encoding $e_c$ for category $c$ becomes\n",
    "$$\n",
    "e_c = \\lambda_c \\cdot \\bar{y}_c + (1-\\lambda_c) \\cdot \\bar{y}, \n",
    "$$\n",
    "where $\\bar{y}_c$ is the category mean and $\\bar{y}$ is the overall mean of the target and $\\lambda \\in [0,1]$ is a parameter controlling the smoothing amount. \n",
    "\n",
    "Complete the `target_encode` function which applies target encoding for a set of columns in a given DataFrame using the scikit-learn implementation. It should behave as follows: \n",
    "- If the target column is specified, the methods `fit` and `transform` should both be used (separately, don't use `fit_transform` to disable cross-fitting). \n",
    "- If the target column is not specified (for test data), only the method `transform` should be used with a pre-trained encoder passed as an argument.\n",
    "- It replaces the values in the original column with the encoded ones.\n",
    "\n",
    "NOTE: _The implementation of \"fit_transform\" in scikit-learn uses cross-fitting internally to avoid over-fitting. One thing needs to be considered at the point: target encoding may also need to be applied to production data (i.e., the real-world data the model used to generate predictions). However, targets are not available during inference, the encodings derived using cross-fitting can not be used. We skip cross-fitting entirely for simplicity in this exercise and rely solely on smoothing._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7ddbc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ef9a7b4d8cd44eaa5c4f1481166e7d8",
     "grade": false,
     "grade_id": "cell-fce62bcce2d10b15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def target_encode(\n",
    "    df: pd.DataFrame,\n",
    "    columns: List[str],\n",
    "    target: Optional[pd.Series] = None,\n",
    "    encoder: Optional[TargetEncoder] = None,\n",
    ") -> Tuple[TargetEncoder, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Encodes postcode and area to numerical format using a target encoder\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding the features\n",
    "        columns (list of strings): Names of the categorical columns to be encoded\n",
    "        target (Series|None): A pandas Series with the target values. This is required only when fitting the encoder.\n",
    "        encoder(TargetEncoder|None): An already fitted encoder. This is required when we want to apply an encoder, which has already been fitted during training.\n",
    "    Returns:\n",
    "        A tuple consisting of the fitted encoder (either a new fitted one or the one passed as an argument) and the pandas DataFrame with the categorical columns encoded\n",
    "    \"\"\"\n",
    "    if encoder is None:\n",
    "        encoder = TargetEncoder(\n",
    "            target_type=\"continuous\", smooth=\"auto\", random_state=RANDOM_SEED\n",
    "        )\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n",
    "    df_copy[columns] = df_copy.loc[:, columns].astype(\"float64\")\n",
    "\n",
    "    return encoder, df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823c4fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f3062dd3b4e84c40cbe9d32c2888e69",
     "grade": false,
     "grade_id": "cell-064e46f810c2d30f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the function to target-encode columns `postcode` and `area` in both `X_train` and `X_test` by running the cell below. You should see the following statistics:\n",
    "\n",
    "| | X_train postcode | X_test postcode | X_train area | X_test area |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| mean | 5.919559e+05 | 5.991515e+05 | 5.897071e+05 | 5.963635e+05 |\n",
    "| std | 1.519866e+05 | 1.602493e+05 | 1.458486e+05 | 1.534131e+05 |\n",
    "| min | 1.506176e+05 | 1.506176e+05 | 1.378392e+05 | 1.619446e+05 |\n",
    "| max | 1.275222e+06 | 1.275222e+06| 1.298436e+06 | 1.298436e+06 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aad376",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f987e2fd5fd0c44587c8e3bd1d3415f",
     "grade": true,
     "grade_id": "cell-8480c6182c092062",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t_encoder, X_train_target_encoded = target_encode(\n",
    "    X_train_condition_encoded, [\"postcode\", \"area\"], target=y_train\n",
    ")\n",
    "_, X_test_target_encoded = target_encode(\n",
    "    X_test_condition_encoded, [\"postcode\", \"area\"], encoder=t_encoder\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Statistics for X_train postcode:\\n\", X_train_target_encoded[\"postcode\"].describe()\n",
    ")\n",
    "print(\n",
    "    \"\\nStatistics for X_test postcode:\\n\", X_test_target_encoded[\"postcode\"].describe()\n",
    ")\n",
    "print(\"Statistics for X_train area:\\n\", X_train_target_encoded[\"area\"].describe())\n",
    "print(\"\\nStatistics for X_test area:\\n\", X_test_target_encoded[\"area\"].describe())\n",
    "\n",
    "\n",
    "### Check if the postcode and area columns are correctly encoded\n",
    "indexes = [\"mean\", \"std\", \"min\", \"max\"]\n",
    "X_train_postcode_column_description = X_train_target_encoded[\"postcode\"].describe()[indexes]\n",
    "expected_X_train_postcode_column_description = pd.Series(\n",
    "    {\n",
    "        \"mean\": 5.919559e05,\n",
    "        \"std\": 1.519866e05,\n",
    "        \"min\": 1.506176e05,\n",
    "        \"max\": 1.275222e06,\n",
    "    }\n",
    ")\n",
    "assert series_approximately_same(\n",
    "    X_train_postcode_column_description, expected_X_train_postcode_column_description, 1\n",
    "), \"The postcode column of the training set is not correctly encoded.\"\n",
    "\n",
    "X_test_postcode_column_description = X_test_target_encoded[\"postcode\"].describe()[indexes]\n",
    "expected_X_test_postcode_column_description = pd.Series(\n",
    "    {\n",
    "        \"mean\": 5.991515e05,\n",
    "        \"std\": 1.602493e05,\n",
    "        \"min\": 1.506176e05,\n",
    "        \"max\": 1.275222e06,\n",
    "    }\n",
    ")\n",
    "assert series_approximately_same(\n",
    "    X_test_postcode_column_description, expected_X_test_postcode_column_description, 1\n",
    "), \"The postcode column of the test set is not correctly encoded.\"\n",
    "\n",
    "X_train_area_column_description = X_train_target_encoded[\"area\"].describe()[indexes]\n",
    "\n",
    "expected_X_train_area_column_description = pd.Series(\n",
    "    {\n",
    "        \"mean\": 5.897071e05,\n",
    "        \"std\": 1.458486e05,\n",
    "        \"min\": 1.378392e05,\n",
    "        \"max\": 1.298436e06,\n",
    "    }\n",
    ")\n",
    "\n",
    "assert series_approximately_same(\n",
    "    X_train_area_column_description, expected_X_train_area_column_description, 1\n",
    "), \"The area column of the training set is not correctly encoded.\"\n",
    "\n",
    "X_test_area_column_description = X_test_target_encoded[\"area\"].describe()[indexes]\n",
    "\n",
    "expected_X_test_area_column_description = pd.Series(\n",
    "    {\n",
    "        \"mean\": 5.963635e05,\n",
    "        \"std\": 1.534131e05,\n",
    "        \"min\": 1.619446e05,\n",
    "        \"max\": 1.298436e06,\n",
    "    }\n",
    ")\n",
    "assert series_approximately_same(\n",
    "    X_test_area_column_description, expected_X_test_area_column_description, 1\n",
    "),  \"The area column of the test set is not correctly encoded.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b739de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "057a3ed14b8cc01a73ec35441d2e5c16",
     "grade": false,
     "grade_id": "cell-c02d9f5ef66f8ad8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have now constructed a somewhat feasible feature extraction pipeline and derived a set of features. In real life, this process would be much more iterative (and time consuming) with a lot of experiments on different ways on how to build the features. We bypassed a lot of topics, such as scaling and normalization of numerical data, which are important parts for the optimal use of many ML algorithms. As already mentioned, this topic is dealt with more thoroughly on the [I2ML](https://studies.helsinki.fi/courses/course-unit/hy-CU-118207827-2021-08-01) course.\n",
    "\n",
    "The Internet is also full of (opinionated) blog posts on feature engineering. These posts make a good source for new ideas on how to do things, but remember to always reserve a hint of scepticism and try to think things through yourself. As a final note, the scikit-learn library offers nice tools for building complex feature engineering pipelines, which we omitted in these exercises due to lack of time. Feel free to [check it out](https://scikit-learn.org/stable/modules/compose.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b637f6c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "439d5cf267d4e72f1d9b54f77bf6b6ac",
     "grade": false,
     "grade_id": "cell-0b669d2a12d1bf6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4c) Persisting derived features and encoders\n",
    "\n",
    "To conclude this exercise, let's store the derived features and the fitted target encoder for future use. In a more demanding setup, we would use a feature store (like [Feast](https://feast.dev/)) for this, but in this exercise, we use just a dedicated folder in the local file system, namely the folder `feature_store`. \n",
    "\n",
    "Create the following functions:\n",
    "1. `store_features`, which takes a set of features and stores them in the `feature_store` folder with a user-specified name in `.parquet` format.\n",
    "1. `store_targets`, which takes a set of targets and stores them in the `feature_store` folder with a user-specified name in `.csv` format. To avoid potential issues in the remaining assignments, don't include index when saving targets into a CSV file. \n",
    "1. `store_encoder`, which takes a fitted `TargetEncoder` and stores it in the subfolder `encoders` inside the `feature_store` folder with a user specified name in `.pkl` (pickle) format.\n",
    "\n",
    "Hint: Pickle files are written in binary format. Use mode 'wb' when writing the pickle file.\n",
    "\n",
    "NOTE: _Pickle is the default format for persisting fitted models in scikit-learn. However, it is not secure. [Only unpickle data you trust](https://docs.python.org/3/library/pickle.html)._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe79516",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b17ebf4df9a1683bf7ca38b8a66d8f15",
     "grade": false,
     "grade_id": "cell-9873f2656c5a9f19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the feature_store and encoders directories if they don't exist\n",
    "encoder_dir = WORKING_DIR / \"feature_store\" / \"encoders\"\n",
    "if not encoder_dir.exists():\n",
    "    encoder_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51acfc38",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0add0995e1d42aa81161a985b09269de",
     "grade": false,
     "grade_id": "cell-20796edee9a5bc6c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def store_features(X: pd.DataFrame, feature_file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Stores a set of features to a specified location\n",
    "    Args:\n",
    "        X (DataFrame): A pandas DataFrame holding the features\n",
    "        feature_file_path (Path): Path for the stored features, e.g., feature_store/housing_train_X.parquet\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n",
    "\n",
    "def store_targets(y: pd.Series, target_file_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Stores a set of features to a specified location\n",
    "    Args:\n",
    "        y (Series): A pandas Series holding the target values\n",
    "        target_file_path (Path): Path for the stored targets, e.g., feature_store/housing_train_y.csv\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n",
    "\n",
    "def store_encoder(encoder: TargetEncoder, encoder_file_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Stores a targetEncoder to a specified location\n",
    "    Args:\n",
    "        encoder (TargetEncoder): A fitted scikit-learn TargetEncoder object\n",
    "        encoder_file_path (Path): Path of the stored target encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e5ca6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0f62ba49348a7c6cfa9bfed1635afc2",
     "grade": false,
     "grade_id": "cell-0df4f85654c5e6db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's again test the above functions. Run the cell below to see if a set of new files appears in your `feature_store` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1f38e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "880827bdeed85c7595e015119a991ff4",
     "grade": true,
     "grade_id": "cell-307ad8edcc7609e9",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "feature_store_path = WORKING_DIR / \"feature_store\"\n",
    "housing_train_x_path = feature_store_path / \"housing_train_X.parquet\"\n",
    "housing_test_x_path = feature_store_path / \"housing_test_X.parquet\"\n",
    "housing_train_y_path = feature_store_path / \"housing_train_y.csv\"\n",
    "housing_test_y_path =  feature_store_path / \"housing_test_y.csv\"\n",
    "housing_target_encoder_path = feature_store_path / \"encoders\" / \"housing_target_encoder.pkl\"\n",
    "\n",
    "store_features(X_train_target_encoded, housing_train_x_path)\n",
    "store_features(X_test_target_encoded, housing_test_x_path)\n",
    "store_targets(y_train, housing_train_y_path)\n",
    "store_targets(y_test, housing_test_y_path)\n",
    "store_encoder(t_encoder, housing_target_encoder_path)\n",
    "\n",
    "for validation_result_json_file in [housing_train_x_path, housing_test_x_path, housing_train_y_path, housing_test_y_path, housing_target_encoder_path]:\n",
    "    assert validation_result_json_file.exists(), f\"{validation_result_json_file} does not exist.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5bf8a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0d55d802ccd5f945e3fac3c0d0dcffd",
     "grade": false,
     "grade_id": "cell-6eb645e39c483cc1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 5: Feature importance and feature selection (2 points)\n",
    "Feature importance indicates how useful a feature is for a model to predict the target. Feature selection refers to the process of reducing the number of input variables in model training to save computational cost and even boost model performance. These topics are dealt in more detail in the [I2ML](https://studies.helsinki.fi/courses/course-unit/hy-CU-118207827-2021-08-01) course, so we'll just gently touch on them here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34106256",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad7a03c81cc19c9ad13ee14ead6d7e9a",
     "grade": false,
     "grade_id": "cell-cb76fdae0c6d70e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5a) Basic trial with random forest regressor\n",
    "\n",
    "Let's start by doing a simple comparison to see if the features we have prepared through the feature engineering steps are any good. In this exercise, we'll use [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) from the scikit-learn library to train two models, each uses different data. One model is trained with the \"fully engineered\" feature set we derived through the feature engineering steps in Assignment 4 and the other is trained with a \"minimally engineered\" dataset. We use [coefficient of determination $R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination) as our metric to evaluate the models (higher is better) and perform no hyperparameter optimization (which will be addressed in next week's exercises).\n",
    "\n",
    "Detailed instructions:\n",
    "1. Prepare the minimally engineered data. \n",
    "    1. Split the training and testing DataFrames `train_df_basic` and `test_df_basic` provided below into features and targets. Specifically, `X_train_basic` contains the un-engineered features for the training dataset, `y_train_basic` contains the target values for the training dataset, `X_test_basic` contains the un-engineered features for the testing dataset, and `y_test_basic` contains the target values for the testing dataset. You can use the function `separate_X_and_y` implemented in Assignment 4.\n",
    "    1. Remove all non-numerical columns (including the column `date`) and all columns with missing values from both `X_train_basic` and `X_test_basic`, since these might cause problems for the regressor models.\n",
    "1. Read the fully engineered feature set and the target set for both training and test data from your `feature_store` directory. Similarly, `X_train` contains the (fully engineered) features for the training dataset, `y_train` contains the corresponding target values for the training dataset, `X_test` contains the (fully engineered) features for the testing dataset, and `y_test` contains the corresponding target values for the testing dataset.\n",
    "1. Train a RandomForestRegressor model `housing_rfr_basic` with `X_train_basic` and `y_train_basic` and evaluate the performance with `X_test_basic` and `y_test_basic` using $R^2$ as score.\n",
    "1. Train another model `housing_rfr` using data `X_train` and `y_train`. Evaluate the models performance with `X_test` and `y_test` using $R^2$ as score.\n",
    "\n",
    "(The expected values are $R^2_{\\text{basic}} = 0.89742$ and $R^2_{\\text{engineered}} = 0.93228$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83301708",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f738cc4a6b70b565ed8b20cc174c0fd",
     "grade": false,
     "grade_id": "cell-dfbabe85b3f67fc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "train_path = WORKING_DIR / \"data\" / \"reference\" / \"train\"\n",
    "test_path = WORKING_DIR / \"data\" / \"reference\" / \"test\"\n",
    "\n",
    "# DataFrames of training and test data (not yet feature-engineered)\n",
    "train_df_basic = data_extraction(train_path, [\"excellent\", \"good\", \"satisfactory\"])\n",
    "test_df_basic = data_extraction(\n",
    "    test_path, [\"poor\", \"tolerable\", \"satisfactory\", \"good\", \"excellent\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59757355",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "743846dc27ff95f9d2e90385e68c4297",
     "grade": false,
     "grade_id": "cell-e83dec03760bf66f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Random forest regressor that will be trained using the minimally engineered data\n",
    "housing_rfr_basic = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED)\n",
    "\n",
    "# Random forest regressor that will be trained using the feature-engineered data\n",
    "housing_rfr = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED)\n",
    "\n",
    "# Names of columns that should not be included when using data without feature engineering to train a model\n",
    "# These columns are categorical or include missing values\n",
    "removed_columns = [\n",
    "    \"postcode\",\n",
    "    \"area\",\n",
    "    \"date\",\n",
    "    \"condition\",\n",
    "    \"sqft_living15\",\n",
    "    \"sqft_lot15\",\n",
    "]\n",
    "\n",
    "# R2 score of the model trained using minimally engineered data\n",
    "r2_score_basic = 0\n",
    "\n",
    "# R2 score of the model trained using the feature-engineered data\n",
    "r2_score = 0\n",
    "\n",
    "# TODO:\n",
    "# X_train_basic, y_train_basic = ...\n",
    "# X_test_basic, y_test_basic = ...\n",
    "# X_train = ...\n",
    "# y_train = ...\n",
    "# X_test = ...\n",
    "# y_test = ...\n",
    "# Remove the removed_columns from X_train_basic and X_test_basic\n",
    "# Then train and evaluate the models (housing_rfr_basic and housing_rfr) and assign the R2 scores to r2_score_basic and r2_score. E.g., \n",
    "# housing_rfr_basic.fit(...)\n",
    "# r2_score_basic = ...\n",
    "# housing_rfr.fit(...)\n",
    "# r2_score = ...\n",
    "\n",
    "### START CODE HERE\n",
    "raise NotImplementedError()\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608c2cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14f72230dd72c5520f5d7b38680994e1",
     "grade": true,
     "grade_id": "cell-0f15be8a0d2b58ae",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the R2 scores\n",
    "assert abs(r2_score_basic - 0.8974238153775849) < 0.01, \"The R^2 score for the test set using only basic (minimally-engineered) features is incorrect.\"\n",
    "assert abs(r2_score - 0.9322807716386374) < 0.01, \"The R^2 score for the test set using engineered features is incorrect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02241955",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63af2bcc038718bcd644cd3269081f5a",
     "grade": false,
     "grade_id": "cell-c1a1336771f33e3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5b) The most important features\n",
    "The random forest regressor implementation in sklearn allows access to the attribute [feature_importances_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#feature-importance-based-on-mean-decrease-in-impurity), which scores the features used in training using the (normalized) total reduction of error brought by that feature (higher values imply more important features).\n",
    "\n",
    "Your task is to obtain the feature importances of the `housing_rfr` model. Please create a variable named `feature_importances` that is a pandas Series and contains the feature importances of each feature in your training dataset used to train the `housing_rfr` model, sorted in descending order. More specifically, the indexes of the Series are the feature names and the values the importance values. E.g., \n",
    "```python\n",
    "print(feature_importances)\n",
    "# output\n",
    "sqft_living    0.6\n",
    "postcode       0.5\n",
    "area           0.4\n",
    "waterfront     0.3\n",
    "sqft_basement  0.2\n",
    "sqft_living15  0.1\n",
    "...\n",
    "```\n",
    "You may find this [example](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#feature-importance-based-on-mean-decrease-in-impurity) helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9cad8b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56f458df589460a8b0c3f475342fd090",
     "grade": false,
     "grade_id": "cell-8b1ee5a01120bce0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: feature_importances = ...\n",
    "### START CODE HERE\n",
    "raise NotImplementedError()\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a689fdc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3afeec34061e4c69fafa2452df95e8e1",
     "grade": true,
     "grade_id": "cell-72f988c19624075f",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(feature_importances) == pd.Series, \"feature_importances ia not a pandas Series.\"\n",
    "assert feature_importances.index[0] == \"sqft_living\", \"The most important feature is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416259a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e52ed4fdfc4e8775821858d3d21ef8d0",
     "grade": false,
     "grade_id": "cell-e3001c774d29e103",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the feature importances\n",
    "fig, ax = plt.subplots()\n",
    "feature_importances.plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42987826",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17b4d79fd128b405c03bfb082902957f",
     "grade": false,
     "grade_id": "cell-b4186acad90bbb6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should see the feature `sqft_living` is significantly more important than other features. \n",
    "\n",
    "<img src=\"./images/feature-importances.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed1697",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ec7d4d0f4225b088835fb103f162494",
     "grade": false,
     "grade_id": "cell-f62cbcd2e89c13cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5c) Select only the most important features\n",
    "In this exercise, you'll gain an intuition of how feature selection can affect model performance. Complete the `select_only_important_features` function. This function receives a Pandas Series holding feature importances, where the indexes of the Series are the feature names and the values the importance values (just like the `feature_importances` variable you created above). It then:\n",
    "1. creates a loop where a given RandomForestRegressor model (`rfr_model`) is fitted on each iteration beginning with just the most important feature from `X_train`. Then, it increases the number of important features by one in each iteration so the model is trained with all the features at the end. In other words, it needs to train a model using the $i+1$ most important features of `X_train` in $i^{th}$ iteration (suppose i starts with zero). The function also needs to evaluate the model performance against the testing dataset (`X_test` and `y_test`) using $R^2$ score on each iteration and append the score to a list.\n",
    "1. During the loop, the function should also keep track of the list of features that yields the best R2 score.\n",
    "1. Finally, the function should return a tuple consisting of two lists: 1) the feature list that yields the best R2 score and 2) a list of the R2 scores obtained from each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00209929",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81e62718136b24cf1e0c12c593eca392",
     "grade": false,
     "grade_id": "cell-a9e90cc8ef74ea8c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def select_only_important_features(\n",
    "    feature_importances: pd.Series,\n",
    "    rfr_model: RandomForestRegressor,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame | np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.DataFrame | np.ndarray,\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    \"\"\"\n",
    "    Select the most important features iteratively and fit a model using those features.\n",
    "    Args:\n",
    "        feature_importances: A pandas Series holding the feature importances\n",
    "        rfr_model: A RandomForestRegressor model to be fitted\n",
    "        X_train: A pandas DataFrame holding the training features\n",
    "        y_train: A pandas DataFrame or numpy array holding the training target values\n",
    "        X_test: A pandas DataFrame holding the test features\n",
    "        y_test: A pandas DataFrame or numpy array holding the test target values\n",
    "    Returns:\n",
    "        A tuple of a list of features that yields the best R2 score and the R2 scores obtained from each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    best_features = [] # keep track of the best features that yield the best R2 score\n",
    "    r2_scores = [] # keep track of the R2 scores obtained from each iteration\n",
    "    best_r2_score = 0 # keep track of the best R2 score obtained\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE\n",
    "\n",
    "    return best_features, r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57388b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c6cb2f3480754a25b6d7c7726bd1b88",
     "grade": false,
     "grade_id": "cell-c6298fe586af30f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run the function using the feature-engineered data and plot the results\n",
    "best_features, r2_scores = select_only_important_features(\n",
    "    feature_importances, RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED), X_train, y_train, X_test, y_test\n",
    ")\n",
    "plt.plot(list(range(1, len(feature_importances) + 1)), r2_scores)\n",
    "plt.xticks(list(range(1, len(X_train.columns) + 1)))\n",
    "plt.title(r\"$R^2$ score with the n most important features\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(r\"$R^2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ccc932",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b3891c3866ba0bb5768ef284c050ea5",
     "grade": true,
     "grade_id": "cell-1c38083e324ba827",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the results\n",
    "optimal_number_of_features = len(best_features)\n",
    "best_r2_score = min(r2_scores)\n",
    "\n",
    "unimportant_features = [x for x in X_train.columns if x not in best_features]\n",
    "\n",
    "assert optimal_number_of_features == 16, \"The number of optimal features is incorrect.\"\n",
    "assert set(best_features) == set(\n",
    "    [\n",
    "        \"sqft_living\",\n",
    "        \"postcode\",\n",
    "        \"area\",\n",
    "        \"waterfront\",\n",
    "        \"sqft_basement\",\n",
    "        \"sqft_living15\",\n",
    "        \"yr_built\",\n",
    "        \"view\",\n",
    "        \"condition\",\n",
    "        \"bathrooms\",\n",
    "        \"sqft_lot15\",\n",
    "        \"sqft_lot\",\n",
    "        \"distance\",\n",
    "        \"grade\",\n",
    "        \"year\",\n",
    "        \"bedrooms\",\n",
    "    ]\n",
    "), \"The best features that yield the best R2 score are incorrect.\"\n",
    "assert set(unimportant_features) == set(\n",
    "    [\n",
    "        \"floors\",\n",
    "        \"similarity_scores\",\n",
    "        \"missingindicator_sqft_living15\",\n",
    "        \"missingindicator_sqft_lot15\",\n",
    "        \"quarter\",\n",
    "        \"weekday\",\n",
    "    ]\n",
    "), \"The unimportant features are incorrect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd649eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87ffa4fee6e61b3caed5727d4fa51b9f",
     "grade": false,
     "grade_id": "cell-b9531379690605a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We now know which features can result in the best $R^2$ score, let's implement the `drop_unimportant_features` that drop those unimportant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23ee1d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bd24a0eb96c9e934d08b933e4309ac9",
     "grade": false,
     "grade_id": "cell-c1b921d0b7b374a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def drop_unimportant_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame from which the unimportant features should be dropped.\n",
    "    Returns:\n",
    "        DataFrame: a new DataFrame with the unimportant features dropped.\n",
    "    \"\"\"\n",
    "    unimportant_features = [\n",
    "        \"floors\",\n",
    "        \"similarity_scores\",\n",
    "        \"missingindicator_sqft_living15\",\n",
    "        \"missingindicator_sqft_lot15\",\n",
    "        \"quarter\",\n",
    "        \"weekday\",\n",
    "    ]\n",
    "    ### START CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5bfae9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfeb06baf775af1fb5a2d977cdf1dbed",
     "grade": true,
     "grade_id": "cell-a18dc2f1e52264f1",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Drop unimportant features\n",
    "X_train = pd.read_parquet(housing_train_x_path)\n",
    "X_test = pd.read_parquet(housing_test_x_path)\n",
    "X_train_unimportant_features_dropped = drop_unimportant_features(X_train)\n",
    "X_test_unimportant_features_dropped = drop_unimportant_features(X_test)\n",
    "\n",
    "assert set(X_train_unimportant_features_dropped.columns) == set(\n",
    "    [\n",
    "        \"sqft_living\",\n",
    "        \"postcode\",\n",
    "        \"area\",\n",
    "        \"waterfront\",\n",
    "        \"sqft_basement\",\n",
    "        \"sqft_living15\",\n",
    "        \"yr_built\",\n",
    "        \"view\",\n",
    "        \"condition\",\n",
    "        \"bathrooms\",\n",
    "        \"sqft_lot15\",\n",
    "        \"sqft_lot\",\n",
    "        \"distance\",\n",
    "        \"grade\",\n",
    "        \"year\",\n",
    "        \"bedrooms\",\n",
    "    ]\n",
    "), \"The columns of the training set are incorrect after the unimportant features are dropped.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2381c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98e7e232cfaabcc81f6ab31bfe3b5b70",
     "grade": false,
     "grade_id": "cell-fa85f1b99a55bbbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Save only important features\n",
    "store_features(X_train_unimportant_features_dropped, feature_store_path / \"housing_train_X_important.parquet\")\n",
    "store_features(X_test_unimportant_features_dropped, feature_store_path / \"housing_test_X_important.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494234d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "690bbe022c1b8d522c6e9dfae513f574",
     "grade": false,
     "grade_id": "cell-f1aae0b8af99462e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To conclude Assignments 4 and 5, let's again wrap everything inside one function that will be responsible for feature engineering. The function takes the data that has been preprocessed and validated, extracts all of the features, and stores them into the feature store. It's worth noting that this function will not only be applied to training data but also to test data and production data. As a result, the following three scenarios need to be considered:\n",
    "|dataset type|target included|fit a new target encoder|load a fitted target encoder|\n",
    "|------------|---------------|------------------------|----------------------------|\n",
    "|training|x|x||\n",
    "|test|x||x|\n",
    "|production|||x|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211411d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51214dad819598c1328891789573979d",
     "grade": false,
     "grade_id": "cell-6d4d0f83d6c75c01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineering_pipeline(df: pd.DataFrame, \n",
    "                                 feature_store_path: Path,\n",
    "                                 feature_file_name: str, \n",
    "                                 encoder_file_name: str, \n",
    "                                 target_file_name: Optional[str]=None, \n",
    "                                 fit_encoder: bool=False, \n",
    "                                 targets_included: bool=True) -> None:\n",
    "    \"\"\"\n",
    "    Converts a given (merged) housing data DataFrame into features and targets, performs feature engineering, and \n",
    "    stores the features along with possible targets and a fitted encoder\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding all of the housing data, or just the features (see targets_included)\n",
    "        feature_store_path (Path): Path of the feature store\n",
    "        feature_file_name (str): Filename for the stored features.\n",
    "        encoder_file_name (str): Filename for the stored encoder.\n",
    "        target_file_name (str|None): Filename for the stored targets.\n",
    "        fit_encoder (bool): Whether a new target encoder should be fitted. If False, uses a previously stored encoder\n",
    "        targets_included (bool):  If True, df has all of the housing data including targets. If False, df has only the features.\n",
    "    \"\"\"\n",
    "    if targets_included:\n",
    "        X, y = separate_X_and_y(df, target='price')   \n",
    "    else:\n",
    "        if fit_encoder:\n",
    "            raise ValueError(\"Target encoder can not be trained without targets.\")\n",
    "\n",
    "    X_missing_imputed_= impute_missing(X)\n",
    "    X_datetime_decomposed = datetime_decomposer(X_missing_imputed_, dt_column_name='date')\n",
    "    X_condition_encoded = condition_encoder(X_datetime_decomposed)\n",
    "    X_unimportant_features_dropped = drop_unimportant_features(X_condition_encoded)\n",
    "\n",
    "    feature_file_path = feature_store_path / feature_file_name\n",
    "    target_file_path = feature_store_path / target_file_name\n",
    "    encoder_file_path = feature_store_path / \"encoders\" / encoder_file_name\n",
    "\n",
    "    if fit_encoder:\n",
    "        t_encoder, X_target_encoded = target_encode(X_unimportant_features_dropped, columns=['postcode', 'area'], target=y)\n",
    "        store_features(X_target_encoded, feature_file_path)\n",
    "        store_targets(y, target_file_path)\n",
    "        store_encoder(t_encoder, encoder_file_path)\n",
    "        \n",
    "    else:\n",
    "        with open(encoder_file_path, 'rb') as enc_f:\n",
    "            t_encoder = pickle.load(enc_f)\n",
    "        _, X_target_encoded = target_encode(X_unimportant_features_dropped, columns=['postcode', 'area'], encoder=t_encoder)\n",
    "        store_features(X_unimportant_features_dropped, feature_file_path)    \n",
    "        if targets_included:\n",
    "            store_targets(y, target_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8469fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "254e62d003fcc9133c3a6cbb03e6afed",
     "grade": false,
     "grade_id": "cell-54fbf57609802b54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 6: One script to rule them all (2 points)\n",
    "\n",
    "The jupyter notebook we have played with this far is a good tool for exploratory and iterative development, but contains all sort of redundant code, which is no longer needed once we want to push the finalized version into production. As a final wrap-up exercise, we'll collect all of the functionalities created in this assignment to a single streamlined `etl.py` module. The file `etl.py` already exists as a template with all the necessary imports and global variables. \n",
    "\n",
    "### 6a) Let's begin with copying the functions...\n",
    "\n",
    "Copy the following functions to `etl.py`: file_reader, dataframe_merger, drop_futile_columns, correct_distance_unit, string_transformer, typo_fixer, data_extraction, batch_creator, create_checkpoint, separate_X_and_y, impute_missing, datetime_decomposer, condition_encoder, target_encode, store_features, store_targets, store_encoder, drop_unimportant_features, feature_engineering_pipeline. \n",
    "\n",
    "There are many ways you can achieve this. Here are three:\n",
    "1. Just copy-paste each function (carefully).\n",
    "1. Use jupyter cell magic. You can add the line `%%writefile -a etl.py` at the beginning of each cell that contains a function. Run each of these cells exactly once. This will append the cell content inside the `etl.py` file. The cells should not contain code outside the function definitions. Once you've done, remove the cell magic commands.\n",
    "1. Use `jupyter nbconvert` function. For example, running the following command will copy all your Python code (while ignoring Markdown cells) to a Python script that has the same name as the notebook. You can then clean the exported Python script and copy needed code such as the code skeleton of the `etl` function from `etl.py`. (Remember to rename your script to `etl.py` once you're done, otherwise you'll encounter issues when running some of the following code cells.)\n",
    "```bash\n",
    "jupyter nbconvert --to python --PythonExporter.exclude_markdown=True --no-prompt <jupyter-botebook-name>.ipynb \n",
    "```\n",
    "### 6b) ...and create a wrapper\n",
    "\n",
    "Create a wrapper function `etl` that loads, merges, cleans, and validates the specified data, extract features, and save the features (and possibly targets) in the feature store. Specifically, the function needs to perform as follows:\n",
    "- It first runs the `data_extraction` function, which will result in a merged, cleaned DataFrame. you can use the hard-coded `correct_condition_values=['poor', 'tolerable', 'satisfactory', 'good', 'excellent']` in the `data_extraction` function.\n",
    "- Then it uses Great Expectations Checkpoint to validate the DataFrame produced by the `data_extraction` function (The `batch_creator` and `create_checkpoint` functions are useful here). You can assume that the Great Expectations Context and the Expectation Suite are already configured. The GX Context is available via `context = gx.get_context(context_root_dir=gx_context_root_dir)` where `gx_context_root_dir` is the directory that contains all your Great Expectations config, which is the `gx` directory in our case. \n",
    "- If some validations fails, a warning should be printed. The warning can be as simple as a sentence like \"Some GX validations failed\". Hint: Please refer to [here](https://docs.greatexpectations.io/docs/reference/api/checkpoint/types/checkpoint_result/checkpointresult_class/) on how to check whether a validation succeeds or not. \n",
    "- Finally, no matter whether or not the DataFrame passes all Expectations, the `etl` function should feature-engineer the DataFrame using the `feature_engineering_pipeline` function. \n",
    "\n",
    "You can see a detailed description of the parameters of the `etl` function in [etl.py](./etl.py). \n",
    "\n",
    "Remember to include the completed `etl.py` in your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27917cd9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e273326167617700c633c799851b1c77",
     "grade": false,
     "grade_id": "cell-0aea2566cf7ccae7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's import the `etl` function and use it to process data from 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b233388",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0dd8f19edff4711714556857b0fbd40",
     "grade": false,
     "grade_id": "cell-ff8c9aec76ad024e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from etl import etl\n",
    "\n",
    "params = {\n",
    "        \"path\": WORKING_DIR / \"data\" / \"2016\",\n",
    "        \"gx_context_root_dir\": WORKING_DIR / \"gx\",\n",
    "        \"gx_datasource_name\": \"housing_datasource\", \n",
    "        \"gx_checkpoint_name\": \"checkpoint_2016\",\n",
    "        \"gx_expectation_suite_name\": \"housing_expectation_suite\",\n",
    "        \"gx_run_name\": \"run_2016\",\n",
    "        \"feature_store_path\": WORKING_DIR / \"feature_store\",\n",
    "        \"feature_file_name\": \"2016_test_X.parquet\",\n",
    "        \"encoder_file_name\": \"housing_target_encoder.pkl\", \n",
    "        \"target_file_name\": \"2016_test_y.csv\", \n",
    "        \"fit_encoder\": False,\n",
    "        \"targets_included\": True\n",
    "    }\n",
    "etl(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85873e88",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48f0b00cacc3be10e7220ac8b5e88eba",
     "grade": false,
     "grade_id": "cell-928db6679be705d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should see a warning telling that two Expectations failed. The failed Expectations are \"expect_column_values_to_be_in_set\" for the \"area\" column and \"expect_column_values_to_be_between\" for the \"date\" column. \n",
    "\n",
    "**Please attach the failed Expectations to your PDF file**, e.g., \n",
    "\n",
    "<img src=\"./images/failed-expectation-example.png\" />\n",
    "\n",
    "(Please note that the provided example doesn't show the failed validations for data from 2016. It just illustrates which parts of the Checkpoint result file should be included in the submitted PDF.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579ffd4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f098ebb105541e58977ab127ac8756c8",
     "grade": false,
     "grade_id": "cell-74bbdf3ba2d00a50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's train a model using only the important features of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb868880",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7735dc5c80e43ef72c67e816c79b8f8f",
     "grade": false,
     "grade_id": "cell-906b9b000b2a8ab7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "WORKING_DIR = Path.cwd()\n",
    "feature_store_path = WORKING_DIR / \"feature_store\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "train_X_important = pd.read_parquet(feature_store_path / \"housing_train_X_important.parquet\")\n",
    "train_y = pd.read_csv(feature_store_path / \"housing_train_y.csv\").values.ravel()\n",
    "housing_rfr2 = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED)\n",
    "housing_rfr2.fit(train_X_important, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c4c95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36a9633af19117f02c9dfa38fec5b355",
     "grade": false,
     "grade_id": "cell-580346d44186a2e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Then, evaluate the `housing_rfr2` model using data from 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec527cd8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bae24c9a90628756682915a52ea0cff3",
     "grade": true,
     "grade_id": "cell-c5562fe378aea204",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_test_2016 = pd.read_parquet(WORKING_DIR / \"feature_store\" / \"2016_test_X.parquet\")\n",
    "y_test_2016 = pd.read_csv(WORKING_DIR / \"feature_store\" / \"2016_test_y.csv\")\n",
    "r2_score = housing_rfr2.score(X_test_2016, y_test_2016)\n",
    "print(f\"Model's R2 score using data from 2016 as test data: {r2_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1ec34",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "500eed6e52979b3921d96790cb2015ef",
     "grade": false,
     "grade_id": "cell-b31e6527d78724cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Model's R2 score using data from 2016 as test data should be 0.92359378. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062aadc8",
   "metadata": {},
   "source": [
    "## What to submit\n",
    "- This Jupyter notebook (`week2_assignments.ipynb`)\n",
    "- etl.py\n",
    "- The PDF file containing your screenshots for Assignments 3 and 6.\n",
    "\n",
    "**N.B.** Before making your submission, please check that your notebook and code files are named **exactly** as specified here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
